{"papers":[{"Conference":"VAST","Year":2006,"Title":"Collaborative Visual Analytics: Inferring from the Spatial Organization and Collaborative Use of Information","DOI":"10.1109/VAST.2006.261415","Link":"http://dx.doi.org/10.1109/VAST.2006.261415","FirstPage":137,"LastPage":144,"PaperType":"C","Abstract":"We introduce a visual analytics environment for the support of remote-collaborative sense-making activities. Team members use their individual graphical interfaces to collect, organize and comprehend task-relevant information relative to their areas of expertise. A system of computational agents infers possible relationships among information items through the analysis of the spatial and temporal organization and collaborative use of information. The computational agents support the exchange of information among team members to converge their individual contributions. Our system allows users to navigate vast amounts of shared information effectively and remotely dispersed team members to work independently without diverting from common objectives as well as to minimize the necessary amount of verbal communication","AuthorNames-Deduped":"Paul E. Keel","AuthorNames":"Paul E. Keel","AuthorAffiliation":"Massachusetts Institute of Technology, Computer Science and Artificial Intelligence Laboratory, e-mail: keel@mit.edu","InternalReferences":null,"AuthorKeywords":"Visual analytics, Spatial information organization,Indirect human computer interaction,Indirect collaboration, Agents,Sense-making","AminerCitationCount_02-2020":26,"AminerCitationCount_06-2020":34,"XploreCitationCount - 2020-01":16,"PubsCited":23,"Award":null,"image":"4035758-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2006,"Title":"Beyond Usability: Evaluation Aspects of Visual Analytic Environments","DOI":"10.1109/VAST.2006.261416","Link":"http://dx.doi.org/10.1109/VAST.2006.261416","FirstPage":145,"LastPage":150,"PaperType":"C","Abstract":"A new field of research, visual analytics, has been introduced. This has been defined as \"the science of analytical reasoning facilitated by interactive visual interfaces\" (Thomas and Cook, 2005). Visual analytic environments, therefore, support analytical reasoning using visual representations and interactions, with data representations and transformation capabilities, to support production, presentation, and dissemination. As researchers begin to develop visual analytic environments, it is advantageous to develop metrics and methodologies to help researchers measure the progress of their work and understand the impact their work has on the users who work in such environments. This paper presents five areas or aspects of visual analytic environments that should be considered as metrics and methodologies for evaluation are developed. Evaluation aspects need to include usability, but it is necessary to go beyond basic usability. The areas of situation awareness, collaboration, interaction, creativity, and utility are proposed as the five evaluation areas for initial consideration. The steps that need to be undertaken to develop systematic evaluation methodologies and metrics for visual analytic environments are outlined","AuthorNames-Deduped":"Jean Scholtz","AuthorNames":"Jean Scholtz","AuthorAffiliation":"Pacific Northwest National Laboratory. e-mail: jean.scholtz@pnl.gov","InternalReferences":"10.1109/VISUAL.1990.146375;10.1109/INFVIS.2004.10;10.1109/INFVIS.1997.636794","AuthorKeywords":"visualization, analytic environments,metrics","AminerCitationCount_02-2020":59,"AminerCitationCount_06-2020":61,"XploreCitationCount - 2020-01":28,"PubsCited":26,"Award":null,"image":""},{"Conference":"VAST","Year":2006,"Title":"Visualizing the Performance of Computational Linguistics Algorithms","DOI":"10.1109/VAST.2006.261417","Link":"http://dx.doi.org/10.1109/VAST.2006.261417","FirstPage":151,"LastPage":157,"PaperType":"C","Abstract":"We have built a visualization system and analysis portal for evaluating the performance of computational linguistics algorithms. Our system focuses on algorithms that classify and cluster documents by assigning weights to words and scoring each document against high dimensional reference concept vectors. The visualization and algorithm analysis techniques include confusion matrices, ROC curves, document visualizations showing word importance, and interactive reports. One of the unique aspects of our system is that the visualizations are thin-client Web-based components built using SVG visualization components","AuthorNames-Deduped":"Stephen G. Eick;Justin Mauger;Alan Ratner","AuthorNames":"Stephen G. Eick;Justin Mauger;Alan Ratner","AuthorAffiliation":"SSS Research, Inc. eick@sss-research.com;SAIC Advanced Systems & Concepts, maugerj@saic.com;National Security Agency, asratne@nsa.gov","InternalReferences":null,"AuthorKeywords":"AJAX, thin-client, SVG, ROC curves, confusion matrices, document categorization","AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":5,"XploreCitationCount - 2020-01":2,"PubsCited":6,"Award":null,"image":"4035760-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"Scentindex: Conceptually Reorganizing Subject Indexes for Reading","DOI":"10.1109/VAST.2006.261418","Link":"http://dx.doi.org/10.1109/VAST.2006.261418","FirstPage":159,"LastPage":166,"PaperType":"C","Abstract":"A great deal of analytical work is done in the context of reading, in digesting the semantics of the material, the identification of important entities, and capturing the relationship between entities. Visual analytic environments, therefore, must encompass reading tools that enable the rapid digestion of large amount of reading material. Other than plain text search, subject indexes, and basic highlighting, tools are needed for rapid foraging of text. In this paper, we describe a technique that presents an enhanced subject index for a book by conceptually reorganizing it to suit particular expressed user information needs. Users first enter information needs via keywords describing the concepts they are trying to retrieve and comprehend. Then our system, called ScentIndex, computes what index entries are conceptually related and reorganizes and displays these index entries on a single page. We also provide a number of navigational cues to help users peruse over this list of index entries and find relevant passages quickly. Compared to regular reading of a paper book, our study showed that users are more efficient and more accurate in finding, comparing, and comprehending material in our system","AuthorNames-Deduped":"Ed H. Chi;Lichan Hong;Julie Heiser;Stuart K. Card","AuthorNames":"Ed H. Chi;Lichan Hong;Julie Heiser;Stuart K. Card","AuthorAffiliation":"Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304. echi@parc.com;Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304. hong@parc.com;Adobe Systems, 321 Park Ave., San Jose, CA 95110, julie.heiser@adobe.com;Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304. card@parc.com","InternalReferences":null,"AuthorKeywords":"Book Index, eBooks, Information Scent, contextualization, personalized information access","AminerCitationCount_02-2020":10,"AminerCitationCount_06-2020":13,"XploreCitationCount - 2020-01":1,"PubsCited":37,"Award":null,"image":"4035761-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"VAST 2006 Contest - A Tale of Alderwood","DOI":"10.1109/VAST.2006.261420","Link":"http://dx.doi.org/10.1109/VAST.2006.261420","FirstPage":215,"LastPage":216,"PaperType":"M","Abstract":"Visual analytics experts realize that one effective way to push the field forward and to develop metrics for measuring the performance of various visual analytics components is to hold an annual competition. The first visual analytics science and technology (VAST) contest was held in conjunction with the 2006 IEEE VAST Symposium. The competition entailed the identification of possible political shenanigans in the fictitious town of Alderwood. A synthetic data set was made available as well as tasks. We summarize how we prepared and advertised the contest, developed some initial metrics for evaluation, and selected the winners. The winners were invited to participate at an additional live competition at the symposium to provide them with feedback from senior analysts","AuthorNames-Deduped":"Georges G. Grinstein;Theresa A. O'Connell;Sharon J. Laskowski;Catherine Plaisant;Jean Scholtz;Mark A. Whiting","AuthorNames":"Georges Grinstein;Theresa O'Connell;Sharon Laskowski;Catherine Plaisant;Jean Scholtz;Mark Whiting","AuthorAffiliation":"University of Massachusetts Lowell, grinstein@cs.uml.edu;National Institute of Standards and Technology, theresa.oconnell@nist.gov;National Institute of Standards and Technology, sharon.laskowski@nist.gov;University of Maryland, plaisant@cs.umd.edu;Pacific Northwest, National Laboratory, jean.scholtz@pnl.com;Pacific Northwest, National Laboratory, mark.a.whiting@pnl.gov","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":20,"AminerCitationCount_06-2020":30,"XploreCitationCount - 2020-01":14,"PubsCited":8,"Award":null,"image":"4035768-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"A Visual Interface for Multivariate Temporal Data: Finding Patterns of Events across Multiple Histories","DOI":"10.1109/VAST.2006.261421","Link":"http://dx.doi.org/10.1109/VAST.2006.261421","FirstPage":167,"LastPage":174,"PaperType":"C","Abstract":"Finding patterns of events over time is important in searching patient histories, Web logs, news stories, and criminal activities. This paper presents PatternFinder, an integrated interface for query and result-set visualization for search and discovery of temporal patterns within multivariate and categorical data sets. We define temporal patterns as sequences of events with inter-event time spans. PatternFinder allows users to specify the attributes of events and time spans to produce powerful pattern queries that are difficult to express with other formalisms. We characterize the range of queries PatternFinder supports as users vary the specificity at which events and time spans are defined. Pattern Finder's query capabilities together with coupled ball-and-chain and tabular visualizations enable users to effectively query, explore and analyze event patterns both within and across data entities (e.g. patient histories, terrorist groups, Web logs, etc.)","AuthorNames-Deduped":"Jerry Alan Fails;Amy K. Karlson;Layla Shahamat;Ben Shneiderman","AuthorNames":"Jerry Alan Fails;Amy Karlson;Layla Shahamat;Ben Shneiderman","AuthorAffiliation":"Department of Computer Science & Human-Computer Interaction Lab, University of Maryland, A. V. Williams Building, College Park, Maryland 20742. fails@cs.umd.edu;Department of Computer Science & Human-Computer Interaction Lab, University of Maryland, A. V. Williams Building, College Park, Maryland 20742. akk@cs.umd.edu;Department of Computer Science & Human-Computer Interaction Lab, University of Maryland, A. V. Williams Building, College Park, Maryland 20742. laylas@cs.umd.edu;Department of Computer Science & Human-Computer Interaction Lab, University of Maryland, A. V. Williams Building, College Park, Maryland 20742. ben@cs.umd.edu","InternalReferences":"10.1109/INFVIS.2001.963273","AuthorKeywords":"Temporal query, information visualization, user interface","AminerCitationCount_02-2020":81,"AminerCitationCount_06-2020":101,"XploreCitationCount - 2020-01":57,"PubsCited":27,"Award":"TT","image":"4035762-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"User Interfaces for the Exploration of Hierarchical Multi-dimensional Data","DOI":"10.1109/VAST.2006.261422","Link":"http://dx.doi.org/10.1109/VAST.2006.261422","FirstPage":175,"LastPage":182,"PaperType":"C","Abstract":"A variety of user interfaces have been developed to support the querying of hierarchical multi-dimensional data in an OLAP setting such as pivot tables and Polaris. They are used to regularly check portions of a dataset and to explore a new dataset for the first time. In this paper, we establish criteria for OLAP user interface capabilities to facilitate comparison. Two criteria are the number of displayed dimensions along which comparisons can be made and the number of dimensions that are viewable at once - visual comparison depth and width. We argue that interfaces with greater visual comparison depth support regular checking of known data by users that know roughly where to look, while interfaces with greater comparison width support exploration of new data by users that have no a priori starting point and need to scan all dimensions. Pivot tables and Polaris are examples of the former. The main contribution of this paper is to introduce a new scalable interface that uses parallel dimension axis which supports the latter, greater visual comparison width. We compare our approach to both table based and parallel coordinate based interfaces. We present an implementation of our interface SGViewer, user scenarios and provide an evaluation that supports the usability of our interface","AuthorNames-Deduped":"Mark Sifer","AuthorNames":"Mark Sifer","AuthorAffiliation":"School of Economics & Information Systems, University of Wollongong, Australia. msifer@uow.edu.au","InternalReferences":"10.1109/INFVIS.2002.1173157;10.1109/INFVIS.2005.1532139","AuthorKeywords":"Data exploration, OLAP, visualization, parallel coordinates","AminerCitationCount_02-2020":13,"AminerCitationCount_06-2020":13,"XploreCitationCount - 2020-01":6,"PubsCited":20,"Award":null,"image":"4035763-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"Pixnostics: Towards Measuring the Value of Visualization","DOI":"10.1109/VAST.2006.261423","Link":"http://dx.doi.org/10.1109/VAST.2006.261423","FirstPage":199,"LastPage":206,"PaperType":"C","Abstract":"During the last two decades a wide variety of advanced methods for the visual exploration of large data sets have been proposed. For most of these techniques user interaction has become a crucial element, since there are many situations in which a user or an analyst has to select the right parameter settings from among many or select a subset of the available attribute space for the visualization process, in order to construct valuable visualizations that provide insight, into the data and reveal interesting patterns. The right choice of input parameters is often essential, since suboptimal parameter settings or the investigation of irrelevant data dimensions make the exploration process more time consuming and may result in wrong conclusions. In this paper we propose a novel method for automatically determining meaningful parameter- and attribute settings based on the information content of the resulting visualizations. Our technique called Pixnostics, in analogy to Scagnostics (Wilkinson et al., 2005), automatically analyses pixel images resulting from diverse parameter mappings and ranks them according to the potential value for the user. This allows a more effective and more efficient visual data analysis process, since the attribute/parameter space is reduced to meaningful selections and thus the analyst obtains faster insight into the data. Real world applications are provided to show the benefit of the proposed approach","AuthorNames-Deduped":"Jörn Schneidewind;Mike Sips;Daniel A. Keim","AuthorNames":"Jorn Schneidewind;Mike Sips;Daniel A. Keim","AuthorAffiliation":"University of Konstanz, Germany. schneide@inf.uni-konstanz.de;Stanford University, USA. ms@pixelmap.org;University of Konstanz, Germany. keim@inf.uni-konstanz.de","InternalReferences":"10.1109/INFVIS.2005.1532145;10.1109/INFVIS.2005.1532142;10.1109/VISUAL.2005.1532782;10.1109/VISUAL.2005.1532781;10.1109/INFVIS.2000.885092","AuthorKeywords":"Visual Data Exploration, Visualization technique,Visual Analytics","AminerCitationCount_02-2020":35,"AminerCitationCount_06-2020":38,"XploreCitationCount - 2020-01":23,"PubsCited":24,"Award":null,"image":"4035766-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"Exploratory Visualization of Multivariate Data with Variable Quality","DOI":"10.1109/VAST.2006.261424","Link":"http://dx.doi.org/10.1109/VAST.2006.261424","FirstPage":183,"LastPage":190,"PaperType":"C","Abstract":"Real-world data is known to be imperfect, suffering from various forms of defects such as sensor variability, estimation errors, uncertainty, human errors in data entry, and gaps in data gathering. Analysis conducted on variable quality data can lead to inaccurate or incorrect results. An effective visualization system must make users aware of the quality of their data by explicitly conveying not only the actual data content, but also its quality attributes. While some research has been conducted on visualizing uncertainty in spatio-temporal data and univariate data, little work has been reported on extending this capability into multivariate data visualization. In this paper we describe our approach to the problem of visually exploring multivariate data with variable quality. As a foundation, we propose a general approach to defining quality measures for tabular data, in which data may experience quality problems at three granularities: individual data values, complete records, and specific dimensions. We then present two approaches to visual mapping of quality information into display space. In particular, one solution embeds the quality measures as explicit values into the original dataset by regarding value quality and record quality as new data dimensions. The other solution is to superimpose the quality information within the data visualizations using additional visual variables. We also report on user studies conducted to assess alternate mappings of quality attributes to visual variables for the second method. In addition, we describe case studies that expose some of the advantages and disadvantages of these two approaches","AuthorNames-Deduped":"Zaixian Xie;Shiping Huang;Matthew O. Ward;Elke A. Rundensteiner","AuthorNames":"Zaixian Xie;Shiping Huang;Matthew O. Ward;Elke A. Rundensteiner","AuthorAffiliation":"Computer Science Department, Worcester Polytechnic Institute, xiezx@cs.wpi.edu;Computer Science Department, Worcester Polytechnic Institute, shiping@cs.wpi.edu;Computer Science Department, Worcester Polytechnic Institute, matt@cs.wpi.edu;Computer Science Department, Worcester Polytechnic Institute, rundenst@cs.wpi.edu","InternalReferences":"10.1109/VISUAL.2000.885679;10.1109/INFVIS.2002.1173145;10.1109/VISUAL.1995.485139;10.1109/INFVIS.2004.10","AuthorKeywords":"Uncertainty visualization, multivariate visualization,data quality","AminerCitationCount_02-2020":32,"AminerCitationCount_06-2020":40,"XploreCitationCount - 2020-01":16,"PubsCited":25,"Award":null,"image":"4035764-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"Semantic Image Browser: Bridging Information Visualization with Automated Intelligent Image Analysis","DOI":"10.1109/VAST.2006.261425","Link":"http://dx.doi.org/10.1109/VAST.2006.261425","FirstPage":191,"LastPage":198,"PaperType":"C","Abstract":"Browsing and retrieving images from large image collections are becoming common and important activities. Semantic image analysis techniques, which automatically detect high level semantic contents of images for annotation, are promising solutions toward this problem. However, few efforts have been made to convey the annotation results to users in an intuitive manner to enable effective image browsing and retrieval. There is also a lack of methods to monitor and evaluate the automatic image analysis algorithms due to the high dimensional nature of image data, features, and contents. In this paper, we propose a novel, scalable semantic image browser by applying existing information visualization techniques to semantic image analysis. This browser not only allows users to effectively browse and search in large image databases according to the semantic content of images, but also allows analysts to evaluate their annotation process through interactive visual exploration. The major visualization components of this browser are multi-dimensional scaling (MDS) based image layout, the value and relation (VaR) display that allows effective high dimensional visualization without dimension reduction, and a rich set of interaction tools such as search by sample images and content relationship detection. Our preliminary user study showed that the browser was easy to use and understand, and effective in supporting image browsing and retrieval tasks","AuthorNames-Deduped":"Jing Yang 0001;Jianping Fan 0001;Daniel Hubball;Yuli Gao;Hangzai Luo;William Ribarsky;Matthew O. Ward","AuthorNames":"Jing Yang;Jianping Fan;Daniel Hubball;Yuli Gao;Hangzai Luo;William Ribarsky;Matthew Ward","AuthorAffiliation":"Dept of Computer Science, University of North Carolina at Charlotte, jyang13@uncc.edu;Dept of Computer Science, University of North Carolina at Charlotte, jfan@uncc.edu;Dept of Computer Science, University of North Carolina at Charlotte, dhubball@uncc.edu;Dept of Computer Science, University of North Carolina at Charlotte, ygao@uncc.edu;Dept of Computer Science, University of North Carolina at Charlotte, hluo@uncc.edu;Dept of Computer Science, University of North Carolina at Charlotte, ribarsky@uncc.edu;Dept of Computer Science, Worcester Polytechnic, Institute. matt@cs.wpi.edu","InternalReferences":"10.1109/INFVIS.1999.801855;10.1109/INFVIS.1995.528686;10.1109/INFVIS.2003.1249009;10.1109/VISUAL.1995.485140;10.1109/INFVIS.2004.71","AuthorKeywords":"Image retrieval, image layout, semantic image classification,multi-dimensional visualization, visual analytics","AminerCitationCount_02-2020":51,"AminerCitationCount_06-2020":58,"XploreCitationCount - 2020-01":31,"PubsCited":23,"Award":null,"image":"4035765-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"NetLens: Iterative Exploration of Content-Actor Network Data","DOI":"10.1109/VAST.2006.261426","Link":"http://dx.doi.org/10.1109/VAST.2006.261426","FirstPage":91,"LastPage":98,"PaperType":"C","Abstract":"Networks have remained a challenge for information retrieval and visualization because of the rich set of tasks that users want to accomplish. This paper offers an abstract content-actor network data model, a classification of tasks, and a tool to support them. The NetLens interface was designed around the abstract content-actor network data model to allow users to pose a series of elementary queries and iteratively refine visual overviews and sorted lists. This enables the support of complex queries that are traditionally hard to specify. NetLens is general and scalable in that it applies to any dataset that can be represented with our abstract data model. This paper describes NetLens applying a subset of the ACM Digital Library consisting of about 4,000 papers from the CM I conference written by about 6,000 authors. In addition, we are now working on a collection of half a million emails, and a dataset of legal cases","AuthorNames-Deduped":"Hyunmo Kang;Catherine Plaisant;Bongshin Lee;Benjamin B. Bederson","AuthorNames":"Hyunmo Kang;Catherine Plaisant;Bongshin Lee;Benjamin B. Bederson","AuthorAffiliation":"University of Maryland Institute for Advanced Computer Studies, kang@cs.umd.edu;University of Maryland Institute for Advanced Computer Studies, plaisant@cs.umd.edu;Human-Computer Interaction Laboratory, Computer Science Department, bongshin@cs.umd.edu;Human-Computer Interaction Laboratory, Computer Science Department; University of Maryland Institute for Advanced Computer Studies, bederson@cs.umd.edu","InternalReferences":"10.1109/INFVIS.2004.1;10.1109/INFVIS.1996.559210;10.1109/INFVIS.2005.1532136","AuthorKeywords":"Human-Computer Interaction, information visualization, network visualization, content-actor network data, iterative query refinement, incremental data exploration, user interfaces, digital library, piccolo","AminerCitationCount_02-2020":29,"AminerCitationCount_06-2020":33,"XploreCitationCount - 2020-01":13,"PubsCited":29,"Award":null,"image":"4035752-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2006,"Title":"Avian Flu Case Study with nSpace and GeoTime","DOI":"10.1109/VAST.2006.261427","Link":"http://dx.doi.org/10.1109/VAST.2006.261427","FirstPage":27,"LastPage":34,"PaperType":"C","Abstract":"GeoTime and nSpace are new analysis tools that provide innovative visual analytic capabilities. This paper uses an epidemiology analysis scenario to illustrate and discuss these new investigative methods and techniques. In addition, this case study is an exploration and demonstration of the analytical synergy achieved by combining GeoTime's geo-temporal analysis capabilities, with the rapid information triage, scanning and sense-making provided by nSpace. A fictional analyst works through the scenario from the initial brainstorming through to a final collaboration and report. With the efficient knowledge acquisition and insights into large amounts of documents, there is more time for the analyst to reason about the problem and imagine ways to mitigate threats. The use of both nSpace and GeoTime initiated a synergistic exchange of ideas, where hypotheses generated in either software tool could be cross-referenced, refuted, and supported by the other tool","AuthorNames-Deduped":"Pascale Proulx;Sumeet Tandon;Adam Bodnar;David Schroh;Robert Harper 0002;William Wright","AuthorNames":"Pascale Proulx;Sumeet Tandon;Adam Bodnar;David Schroh;Robert Harper;William Wright","AuthorAffiliation":"Oculus Info Inc., email: pascale.proulx@oculusinfo.com;Oculus Info Inc., email: sumeet.tandon@oculusinfo.com;Oculus Info Inc., email: adam.bodnar@oculusinfo.com;Oculus Info Inc., email: david.schroh@oculusinfo.com;Oculus Info Inc., email: rob.harper@oculusinfo.com;Oculus Info Inc., email: bill.wright@oculusinfo.com","InternalReferences":"10.1109/INFVIS.2004.27","AuthorKeywords":"visual analytics, information visualization, human information interaction, sense making, geo-spatial information systems, temporal analysis, user centered design","AminerCitationCount_02-2020":18,"AminerCitationCount_06-2020":18,"XploreCitationCount - 2020-01":10,"PubsCited":22,"Award":null,"image":"4035744-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2006,"Title":"Visual Analysis of Historic Hotel Visitation Patterns","DOI":"10.1109/VAST.2006.261428","Link":"http://dx.doi.org/10.1109/VAST.2006.261428","FirstPage":35,"LastPage":42,"PaperType":"C","Abstract":"Understanding the space and time characteristics of human interaction in complex social networks is a critical component of visual tools for intelligence analysis, consumer behavior analysis, and human geography. Visual identification and comparison of patterns of recurring events is an essential feature of such tools. In this paper, we describe a tool for exploring hotel visitation patterns in and around Rebersburg, Pennsylvania from 1898-1900. The tool uses a wrapping spreadsheet technique, called reruns, to display cyclic patterns of geographic events in multiple overlapping natural and artificial calendars. Implemented as an improvise visualization, the tool is in active development through a iterative process of data collection, hypothesis, design, discovery, and evaluation in close collaboration with historical geographers. Several discoveries have inspired ongoing data collection and plans to expand exploration to include historic weather records and railroad schedules. Distributed online evaluations of usability and usefulness have resulted in numerous feature and design recommendations","AuthorNames-Deduped":"Chris Weaver;David Fyfe;Anthony C. Robinson;Deryck Holdsworth;Donna Peuquet;Alan M. MacEachren","AuthorNames":"Chris Weaver;David Fyfe;Anthony Robinson;Deryck Holdsworth;Donna Peuquet;Alan M. MacEachren","AuthorAffiliation":"The GeoVISTA Center and Department of Geography, The Pennsylvania State University, e-mail: cweaver@psu.edu;The GeoVISTA Center and Department of Geography, The Pennsylvania State University, e-mail: dfyfe@psu.edu;The GeoVISTA Center and Department of Geography, The Pennsylvania State University, e-mail: arobinson@psu.edu;The GeoVISTA Center and Department of Geography, The Pennsylvania State University, e-mail: dwh6@ems.psu.edu;The GeoVISTA Center and Department of Geography, The Pennsylvania State University, e-mail: peuquet@psu.edu;The GeoVISTA Center and Department of Geography, The Pennsylvania State University, e-mail: maceachren@psu.edu","InternalReferences":"10.1109/INFVIS.2004.12;10.1109/INFVIS.2002.1173155;10.1109/INFVIS.2004.64","AuthorKeywords":"Geovisualization, exploratory visualization, historical geography, coordinated multiple views, travel pattern analysis","AminerCitationCount_02-2020":20,"AminerCitationCount_06-2020":21,"XploreCitationCount - 2020-01":8,"PubsCited":18,"Award":null,"image":"4035745-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"D-Dupe: An Interactive Tool for Entity Resolution in Social Networks","DOI":"10.1109/VAST.2006.261429","Link":"http://dx.doi.org/10.1109/VAST.2006.261429","FirstPage":43,"LastPage":50,"PaperType":"C","Abstract":"Visualizing and analyzing social networks is a challenging problem that has been receiving growing attention. An important first step, before analysis can begin, is ensuring that the data is accurate. A common data quality problem is that the data may inadvertently contain several distinct references to the same underlying entity; the process of reconciling these references is called entity-resolution. D-Dupe is an interactive tool that combines data mining algorithms for entity resolution with a task-specific network visualization. Users cope with complexity of cleaning large networks by focusing on a small subnetwork containing a potential duplicate pair. The subnetwork highlights relationships in the social network, making the common relationships easy to visually identify. D-Dupe users resolve ambiguities either by merging nodes or by marking them distinct. The entity resolution process is iterative: as pairs of nodes are resolved, additional duplicates may be revealed; therefore, resolution decisions are often chained together. We give examples of how users can flexibly apply sequences of actions to produce a high quality entity resolution result. We illustrate and evaluate the benefits of D-Dupe on three bibliographic collections. Two of the datasets had already been cleaned, and therefore should not have contained duplicates; despite this fact, many duplicates were rapidly identified using D-Dupe's unique combination of entity resolution algorithms within a task-specific visual interface","AuthorNames-Deduped":"Mustafa Bilgic 0001;Louis Licamele;Lise Getoor;Ben Shneiderman","AuthorNames":"Mustafa Bilgic;Louis Licamele;Lise Getoor;Ben Shneiderman","AuthorAffiliation":"University of Maryland, College Park, MD. mbilgic@cs.umd.edu;University of Maryland, College Park, MD. licamele@cs.umd.edu;University of Maryland, College Park, MD. getoor@cs.umd.edu;University of Maryland, College Park, MD. ben@cs.umd.edu","InternalReferences":null,"AuthorKeywords":"Data cleaning and integration, user interfaces, visual analytics, visual data mining","AminerCitationCount_02-2020":54,"AminerCitationCount_06-2020":63,"XploreCitationCount - 2020-01":29,"PubsCited":28,"Award":null,"image":"4035746-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"Interactive Visual Synthesis of Analytic Knowledge","DOI":"10.1109/VAST.2006.261430","Link":"http://dx.doi.org/10.1109/VAST.2006.261430","FirstPage":51,"LastPage":58,"PaperType":"C","Abstract":"A visual investigation involves both the examination of existing information and the synthesis of new analytic knowledge. This is a progressive process in which newly synthesized knowledge becomes the foundation for future discovery. In this paper, we present a novel system supporting interactive, progressive synthesis of analytic knowledge. Here we use the term \"analytic knowledge\" to refer to concepts that a user derives from existing data along with the evidence supporting such concepts. Unlike existing visual analytic-tools, which typically support only exploration of existing information, our system offers two unique features. First, we support user-system cooperative visual synthesis of analytic knowledge from existing data. Specifically, users can visually define new concepts by annotating existing information, and refine partially formed concepts by linking additional evidence or manipulating related concepts. In response to user actions, our system can automatically manage the evolving corpus of synthesized knowledge and its corresponding evidence. Second, we support progressive visual analysis of synthesized knowledge. This feature allows analysts to visually explore both existing knowledge and synthesized knowledge, dynamically incorporating earlier analytic conclusions into the ensuing discovery process. We have applied our system to two complex but very different analytic applications. Our preliminary evaluation shows the promise of our work","AuthorNames-Deduped":"David Gotz;Michelle X. Zhou;Vikram Aggarwal","AuthorNames":"David Gotz;Michelle X. Zhou;Vikram Aggarwal","AuthorAffiliation":"IBM T.J. Watson Research Center, e-mail: dgotz@us.ibm.com;IBM T.J. Watson Research Center, e-mail: mzhou@us.ibm.com;IBM T.J. Watson Research Center, email: vikram@us.ibm.com","InternalReferences":"10.1109/INFVIS.2005.1532146;10.1109/INFVIS.2005.1532136;10.1109/INFVIS.2001.963287;10.1109/INFVIS.1996.559210","AuthorKeywords":"Visual Analytics, Intelligence analysis, Problem solving environments, Visual Knowledge Discovery","AminerCitationCount_02-2020":47,"AminerCitationCount_06-2020":50,"XploreCitationCount - 2020-01":24,"PubsCited":26,"Award":null,"image":"4035747-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"Visual Analysis of Conflicting Opinions","DOI":"10.1109/VAST.2006.261431","Link":"http://dx.doi.org/10.1109/VAST.2006.261431","FirstPage":59,"LastPage":66,"PaperType":"C","Abstract":"Understanding the nature and dynamics of conflicting opinions is a profound and challenging issue. In this paper we address several aspects of the issue through a study of more than 3,000 Amazon customer reviews of the controversial bestseller The Da Vinci Code, including 1,738 positive and 918 negative reviews. The study is motivated by critical questions such as: what are the differences between positive and negative reviews? What is the origin of a particular opinion? How do these opinions change over time? To what extent can differentiating features be identified from unstructured text? How accurately can these features predict the category of a review? We first analyze terminology variations in these reviews in terms of syntactic, semantic, and statistic associations identified by TermWatch and use term variation patterns to depict underlying topics. We then select the most predictive terms based on log likelihood tests and demonstrate that this small set of terms classifies over 70% of the conflicting reviews correctly. This feature selection process reduces the dimensionality of the feature space from more than 20,000 dimensions to a couple of hundreds. We utilize automatically generated decision trees to facilitate the understanding of conflicting opinions in terms of these highly predictive terms. This study also uses a number of visualization and modeling tools to identify not only what positive and negative reviews have in common, but also they differ and evolve over time","AuthorNames-Deduped":"Chaomei Chen;Fidelia Ibekwe-Sanjuan;Eric SanJuan;Chris Weaver","AuthorNames":"Chaomei Chen;Fidelia Ibekwe-SanJuan;Eric SanJuan;Chris Weaver","AuthorAffiliation":"Drexel University, USA. e-mail: chaomei.chen@cis.drexel.edu;Université de Lyon 3, France. e-mail: ibekwe@univ-lyon3.fr;Université d'Avignon, France. e-mail: eric.sanjuan@lia-univ-avignon.fr;Penn State University, USA. e-mail: cew15@psu.edu","InternalReferences":"10.1109/INFVIS.2002.1173155","AuthorKeywords":"Visual Analytics, Intelligence analysis, Problemsolving environments, Visual Knowledge Discovery","AminerCitationCount_02-2020":63,"AminerCitationCount_06-2020":71,"XploreCitationCount - 2020-01":37,"PubsCited":21,"Award":null,"image":"4035748-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"Have Green - A Visual Analytics Framework for Large Semantic Graphs","DOI":"10.1109/VAST.2006.261432","Link":"http://dx.doi.org/10.1109/VAST.2006.261432","FirstPage":67,"LastPage":74,"PaperType":"C","Abstract":"A semantic graph is a network of heterogeneous nodes and links annotated with a domain ontology. In intelligence analysis, investigators use semantic graphs to organize concepts and relationships as graph nodes and links in hopes of discovering key trends, patterns, and insights. However, as new information continues to arrive from a multitude of sources, the size and complexity of the semantic graphs will soon overwhelm an investigator's cognitive capacity to carry out significant analyses. We introduce a powerful visual analytics framework designed to enhance investigators' natural analytical capabilities to comprehend and analyze large semantic graphs. The paper describes the overall framework design, presents major development accomplishments to date, and discusses future directions of a new visual analytics system known as Have Green","AuthorNames-Deduped":"Pak Chung Wong;George Chin Jr.;Harlan Foote;Patrick Mackey;James J. Thomas","AuthorNames":"Pak Chung Wong;George Chin;Harlan Foote;Patrick Mackey;Jim Thomas","AuthorAffiliation":"Pacific Northwest National Laboratory, Email: pak.wong@pnl.gov;Pacific Northwest National Laboratory, Email: george.chin@pnl.gov;Pacific Northwest National Laboratory, Email: harlan.foote@pnl.gov;Pacific Northwest National Laboratory, Email: patrick.mackey@pnl.gov;Pacific Northwest National Laboratory, Email: jim.thomas@pnl.gov","InternalReferences":"10.1109/INFVIS.2003.1249014;10.1109/INFVIS.2005.1532131","AuthorKeywords":"Visual Analytics, Graph and Network Visualization, Information Analytics, Information Visualization","AminerCitationCount_02-2020":26,"AminerCitationCount_06-2020":26,"XploreCitationCount - 2020-01":12,"PubsCited":42,"Award":null,"image":"4035749-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"Exploring Large-Scale Video News via Interactive Visualization","DOI":"10.1109/VAST.2006.261433","Link":"http://dx.doi.org/10.1109/VAST.2006.261433","FirstPage":75,"LastPage":82,"PaperType":"C","Abstract":"In this paper, we have developed a novel visualization framework to enable more effective visual analysis of large-scale news videos, where keyframes and keywords are automatically extracted from news video clips and visually represented according to their interestingness measurement to help audiences rind news stories of interest at first glance. A computational approach is also developed to quantify the interestingness measurement of video clips. Our experimental results have shown that our techniques for intelligent news video analysis have the capacity to enable more effective visualization of large-scale news videos. Our news video visualization system is very useful for security applications and for general audiences to quickly find news topics of interest from among many channels","AuthorNames-Deduped":"Hangzai Luo;Jianping Fan 0001;Jing Yang 0001;William Ribarsky;Shin'ichi Satoh","AuthorNames":"Hangzai Luo;Jianping Fan;Jing Yang;William Ribarsky;Shin'ichi Satoh","AuthorAffiliation":"Department of Computer Science, UNC-Charlotte, Charlotte, NC, USA. e-mail: hluo@uncc.edu;Department of Computer Science, UNC-Charlotte, Charlotte, NC, USA. e-mail: jfan@uncc.edu;Department of Computer Science, UNC-Charlotte, Charlotte, NC, USA. e-mail: jyang13@uncc.edu;Department of Computer Science, UNC-Charlotte, Charlotte, NC, USA. e-mail: rebarsky@uncc.edu;National Institute of Informatics, Tokyo, Japan. e-mail: satoh@nii.ac.jp","InternalReferences":"10.1109/INFVIS.1998.729570;10.1109/INFVIS.2003.1249019;10.1109/VISUAL.1991.175815","AuthorKeywords":"News Visualization, Semantic Video Classification","AminerCitationCount_02-2020":27,"AminerCitationCount_06-2020":29,"XploreCitationCount - 2020-01":17,"PubsCited":25,"Award":null,"image":"4035750-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"Interactive Visualization and Analysis of Network and Sensor Data on Mobile Devices","DOI":"10.1109/VAST.2006.261434","Link":"http://dx.doi.org/10.1109/VAST.2006.261434","FirstPage":83,"LastPage":90,"PaperType":"C","Abstract":"Mobile devices are rapidly gaining popularity due to their small size and their wide range of functionality. With the constant improvement in wireless network access, they are an attractive option not only for day to day use. but also for in-field analytics by first responders in widespread areas. However, their limited processing, display, graphics and power resources pose a major challenge in developing effective applications. Nevertheless, they are vital for rapid decision making in emergencies when combined with appropriate analysis tools. In this paper, we present an efficient, interactive visual analytic system using a PDA to visualize network information from Purdue's Ross-Ade Stadium during football games as an example of in-held data analytics combined with text and video analysis. With our system, we can monitor the distribution of attendees with mobile devices throughout the stadium through their access of information and association/disassociation from wireless access points, enabling the detection of crowd movement and event activity. Through correlative visualization and analysis of synchronized video (instant replay video) and text information (play statistics) with the network activity, we can provide insightful information to network monitoring personnel, safety personnel and analysts. This work provides a demonstration and testbed for mobile sensor analytics that will help to improve network performance and provide safety personnel with information for better emergency planning and guidance","AuthorNames-Deduped":"Avin Pattath;Brian D. Bue;Yun Jang;David S. Ebert;Xuan Zhong;Aaron Ault;Edward J. Coyle","AuthorNames":"Avin Pattath;Brian Bue;Yun Jang;David Ebert;Xuan Zhong;Aaron Ault;Edward Coyle","AuthorAffiliation":"Purdue University Regional Visualization and Analytics Center (PURVAC), Purdue University, West Lafayette, IN. e-mail: apattath@purdue.edu;Purdue University Regional Visualization and Analytics Center (PURVAC), Purdue University, West Lafayette, IN. e-mail: bbue@purdue.edu;Purdue University Regional Visualization and Analytics Center (PURVAC), Purdue University, West Lafayette, IN. e-mail: jangy@purdue.edu;Purdue University Regional Visualization and Analytics Center (PURVAC), Purdue University, West Lafayette, IN. e-mail: ebertd@purdue.edu;Center for Wireless Systems and Applications (CWSA), Purdue University, West Lafayette, IN. e-mail: zhongx@purdue.edu;Center for Wireless Systems and Applications (CWSA), Purdue University, West Lafayette, IN. e-mail: aultac@purdue.edu;Center for Wireless Systems and Applications (CWSA), Purdue University, West Lafayette, IN. e-mail: coyle@purdue.edu","InternalReferences":"10.1109/INFVIS.2004.27;10.1109/VISUAL.2001.964496;10.1109/INFVIS.2005.1532135;10.1109/INFVIS.2005.1532131;10.1109/INFVIS.2000.885097","AuthorKeywords":"mobile visualization, network visualization, visual analytics","AminerCitationCount_02-2020":17,"AminerCitationCount_06-2020":20,"XploreCitationCount - 2020-01":10,"PubsCited":26,"Award":null,"image":"4035751-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"Interactive Wormhole Detection in Large Scale Wireless Networks","DOI":"10.1109/VAST.2006.261435","Link":"http://dx.doi.org/10.1109/VAST.2006.261435","FirstPage":99,"LastPage":106,"PaperType":"C","Abstract":"Wormhole attacks in wireless networks can severely deteriorate the network performance and compromise the security through spoiling the routing protocols and weakening the security enhancements. This paper develops an approach, interactive visualization of wormholes (IVoW), to monitor and detect such attacks in large scale wireless networks in real time. We characterize the topology features of a network under wormhole attacks through the node position changes and visualize the information at dynamically adjusted scales. We integrate an automatic detection algorithm with appropriate user interactions to handle complicated scenarios that include a large number of moving nodes and multiple worm-hole attackers. Various visual forms have been adopted to assist the understanding and analysis of the reconstructed network topology and improve the detection accuracy. Extended simulation has demonstrated that the proposed approach can effectively locate the fake neighbor connections without introducing many false alarms. IVoW does not require the wireless nodes to be equipped with any special hardware, thus avoiding any additional cost. The proposed approach demonstrates that interactive visualization can be successfully combined with network security mechanisms to greatly improve the intrusion detection capabilities","AuthorNames-Deduped":"Weichao Wang;Aidong Lu","AuthorNames":"Weichao Wang;Aidong Lu","AuthorAffiliation":"University of Kansas, e-mail: weichaow@eecs.ku.edu;University of North Carolina at Charlotte, e-mail: alul1@uncc.edu","InternalReferences":"10.1109/VISUAL.1996.567787;10.1109/VISUAL.2005.1532819;10.1109/INFVIS.2002.1173161;10.1109/INFVIS.2004.60","AuthorKeywords":"Interactive Detection, Wormhole Attacks, Visualization on Network Security, Wireless Networks, Topology Visualization","AminerCitationCount_02-2020":11,"AminerCitationCount_06-2020":11,"XploreCitationCount - 2020-01":6,"PubsCited":43,"Award":null,"image":"4035753-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"Enhancing Visual Analysis of Network Traffic Using a Knowledge Representation","DOI":"10.1109/VAST.2006.261436","Link":"http://dx.doi.org/10.1109/VAST.2006.261436","FirstPage":107,"LastPage":114,"PaperType":"C","Abstract":"This paper presents a network traffic analysis system that couples visual analysis with a declarative knowledge representation. The system supports multiple iterations of the sense-making loop of analytic reasoning by allowing users to save discoveries as they are found and to reuse them in future iterations. We show how the knowledge representation can be used to improve both the visual representations and the basic analytical tasks of filtering and changing level of detail. We describe how the system can be used to produce models of network patterns, and show results from classifying one day of network traffic in our laboratory","AuthorNames-Deduped":"Ling Xiao;John Gerth;Pat Hanrahan","AuthorNames":"Ling Xiao;John Gerth;Pat Hanrahan","AuthorAffiliation":"Stanford University, Email: lingxiao@cs.stanford.edu;Stanford University, Email: gerth@cs.stanford.edu;Stanford University, Email: hanrahan@cs.stanford.edu","InternalReferences":"10.1109/INFVIS.1996.559226","AuthorKeywords":"network traffic visualization, visual analysis","AminerCitationCount_02-2020":41,"AminerCitationCount_06-2020":44,"XploreCitationCount - 2020-01":27,"PubsCited":23,"Award":null,"image":"4035754-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"Accelerating Network Traffic Analytics Using Query-Driven Visualization","DOI":"10.1109/VAST.2006.261437","Link":"http://dx.doi.org/10.1109/VAST.2006.261437","FirstPage":115,"LastPage":122,"PaperType":"C","Abstract":"Realizing operational analytics solutions where large and complex data must be analyzed in a time-critical fashion entails integrating many different types of technology. This paper focuses on an interdisciplinary combination of scientific data management and visualization/analysis technologies targeted at reducing the time required for data filtering, querying, hypothesis testing and knowledge discovery in the domain of network connection data analysis. We show that use of compressed bitmap indexing can quickly answer queries in an interactive visual data analysis application, and compare its performance with two alternatives for serial and parallel filtering/querying on 2.5 billion records' worth of network connection data collected over a period of 42 weeks. Our approach to visual network connection data exploration centers on two primary factors: interactive ad-hoc and multiresolution query formulation and execution over n dimensions and visual display of the n-dimensional histogram results. This combination is applied in a case study to detect a distributed network scan and to then identify the set of remote hosts participating in the attack. Our approach is sufficiently general to be applied to a diverse set of data understanding problems as well as used in conjunction with a diverse set of analysis and visualization tools","AuthorNames-Deduped":"E. Wes Bethel;Scott Campbell;Eli Dart;Kurt Stockinger;Kesheng Wu","AuthorNames":"E. Wes Bethel;Scott Campbell;Eli Dart;Kurt Stockinger;Kesheng Wu","AuthorAffiliation":"Computational Research Division, e-mail: ewbethel@lbl.gov;National Energy Research Scientific Computing Center Division, e-mail: scampbell@lbl.gov;Energy Sciences Network, e-mail: dart@es.net;Computational Research Division, Lawrence Berkeley National Laboratory, University of California, Berkeley, CA 94720. e-mail: kstockinger@lbl.gov;Computational Research Division, Lawrence Berkeley National Laboratory, University of California, Berkeley, CA 94720. e-mail: kwu@lbl.gov","InternalReferences":"10.1109/VISUAL.1999.809930;10.1109/VISUAL.2005.1532792","AuthorKeywords":"query-driven visualization, network security, data mining, visual analytics","AminerCitationCount_02-2020":22,"AminerCitationCount_06-2020":37,"XploreCitationCount - 2020-01":19,"PubsCited":44,"Award":null,"image":"4035755-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"Monitoring Network Traffic with Radial Traffic Analyzer","DOI":"10.1109/VAST.2006.261438","Link":"http://dx.doi.org/10.1109/VAST.2006.261438","FirstPage":123,"LastPage":128,"PaperType":"C","Abstract":"Extensive spread of malicious code on the Internet and also within intranets has risen the user's concern about what kind of data is transferred between her or his computer and other hosts on the network. Visual analysis of this kind of information is a challenging task, due to the complexity and volume of the data type considered, and requires special design of appropriate visualization techniques. In this paper, we present a scalable visualization toolkit for analyzing network activity of computer hosts on a network. The visualization combines network packet volume and type distribution information with geographic information, enabling the analyst to use geographic distortion techniques such as the HistoMap technique to become aware of the traffic components in the course of the analysis. The presented analysis tool is especially useful to compare important network load characteristics in a geographically aware display, to relate communication partners, and to identify the type of network traffic occurring. The results of the analysis are helpful in understanding typical network communication activities, and in anticipating potential performance bottlenecks or problems. It is suited for both off-line analysis of historic data, and via animation for on-line monitoring of packet-based network traffic in real time","AuthorNames-Deduped":"Daniel A. Keim;Florian Mansmann;Jörn Schneidewind;Tobias Schreck","AuthorNames":"Daniel A. Keim;Florian Mansmann;Jorn Schneidewind;Tobias Schreck","AuthorAffiliation":"Databases, Data Mining and Visualization Group, University of Konstanz, Germany. keim@inf.uni-konstanz.de;Databases, Data Mining and Visualization Group, University of Konstanz, Germany. mansmann@inf.uni-konstanz.de;Databases, Data Mining and Visualization Group, University of Konstanz, Germany. schneide@inf.uni-konstanz.de;Databases, Data Mining and Visualization Group, University of Konstanz, Germany. schreck@inf.uni-konstanz.de","InternalReferences":"10.1109/INFVIS.2000.885091;10.1109/INFVIS.1998.729557","AuthorKeywords":"Visual Analytics, Network Traffic Monitoring, Information Visualization and Geography-based Solutions","AminerCitationCount_02-2020":52,"AminerCitationCount_06-2020":56,"XploreCitationCount - 2020-01":29,"PubsCited":24,"Award":null,"image":"4035756-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"Toward a Multi-Analyst, Collaborative Framework for Visual Analytics","DOI":"10.1109/VAST.2006.261439","Link":"http://dx.doi.org/10.1109/VAST.2006.261439","FirstPage":129,"LastPage":136,"PaperType":"C","Abstract":"We describe a framework for the display of complex, multidimensional data, designed to facilitate exploration, analysis, and collaboration among multiple analysts. This framework aims to support human collaboration by making it easier to share representations, to translate from one point of view to another, to explain arguments, to update conclusions when underlying assumptions change, and to justify or account for decisions or actions. Multidimensional visualization techniques are used with interactive, context-sensitive, and tunable graphs. Visual representations are flexibly generated using a knowledge representation scheme based on annotated logic; this enables not only tracking and fusing different viewpoints, but also unpacking them. Fusing representations supports the creation of multidimensional meta-displays as well as the translation or mapping from one point of view to another. At the same time, analysts also need to be able to unpack one another's complex chains of reasoning, especially if they have reached different conclusions, and to determine the implications, if any, when underlying assumptions or evidence turn out to be false. The framework enables us to support a variety of scenarios as well as to systematically generate and test experimental hypotheses about the impact of different kinds of visual representations upon interactive collaboration by teams of distributed analysts","AuthorNames-Deduped":"Susan Brennan;Klaus Mueller;Gregory J. Zelinsky;I. V. Ramakrishnan;David Scott Warren;Arie E. Kaufman","AuthorNames":"Susan E. Brennan;Klaus Mueller;Greg Zelinsky;IV Ramakrishnan;David S. Warren;Arie Kaufman","AuthorAffiliation":"Stony Brook University, Psychology Department, Stony Brook University, Stony Brook, NY 11794-3300, susan.brennan@sunysb.edu;Stony Brook University, Computer Science Department, Stony Brook University, Stony Brook, NY 11794-4400. mueller@cs.sunysb.edu;Stony Brook University, Psychology Department, Stony Brook University, Stony Brook, NY 11794-3300, gzelinsky@ms.cc.sunysb.edu;Stony Brook University, Computer Science Department, Stony Brook University, Stony Brook, NY 11794-4400. ram@cs.sunysb.edu;Stony Brook University, Computer Science Department, Stony Brook University, Stony Brook, NY 11794-4400. warren@cs.sunysb.edu;Stony Brook University, Computer Science Department, Stony Brook University, Stony Brook, NY 11794-4400. ari@cs.sunysb.edu","InternalReferences":null,"AuthorKeywords":"visual analytics, collaborative and distributed visualization, data management and knowledge representation, visual knowledge discovery","AminerCitationCount_02-2020":32,"AminerCitationCount_06-2020":36,"XploreCitationCount - 2020-01":25,"PubsCited":25,"Award":null,"image":"4035757-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"Time Tree: Exploring Time Changing Hierarchies","DOI":"10.1109/VAST.2006.261450","Link":"http://dx.doi.org/10.1109/VAST.2006.261450","FirstPage":3,"LastPage":10,"PaperType":"C","Abstract":"Intelligence analysis often involves the task of gathering information about an organization. Knowledge about individuals in an organization and their relationships, often represented as a hierarchical organization chart, is crucial for understanding the organization. However, it is difficult for intelligence analysts to follow all individuals in an organization. Existing hierarchy visualizations have largely focused on the visualization of fixed structures and can not effectively depict the evolution of a hierarchy over time. We introduce TimeTree, a novel visualization tool designed to enable exploration of a changing hierarchy. TimeTree enables analysts to navigate the history of an organization, identify events associated with a specific entity (visualized on a TimeSlider), and explore an aggregate view of an individual's career path (a CareerTree). We demonstrate the utility of TimeTree by investigating a set of scenarios developed by an expert intelligence analyst. The scenarios are evaluated using a real dataset composed of eighteen thousand career events from more than eight thousand individuals. Insights gained from this analysis are presented","AuthorNames-Deduped":"Stuart K. Card;Bongwon Suh;Bryan A. Pendleton;Bryan Heer;John W. Bodnar","AuthorNames":"Stuart K. Card;Bongwon Suh;Bryan A. Pendleton;Jeffrey Heer;John W. Bodnar","AuthorAffiliation":"Palo Alto Research Center, 3333 Coyote Hill Road Palo Alto, CA 94304. E-mail: card@parc.com;Palo Alto Research Center, 3333 Coyote Hill Road Palo Alto, CA 94304. E-mail: suh@parc.com;Palo Alto Research Center, 3333 Coyote Hill Road Palo Alto, CA 94304. E-mail: bpendlet@parc.com;University of California at Berkeley, jheer@cs.berkeley.edu;SAIC, john.w.bodnar@saic.com","InternalReferences":"10.1109/INFVIS.2003.1249010;10.1109/VISUAL.1991.175815","AuthorKeywords":"TimeTree, DOI Tree, tree visualization,organizational chart, timeseries data, visual analytics","AminerCitationCount_02-2020":41,"AminerCitationCount_06-2020":43,"XploreCitationCount - 2020-01":16,"PubsCited":29,"Award":null,"image":"4035741-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"Visual Exploration of Spatio-temporal Relationships for Scientific Data","DOI":"10.1109/VAST.2006.261451","Link":"http://dx.doi.org/10.1109/VAST.2006.261451","FirstPage":11,"LastPage":18,"PaperType":"C","Abstract":"Spatio-temporal relationships among features extracted from temporally-varying scientific datasets can provide useful information about the evolution of an individual feature and its interactions with other features. However, extracting such useful relationships without user guidance is cumbersome and often an error prone process. In this paper, we present a visual analysis system that interactively discovers such relationships from the trajectories of derived features. We describe analysis algorithms to derive various spatial and spatio-temporal relationships. A visual interface is presented using which the user can interactively select spatial and temporal extents to guide the knowledge discovery process. We show the usefulness of our proposed algorithms on datasets originating from computational fluid dynamics. We also demonstrate how the derived relationships can help in explaining the occurrence of critical events like merging and bifurcation of the vortices","AuthorNames-Deduped":"Bryan Mehta;Srinivasan Parthasarathy 0001;Raghu Machiraju","AuthorNames":"Sameep Mehta;Srinivasan Parthasarathy;Raghu Machiraju","AuthorAffiliation":"Computer Science & Engineering, The Ohio State University, mehtas@cse.ohio-state.edu;Computer Science & Engineering, The Ohio State University, srini@cse.ohio-state.edu;Computer Science & Engineering, The Ohio State University, raghu@cse.ohio-state.edu","InternalReferences":"10.1109/VISUAL.2002.1183789","AuthorKeywords":"Knowledge Discovery, Scientific Analytics, Trajectory Analysis, Feature Extraction, Spatio-temporal Predicates, Visual Analytics","AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":5,"XploreCitationCount - 2020-01":4,"PubsCited":28,"Award":null,"image":"4035742-fig-1-source-large.gif"},{"Conference":"VAST","Year":2006,"Title":"Visual Analytics of Paleoceanographic Conditions","DOI":"10.1109/VAST.2006.261452","Link":"http://dx.doi.org/10.1109/VAST.2006.261452","FirstPage":19,"LastPage":26,"PaperType":"C","Abstract":"Decade scale oceanic phenomena like El Nino are correlated with weather anomalies all over the globe. Only by understanding the events that produced the climatic conditions in the past will it be possible to forecast abrupt climate changes and prevent disastrous consequences for human beings and their environment. Paleoceanography research is a collaborative effort that requires the analysis of paleo time-series, which are obtained from a number of independent techniques and instruments and produced by a variety of different researchers and/or laboratories. The complexity of these phenomena that consist of massive, dynamic and often conflicting data can only be faced by means of analytical reasoning supported by a highly interactive visual interface. This paper presents an interactive visual analysis environment for paleoceanography that permits to gain insight into the paleodata and allow the control and steering of the analytical methods involved in the reconstruction of the climatic conditions of the past","AuthorNames-Deduped":"Roberto Therón","AuthorNames":"Roberto Theron","AuthorAffiliation":"Departamento de Informática y Automática, Universidad de Salamanca, e-mail: theron@usal.es","InternalReferences":null,"AuthorKeywords":"Infovis, parallel coordinates, multiple linked views,exploratory analysis","AminerCitationCount_02-2020":11,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":6,"PubsCited":23,"Award":null,"image":"4035743-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"InfoVis as Seen by the World Out There: 2007 in Review","DOI":"10.1109/VAST.2007.4388989","Link":"http://dx.doi.org/10.1109/VAST.2007.4388989","FirstPage":"x","LastPage":"x","PaperType":"M","Abstract":"How we as insiders see and understand InfoVis is quite different from how it is seen by most people in the world out there. Most people get only glimpses of what we do, and those glimpses rarely tell the story clearly. Think about the view of InfoVis that has been created in 2007 through marketing, blogs, and articles. This view is peppered with misperception. In this presentation, I'll take you on a tour of InfoVis' exposure in 2007: the highlights and the failures that have shaped the world's perception of our beloved and important work. The world needs what we do, but this need remains largely unsatisfied.","AuthorNames-Deduped":"Stephen Few","AuthorNames":"Stephen Few","AuthorAffiliation":"Perceptual Edge, University of California, Berkeley","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":1,"PubsCited":0,"Award":null,"image":""},{"Conference":"VAST","Year":2007,"Title":"Activity Analysis Using Spatio-Temporal Trajectory Volumes in Surveillance Applications","DOI":"10.1109/VAST.2007.4388990","Link":"http://dx.doi.org/10.1109/VAST.2007.4388990","FirstPage":3,"LastPage":10,"PaperType":"C","Abstract":"In this paper, we present a system to analyze activities and detect anomalies in a surveillance application, which exploits the intuition and experience of security and surveillance experts through an easy- to-use visual feedback loop. The multi-scale and location specific nature of behavior patterns in space and time is captured using a wavelet-based feature descriptor. The system learns the fundamental descriptions of the behavior patterns in a semi-supervised fashion by the higher order singular value decomposition of the space described by the training data. This training process is guided and refined by the users in an intuitive fashion. Anomalies are detected by projecting the test data into this multi-linear space and are visualized by the system to direct the attention of the user to potential problem spots. We tested our system on real-world surveillance data, and it satisfied the security concerns of the environment.","AuthorNames-Deduped":"Firdaus Janoos;Shantanu Singh;M. Okan Irfanoglu;Raghu Machiraju;Richard E. Parent","AuthorNames":"Firdaus Janoos;Shantanu Singh;Okan Irfanoglu;Raghu Machiraju;Richard Parent","AuthorAffiliation":"Dept. of Computer Science and Engineering, Ohio State University. e-mail: janoos@cse.ohio-state.edu;Dept. of Computer Science and Engineering, Ohio State University. e-mail: singhsh@cse.ohio-state.edu;Dept. of Computer Science and Engineering, Ohio State University. e-mail: irfanogl@cse.ohio-state.edu;Dept. of Computer Science and Engineering, Ohio State University. e-mail: raghu@cse.ohio-state.edu;Dept. of Computer Science and Engineering, Ohio State University. e-mail: parent@cse.ohio-state.edu","InternalReferences":"10.1109/TVCG.2006.194","AuthorKeywords":"wavelets, HOSVD, surveillance, anomaly detection, trajectory","AminerCitationCount_02-2020":13,"AminerCitationCount_06-2020":15,"XploreCitationCount - 2020-01":7,"PubsCited":29,"Award":null,"image":"4388990-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"FemaRepViz: Automatic Extraction and Geo-Temporal Visualization of FEMA National Situation Updates","DOI":"10.1109/VAST.2007.4388991","Link":"http://dx.doi.org/10.1109/VAST.2007.4388991","FirstPage":11,"LastPage":18,"PaperType":"C","Abstract":"An architecture for visualizing information extracted from text documents is proposed. In conformance with this architecture, a toolkit, FemaRepViz, has been implemented to extract and visualize temporal, geospatial, and summarized information from FEMA national update reports. Preliminary tests have shown satisfactory accuracy for FEMARepViz. A central component of the architecture is an entity extractor that extracts named entities like person names, location names, temporal references, etc. FEMARepViz is based on FactXtractor, an entity-extractor that works on text documents. The information extracted using FactXtractor is processed using GeoTagger, a geographical name disambiguation tool based on a novel clustering-based disambiguation algorithm. To extract relationships among entities, we propose a machine-learning based algorithm that uses a novel stripped dependency tree kernel. We illustrate and evaluate the usefulness of our system on the FEMA National Situation Updates. Daily reports are fetched by FEMARepViz from the FEMA website, segmented into coherent sections and each section is classified into one of several known incident types. We use concept Vista, Google maps and Google earth to visualize the events extracted from the text reports and allow the user to interactively filter the topics, locations, and time-periods of interest to create a visual analytics toolkit that is useful for rapid analysis of events reported in a large set of text documents.","AuthorNames-Deduped":"Chi-Chun Pan;Prasenjit Mitra","AuthorNames":"Chi-Chun Pan;Prasenjit Mitra","AuthorAffiliation":"The Pennsylvania State University. e-mail: julianpan@psu.edu;The Pennsylvania State University. e-mail: pmitra@ist.psu.edu","InternalReferences":null,"AuthorKeywords":"visual analytics, geo-temporal visualization, text processing, knowledge discovery, geospatial analytics","AminerCitationCount_02-2020":18,"AminerCitationCount_06-2020":20,"XploreCitationCount - 2020-01":9,"PubsCited":0,"Award":null,"image":""},{"Conference":"VAST","Year":2007,"Title":"Stories in GeoTime","DOI":"10.1109/VAST.2007.4388992","Link":"http://dx.doi.org/10.1109/VAST.2007.4388992","FirstPage":19,"LastPage":26,"PaperType":"C","Abstract":"A story is a powerful abstraction used by intelligence analysts to conceptualize threats and understand patterns as part of the analytical process. This paper demonstrates a system that detects geo-temporal patterns and integrates story narration to increase analytic sense-making cohesion in GeoTime. The GeoTime geo-temporal event visualization tool was augmented with a story system that uses narratives, hypertext linked visualizations, visual annotations, and pattern detection to create an environment for analytic exploration and communication, thereby assisting the analyst in identifying, extracting, arranging and presenting stories within the data The story system lets analysts operate at the story level with higher-level abstractions of data, such as behaviors and events, while staying connected to the evidence. The story system was developed and evaluated in collaboration with analysts.","AuthorNames-Deduped":"Ryan Eccles;Thomas Kapler;Robert Harper 0002;William Wright","AuthorNames":"Ryan Eccles;Thomas Kapler;Robert Harper;William Wright","AuthorAffiliation":"Oculus Info Inc. reccles@oculusinfo.com;Oculus Info Inc. tkapler@oculusinfo.com;Oculus Info Inc. rharper@oculusinfo.com;Oculus Info Inc. bwright@oculusinfo.com","InternalReferences":"10.1109/INFVIS.2004.27;10.1109/VAST.2006.261436","AuthorKeywords":"human information interaction, visual analytics, sense-making, narrative, pattern detection, story making, story telling","AminerCitationCount_02-2020":85,"AminerCitationCount_06-2020":106,"XploreCitationCount - 2020-01":36,"PubsCited":19,"Award":null,"image":"4388992-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"LAHVA: Linked Animal-Human Health Visual Analytics","DOI":"10.1109/VAST.2007.4388993","Link":"http://dx.doi.org/10.1109/VAST.2007.4388993","FirstPage":27,"LastPage":34,"PaperType":"C","Abstract":"Coordinated animal-human health monitoring can provide an early warning system with fewer false alarms for naturally occurring disease outbreaks, as well as biological, chemical and environmental incidents. This monitoring requires the integration and analysis of multi-field, multi-scale and multi-source data sets. In order to better understand these data sets, models and measurements at different resolutions must be analyzed. To facilitate these investigations, we have created an application to provide a visual analytics framework for analyzing both human emergency room data and veterinary hospital data. Our integrated visual analytic tool links temporally varying geospatial visualization of animal and human patient health information with advanced statistical analysis of these multi-source data. Various statistical analysis techniques have been applied in conjunction with a spatio-temporal viewing window. Such an application provides researchers with the ability to visually search the data for clusters in both a statistical model view and a spatio-temporal view. Our interface provides a factor specification/filtering component to allow exploration of causal factors and spread patterns. In this paper, we will discuss the application of our linked animal-human visual analytics (LAHVA) tool to two specific case studies. The first case study is the effect of seasonal influenza and its correlation with different companion animals (e.g., cats, dogs) syndromes. Here we use data from the Indiana Network for Patient Care (INPC) and Banfield Pet Hospitals in an attempt to determine if there are correlations between respiratory syndromes representing the onset of seasonal influenza in humans and general respiratory syndromes in cats and dogs. Our second case study examines the effect of the release of industrial wastewater in a community through companion animal surveillance.","AuthorNames-Deduped":"Ross Maciejewski;Benjamin Tyner;Yun Jang;Cheng Zheng;Rimma V. Nehme;David S. Ebert;William S. Cleveland;Mourad Ouzzani;Shaun J. Grannis;Lawrence T. Glickman","AuthorNames":"Ross Maciejewski;Benjamin Tyner;Yun Jang;Cheng Zheng;Rimma V. Nehme;David S. Ebert;William S. Cleveland;Mourad Ouzzani;Shaun J. Grannis;Lawrence T. Glickman","AuthorAffiliation":"Purdue University Regional Visualization and Analytics Center (PURVAC);Purdue University Regional Visualization and Analytics Center (PURVAC);Purdue University Regional Visualization and Analytics Center (PURVAC);Purdue University Regional Visualization and Analytics Center (PURVAC);Purdue University Regional Visualization and Analytics Center (PURVAC);Purdue University Regional Visualization and Analytics Center (PURVAC);Purdue University Regional Visualization and Analytics Center (PURVAC);Purdue University Regional Visualization and Analytics Center (PURVAC);Regenstrief Institute and Indiana University School of Medicine;Purdue University School of Veterinary Medicine Clinical Epidemiology Group","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":15,"AminerCitationCount_06-2020":17,"XploreCitationCount - 2020-01":10,"PubsCited":27,"Award":null,"image":"4388993-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"Visual Analytics on Mobile Devices for Emergency Response","DOI":"10.1109/VAST.2007.4388994","Link":"http://dx.doi.org/10.1109/VAST.2007.4388994","FirstPage":35,"LastPage":42,"PaperType":"C","Abstract":"Using mobile devices for visualization provides a ubiquitous environment for accessing information and effective decision making. These visualizations are critical in satisfying the knowledge needs of operators in areas as diverse as education, business, law enforcement, protective services, medical services, scientific discovery, and homeland security. In this paper, we present an efficient and interactive mobile visual analytic system for increased situational awareness and decision making in emergency response and training situations. Our system provides visual analytics with locational scene data within a simple interface tailored to mobile device capabilities. In particular, we focus on processing and displaying sensor network data for first responders. To verify our system, we have used simulated data of The Station nightclub fire evacuation.","AuthorNames-Deduped":"SungYe Kim;Yun Jang;Angela Mellema;David S. Ebert;Timothy W. Collins","AuthorNames":"Sung Ye Kim;Yun Jang;Angela Mellema;David S. Ebert;Timothy Collinss","AuthorAffiliation":"Purdue University Regional Visualization and Analytics Center (PURVAC), Purdue University, West Lafayette, IN. e-mail: inside@purdue.edu;Purdue University Regional Visualization and Analytics Center (PURVAC), Purdue University, West Lafayette, IN. e-mail: inside@purdue.edu, jangy@purdue.edu;Purdue University Regional Visualization and Analytics Center (PURVAC), Purdue University, West Lafayette, IN. e-mail: inside@purdue.edu, amellema@purdue.edu;Purdue University Regional Visualization and Analytics Center (PURVAC), Purdue University, West Lafayette, IN. e-mail: inside@purdue.edu, ebertd@purdue.edu;Purdue Homeland Security Institute (PHSI), Purdue University, West Lafayette, IN. e-mail: tfcollins@purdue.edu","InternalReferences":"10.1109/VAST.2006.261434","AuthorKeywords":"mobile visualization, visual analytics, emergency response","AminerCitationCount_02-2020":38,"AminerCitationCount_06-2020":44,"XploreCitationCount - 2020-01":24,"PubsCited":20,"Award":null,"image":"4388994-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"Visual Analytics Approach to User-Controlled Evacuation Scheduling","DOI":"10.1109/VAST.2007.4388995","Link":"http://dx.doi.org/10.1109/VAST.2007.4388995","FirstPage":43,"LastPage":50,"PaperType":"C","Abstract":"Application of the ideas of visual analytics is a promising approach to supporting decision making, in particular, where the problems have geographic (or spatial) and temporal aspects. Visual analytics may be especially helpful in time-critical applications, which pose hard challenges to decision support. We have designed a suite of tools to support transportation-planning tasks such as emergency evacuation of people from a disaster- affected area. The suite combines a tool for automated scheduling based on a genetic algorithm with visual analytics techniques allowing the user to evaluate tool results and direct its work. A transportation schedule, which is generated by the tool, is a complex construct involving geographical space, time, and heterogeneous objects (people and vehicles) with states and positions varying in time. We apply task-analytical approach to design techniques that could effectively support a human planner in the analysis of this complex information H. 1.2 [User/Machine Systems]: Human information processing - Visual Analytics; 1.6.9 [Visualization]: information visualization.","AuthorNames-Deduped":"Gennady L. Andrienko;Natalia V. Andrienko;Ulrich Bartling","AuthorNames":"Gennady Andrienko;Natalia Andrienko;Ulrich Bartling","AuthorAffiliation":"Fraunhofer Institute IAIS (Intelligent Analysis and Information Systems), Germany;Fraunhofer Institute IAIS (Intelligent Analysis and Information Systems), Germany;Fraunhofer Institute IAIS (Intelligent Analysis and Information Systems), Germany","InternalReferences":null,"AuthorKeywords":"Geovisualization, transportation planning, vehicle scheduling, task-centered design, coordinated multiple views","AminerCitationCount_02-2020":16,"AminerCitationCount_06-2020":17,"XploreCitationCount - 2020-01":3,"PubsCited":16,"Award":null,"image":"4388995-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"Thin Client Visualization","DOI":"10.1109/VAST.2007.4388996","Link":"http://dx.doi.org/10.1109/VAST.2007.4388996","FirstPage":51,"LastPage":58,"PaperType":"C","Abstract":"We have developed a Web 2.0 thin client visualization framework called GeoBoosttrade. Our framework focuses on geospatial visualization and using scalable vector graphics (SVG), AJAX, RSS and GeoRSS we have built a complete thin client component set. Our component set provides a rich user experience that is completely browser based. It includes maps, standard business charts, graphs, and time-oriented components. The components are live, interactive, linked, and support real time collaboration.","AuthorNames-Deduped":"Stephen G. Eick;M. Andrew Eick;Jesse Fugitt;Brian Horst;Maxim Khailo;Russell A. Lankenau","AuthorNames":"Stephen G. Eick;M. Andrew Eick;Jesse Fugitt;Brian Horst;Maxim Khailo;Russell A. Lankenau","AuthorAffiliation":"SSS Research, Inc, 600 S. Washington, Suite 100, Naperville, IL 60540. eick@sss-research.com;SSS Research, Inc, 600 S. Washington, Suite 100, Naperville, IL 60540;SSS Research, Inc, 600 S. Washington, Suite 100, Naperville, IL 60540;SSS Research, Inc, 600 S. Washington, Suite 100, Naperville, IL 60540;SSS Research, Inc, 600 S. Washington, Suite 100, Naperville, IL 60540;SSS Research, Inc, 600 S. Washington, Suite 100, Naperville, IL 60540","InternalReferences":null,"AuthorKeywords":"web 20, JavaScript, scalable vector graphics, visualization components, linked view visual analytics","AminerCitationCount_02-2020":19,"AminerCitationCount_06-2020":20,"XploreCitationCount - 2020-01":14,"PubsCited":9,"Award":null,"image":"4388996-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"IMAS: The Interactive Multigenomic Analysis System","DOI":"10.1109/VAST.2007.4388997","Link":"http://dx.doi.org/10.1109/VAST.2007.4388997","FirstPage":59,"LastPage":66,"PaperType":"C","Abstract":"This paper introduces a new Visual Analysis tool named IMAS (Interactive Multigenomic Analysis System), which combines common analysis tools such as Glimmer, BLAST, and Clustal-W into a unified Visual Analytic framework. IMAS displays the primary DNA sequence being analyzed by the biologist in a highly interactive, zoomable visual display. The user may analyze the sequence in a number of ways, and visualize these analyses in a coherent, sequence aligned form, with all related analysis products grouped together. This enables the user to rapidly perform analyses of DNA sequences without the need for tedious and error-prone cutting and pasting of sequence data from text files to and from web-based databases and data analysis services, as is now common practice.","AuthorNames-Deduped":"Chris Shaw 0002;Greg A. Dasch;Marina E. Eremeeva","AuthorNames":"Christopher D. Shaw;Gregory A. Dasch;Marina E. Eremeeva","AuthorAffiliation":"School of Interactive Arts & Technology, Simon Fraser University Surrey, Surrey, BC, Canada. shaw@sfu.ca;Viral and Rickettsial Zoonoses Branch, National Center for Infectious Diseases, Centers for Disease Control and Prevention, Atlanta, Georgia 30333. ged4@cdc.gov;Viral and Rickettsial Zoonoses Branch, National Center for Infectious Diseases, Centers for Disease Control and Prevention, Atlanta, Georgia 30333. mge6@cdc.gov","InternalReferences":null,"AuthorKeywords":"Bioinformatics, Visual Analytics","AminerCitationCount_02-2020":8,"AminerCitationCount_06-2020":8,"XploreCitationCount - 2020-01":5,"PubsCited":18,"Award":null,"image":"4388997-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"Balancing Interactive Data Management of Massive Data with Situational Awareness through Smart Aggregation","DOI":"10.1109/VAST.2007.4388998","Link":"http://dx.doi.org/10.1109/VAST.2007.4388998","FirstPage":67,"LastPage":74,"PaperType":"C","Abstract":"Designing a visualization system capable of processing, managing, and presenting massive data sets while maximizing the user's situational awareness (SA) is a challenging, but important, research question in visual analytics. Traditional data management and interactive retrieval approaches have often focused on solving the data overload problem at the expense of the user's SA. This paper discusses various data management strategies and the strengths and limitations of each approach in providing the user with SA. A new data management strategy, coined Smart Aggregation, is presented as a powerful approach to overcome the challenges of both massive data sets and maintaining SA. By combining automatic data aggregation with user-defined controls on what, how, and when data should be aggregated, we present a visualization system that can handle massive amounts of data while affording the user with the best possible SA. This approach ensures that a system is always usable in terms of both system resources and human perceptual resources. We have implemented our Smart Aggregation approach in a visual analytics system called VIAssist (Visual Assistant for Information Assurance Analysis) to facilitate exploration, discovery, and SA in the domain of Information Assurance.","AuthorNames-Deduped":"Daniel R. Tesone;John R. Goodall","AuthorNames":"Daniel R. Tesone;John R. Goodall","AuthorAffiliation":"Secure Decisions, a division of Applied Visions Inc. e-mail: dant@securedecisions.avi.com;Secure Decisions, a division of Applied Visions Inc. e-mail: johng@securedecisions.avi.com","InternalReferences":"10.1109/VAST.2006.261437;10.1109/VISUAL.2005.1532792;10.1109/INFVIS.2004.10","AuthorKeywords":"Data management, visual analytics, data retrieval, information visualization, smart aggregation, situational awareness","AminerCitationCount_02-2020":13,"AminerCitationCount_06-2020":13,"XploreCitationCount - 2020-01":10,"PubsCited":9,"Award":null,"image":"4388998-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2007,"Title":"ClusterSculptor: A Visual Analytics Tool for High-Dimensional Data","DOI":"10.1109/VAST.2007.4388999","Link":"http://dx.doi.org/10.1109/VAST.2007.4388999","FirstPage":75,"LastPage":82,"PaperType":"C","Abstract":"Cluster analysis (CA) is a powerful strategy for the exploration of high-dimensional data in the absence of a-priori hypotheses or data classification models, and the results of CA can then be used to form such models. But even though formal models and classification rules may not exist in these data exploration scenarios, domain scientists and experts generally have a vast amount of non-compiled knowledge and intuition that they can bring to bear in this effort. In CA, there are various popular mechanisms to generate the clusters, however, the results from their non- supervised deployment rarely fully agree with this expert knowledge and intuition. To this end, our paper describes a comprehensive and intuitive framework to aid scientists in the derivation of classification hierarchies in CA, using k-means as the overall clustering engine, but allowing them to tune its parameters interactively based on a non-distorted compact visual presentation of the inherent characteristics of the data in high- dimensional space. These include cluster geometry, composition, spatial relations to neighbors, and others. In essence, we provide all the tools necessary for a high-dimensional activity we call cluster sculpting, and the evolving hierarchy can then be viewed in a space-efficient radial dendrogram. We demonstrate our system in the context of the mining and classification of a large collection of millions of data items of aerosol mass spectra, but our framework readily applies to any high-dimensional CA scenario.","AuthorNames-Deduped":"Eun Ju Nam;Yiping Han;Klaus Mueller;Alla Zelenyuk;Dan Imre","AuthorNames":"Eun Ju Nam;Yiping Han;Klaus Mueller;Alla Zelenyuk;Dan Imre","AuthorAffiliation":"Stony Brook University. email: ejnam@cs.sunysb.edu;Stony Brook University. email: yiping@cs.sunysb.edu;Stony Brook University. email: mueller@cs.sunysb.edu;Pacific Northwest National Lab. email: alla.zelenyuk@pnl.gov;Imre Consulting. email: dimre2b@charter.net","InternalReferences":"10.1109/VISUAL.1997.663916;10.1109/INFVIS.2004.15;10.1109/INFVIS.1999.801859;10.1109/INFVIS.2004.68;10.1109/VISUAL.1990.146402","AuthorKeywords":"Visual Analytics, High-Dimensional Data, Visual Data Mining, Visualization in Earth/Space/ and Environmental Sciences","AminerCitationCount_02-2020":40,"AminerCitationCount_06-2020":51,"XploreCitationCount - 2020-01":33,"PubsCited":28,"Award":null,"image":"4388999-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2007,"Title":"Analysis Guided Visual Exploration of Multivariate Data","DOI":"10.1109/VAST.2007.4389000","Link":"http://dx.doi.org/10.1109/VAST.2007.4389000","FirstPage":83,"LastPage":90,"PaperType":"C","Abstract":"Visualization systems traditionally focus on graphical representation of information. They tend not to provide integrated analytical services that could aid users in tackling complex knowledge discovery tasks. Users' exploration in such environments is usually impeded due to several problems: 1) valuable information is hard to discover when too much data is visualized on the screen; 2) Users have to manage and organize their discoveries off line, because no systematic discovery management mechanism exists; 3) their discoveries based on visual exploration alone may lack accuracy; 4) and they have no convenient access to the important knowledge learned by other users. To tackle these problems, it has been recognized that analytical tools must be introduced into visualization systems. In this paper, we present a novel analysis-guided exploration system, called the nugget management system (NMS). It leverages the collaborative effort of human comprehensibility and machine computations to facilitate users' visual exploration processes. Specifically, NMS first extracts the valuable information (nuggets) hidden in datasets based on the interests of users. Given that similar nuggets may be re-discovered by different users, NMS consolidates the nugget candidate set by clustering based on their semantic similarity. To solve the problem of inaccurate discoveries, localized data mining techniques are applied to refine the nuggets to best represent the captured patterns in datasets. Lastly, the resulting well-organized nugget pool is used to guide users' exploration. To evaluate the effectiveness of NMS, we integrated NMS into Xmd- vTool, a freeware multivariate visualization system. User studies were performed to compare the users' efficiency and accuracy in finishing tasks on real datasets, with and without the help of NMS. Our user studies confirmed the effectiveness of NMS.","AuthorNames-Deduped":"Di Yang;Elke A. Rundensteiner;Matthew O. Ward","AuthorNames":"Di Yang;Elke A. Rundensteiner;Matthew O. Ward","AuthorAffiliation":"Worcester Polytechnic Institute. e-mail: diyang@wpi.edu;Worcester Polytechnic Institute. e-mail: rundenst@es.wpi.edu;Worcester Polytechnic Institute. e-mail: matt@cs.wpi.edu","InternalReferences":"10.1109/VISUAL.1994.346302;10.1109/VAST.2006.261415;10.1109/INFVIS.2004.71;10.1109/INFVIS.1997.636793;10.1109/VAST.2006.261430","AuthorKeywords":"Visual Analytics, Visual Knowledge Discovery, Discovery Management, Analysis Guided Exploration","AminerCitationCount_02-2020":32,"AminerCitationCount_06-2020":52,"XploreCitationCount - 2020-01":14,"PubsCited":27,"Award":null,"image":"4389000-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"Intelligent Visual Analytics Queries","DOI":"10.1109/VAST.2007.4389001","Link":"http://dx.doi.org/10.1109/VAST.2007.4389001","FirstPage":91,"LastPage":98,"PaperType":"C","Abstract":"Visualizations of large multi-dimensional data sets, occurring in scientific and commercial applications, often reveal interesting local patterns. Analysts want to identify the causes and impacts of these interesting areas, and they also want to search for similar patterns occurring elsewhere in the data set. In this paper we introduce the Intelligent Visual Analytics Query (IVQuery) concept that combines visual interaction with automated analytical methods to support analysts in discovering the special properties and relations of identified patterns. The idea of IVQuery is to interactively select focus areas in the visualization. Then, according to the characteristics of the selected areas, such as the data dimensions and records, IVQuery employs analytical methods to identify the relationships to other portions of the data set. Finally, IVQuery generates visual representations for analysts to view and refine the results. IVQuery has been applied successfully to different real-world data sets, such as data warehouse performance, product sales, and sever performance analysis, and demonstrates the benefits of this technique over traditional filtering and zooming techniques. The visual analytics query technique can be used with many different types of visual representation. In this paper we show how to use IVQuery with parallel coordinates, visual maps, and scatter plots.","AuthorNames-Deduped":"Ming C. Hao;Umeshwar Dayal;Daniel A. Keim;D. Morent;Jörn Schneidewind","AuthorNames":"Ming C. Hao;Umeshwar Dayal;Daniel A. Keim;Dominik Morent;Joern Schneidewind","AuthorAffiliation":"Hewlett Packard Laboratories, CA. Electronic Mail: Ming.hao@hp.com;Hewlett Packard Laboratories, CA. Electronic Mail: Umeshwar.dayal@hp.com;University of Konstanz, Germany. Electronic Mail: keim@informatik.uni-konstanz.de;University of Konstanz, Germany. Electronic Mail: Dominik.Morent@uni-konstanz.de;University of Konstanz, Germany. Electronic Mail: schneide@inf.uni-konstanz.de","InternalReferences":"10.1109/TVCG.2006.200;10.1109/INFVIS.1998.729559;10.1109/VISUAL.1990.146402;10.1109/INFVIS.2000.885086;10.1109/VISUAL.1994.346302","AuthorKeywords":"Visual Analytics Query, Similarity Queries, Interactive Queries","AminerCitationCount_02-2020":38,"AminerCitationCount_06-2020":40,"XploreCitationCount - 2020-01":23,"PubsCited":18,"Award":null,"image":"4389001-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"Point Placement by Phylogenetic Trees and its Application to Visual Analysis of Document Collections","DOI":"10.1109/VAST.2007.4389002","Link":"http://dx.doi.org/10.1109/VAST.2007.4389002","FirstPage":99,"LastPage":106,"PaperType":"C","Abstract":"The task of building effective representations to visualize and explore collections with moderate to large number of documents is hard. It depends on the evaluation of some distance measure among texts and also on the representation of such relationships in bi- dimensional spaces. In this paper we introduce an alternative approach for building visual maps of documents based on their content similarity, through reconstruction of phylogenetic trees. The tree is capable of representing relationships that allows the user to quickly recover information detected by the similarity metric. For a variety of text collections of different natures we show that we can achieve improved exploration capability and more clear visualization of relationships amongst documents.","AuthorNames-Deduped":"Ana M. Cuadros;Fernando Vieira Paulovich;Rosane Minghim;Guilherme P. Telles","AuthorNames":"Ana M. Cuadros;Fernando V. Paulovich;Rosane Minghim;Guilherme P. Telles","AuthorAffiliation":"Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, São Carlos, Brazil. e-mail: anamaria@icmc.usp.br;Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, São Carlos, Brazil. e-mail: paulovic@icmc.usp.br;Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, São Carlos, Brazil. e-mail: rminghim@icmc.usp.br;Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, São Carlos, Brazil. e-mail: gpt@icmc.usp.br","InternalReferences":"10.1109/INFVIS.1995.528686;10.1109/VISUAL.1996.567787","AuthorKeywords":"Document Visualization, Multidimensional Visualization, Document Analysis, Text Analytics, Phylogenetic Trees","AminerCitationCount_02-2020":25,"AminerCitationCount_06-2020":33,"XploreCitationCount - 2020-01":26,"PubsCited":29,"Award":null,"image":"4389002-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"Analyzing Large-Scale News Video Databases to Support Knowledge Visualization and Intuitive Retrieval","DOI":"10.1109/VAST.2007.4389003","Link":"http://dx.doi.org/10.1109/VAST.2007.4389003","FirstPage":107,"LastPage":114,"PaperType":"C","Abstract":"In this paper, we have developed a novel framework to enable more effective investigation of large-scale news video database via knowledge visualization. To relieve users from the burdensome exploration of well-known and uninteresting knowledge of news reports, a novel interestingness measurement for video news reports is presented to enable users to find news stories of interest at first glance and capture the relevant knowledge in large-scale video news databases efficiently. Our framework takes advantage of both automatic semantic video analysis and human intelligence by integrating with visualization techniques on semantic video retrieval systems. Our techniques on intelligent news video analysis and knowledge discovery have the capacity to enable more effective visualization and exploration of large-scale news video collections. In addition, news video visualization and exploration can provide valuable feedback to improve our techniques for intelligent news video analysis and knowledge discovery.","AuthorNames-Deduped":"Hangzai Luo;Jianping Fan 0001;Jing Yang 0001;William Ribarsky;Shin'ichi Satoh","AuthorNames":"Hangzai Luo;Jianping Fan;Jing Yang;William Ribarsky;Shin'ichi Satoh","AuthorAffiliation":"Software Engineering Institute, East China Normal University, Shanghai, China. e-mail: memcache@gmail.com;Department of Computer Science, UNC-Charlotte, Charlotte, NC, USA. e-mail: jfan@uncc.edu;Department of Computer Science, UNC-Charlotte, Charlotte, NC, USA. e-mail: jyang13@uncc.edu;Department of Computer Science, UNC-Charlotte, Charlotte, NC, USA. e-mail: ribarsky@uncc.edu;National Institute of Informatics, Tokyo, Japan. e-mail: satoh@nii.ac.jp","InternalReferences":"10.1109/INFVIS.1998.729570;10.1109/TVCG.2006.179;10.1109/INFVIS.1995.528686;10.1109/INFVIS.2000.885098;10.1109/VAST.2006.261433","AuthorKeywords":"Semantic Video Classification, Knowledge Discovery, Knowledge Visualization","AminerCitationCount_02-2020":14,"AminerCitationCount_06-2020":18,"XploreCitationCount - 2020-01":14,"PubsCited":16,"Award":null,"image":"4389003-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"Literature Fingerprinting: A New Method for Visual Literary Analysis","DOI":"10.1109/VAST.2007.4389004","Link":"http://dx.doi.org/10.1109/VAST.2007.4389004","FirstPage":115,"LastPage":122,"PaperType":"C","Abstract":"In computer-based literary analysis different types of features are used to characterize a text. Usually, only a single feature value or vector is calculated for the whole text. In this paper, we combine automatic literature analysis methods with an effective visualization technique to analyze the behavior of the feature values across the text. For an interactive visual analysis, we calculate a sequence of feature values per text and present them to the user as a characteristic fingerprint. The feature values may be calculated on different hierarchy levels, allowing the analysis to be done on different resolution levels. A case study shows several successful applications of our new method to known literature problems and demonstrates the advantage of our new visual literature fingerprinting.","AuthorNames-Deduped":"Daniel A. Keim;Daniela Oelke","AuthorNames":"Daniel A. Keim;Daniela Oelke","AuthorAffiliation":"University of Konstanz. e-mail: keim@inf.uni-konstanz.de;University of Konstanz. e-mail: oelke@inf.uni-konstanz.de","InternalReferences":null,"AuthorKeywords":"Visual literature analysis, visual analytics, literature fingerprinting","AminerCitationCount_02-2020":66,"AminerCitationCount_06-2020":75,"XploreCitationCount - 2020-01":38,"PubsCited":13,"Award":null,"image":"4389004-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"NewsLab: Exploratory Broadcast News Video Analysis","DOI":"10.1109/VAST.2007.4389005","Link":"http://dx.doi.org/10.1109/VAST.2007.4389005","FirstPage":123,"LastPage":130,"PaperType":"C","Abstract":"In this paper, we introduce NewsLab, an exploratory visualization approach for the analysis of large scale broadcast news video collections containing many thousands of news stories over extended periods of time. A river metaphor is used to depict the thematic changes of the news over time. An interactive lens metaphor allows the playback of fine-grained video segments selected through the river overview. Multi-resolution navigation is supported via a hierarchical time structure as well as a hierarchical theme structure. Themes can be explored hierarchically according to their thematic structure, or in an unstructured fashion using various ranking criteria. A rich set of interactions such as filtering, drill-down/roll-up navigation, history animation, and keyword based search are also provided. Our case studies show how this set of tools can be used to find emerging topics in the news, compare different broadcasters, or mine the news for topics of interest.","AuthorNames-Deduped":"Mohammad Ghoniem;Dongning Luo;Jing Yang 0001;William Ribarsky","AuthorNames":"Mohammad Ghoniem;Dongning Luo;Jing Yang;William Ribarsky","AuthorAffiliation":"UNC Charlotte. e-mail: mghoniem@uncc.edu;UNC Charlotte. e-mail: dluo2@uncc.edu;UNC Charlotte. e-mail: jyang13@uncc.edu;UNC Charlotte. e-mail: ribarsky@uncc.edu","InternalReferences":"10.1109/VAST.2006.261433;10.1109/VAST.2006.261425;10.1109/VISUAL.1996.568118;10.1109/INFVIS.1998.729559;10.1109/INFVIS.1999.801858;10.1109/INFVIS.2005.1532122","AuthorKeywords":"Large data exploration, broadcast video analysis, time filtering, clustering, animation, comparative analysis","AminerCitationCount_02-2020":22,"AminerCitationCount_06-2020":23,"XploreCitationCount - 2020-01":12,"PubsCited":16,"Award":null,"image":"4389005-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2007,"Title":"Jigsaw: Supporting Investigative Analysis through Interactive Visualization","DOI":"10.1109/VAST.2007.4389006","Link":"http://dx.doi.org/10.1109/VAST.2007.4389006","FirstPage":131,"LastPage":138,"PaperType":"C","Abstract":"Investigative analysts who work with collections of text documents connect embedded threads of evidence in order to formulate hypotheses about plans and activities of potential interest. As the number of documents and the corresponding number of concepts and entities within the documents grow larger, sense-making processes become more and more difficult for the analysts. We have developed a visual analytic system called Jigsaw that represents documents and their entities visually in order to help analysts examine reports more efficiently and develop theories about potential actions more quickly. Jigsaw provides multiple coordinated views of document entities with a special emphasis on visually illustrating connections between entities across the different documents.","AuthorNames-Deduped":"John T. Stasko;Carsten Görg;Zhicheng Liu;Kanupriya Singhal","AuthorNames":"John Stasko;Carsten Gorg;Zhicheng Liu;Kanupriya Singhal","AuthorAffiliation":"School of Interactive Computing & GVU Center, Georgia Institute of Technology. e-mail: stasko@cc.gatech.edu;School of Interactive Computing & GVU Center, Georgia Institute of Technology. e-mail: goerg@cc.gatech.edu;School of Interactive Computing & GVU Center, Georgia Institute of Technology. e-mail: zcliu@cc.gatech.edu;School of Interactive Computing & GVU Center, Georgia Institute of Technology. e-mail: ksinghal@cc.gatech.edu","InternalReferences":"10.1109/INFVIS.1995.528686;10.1109/INFVIS.2004.27;10.1109/VAST.2006.261432","AuthorKeywords":"Visual analytics, investigative analysis, intelligence analysis, information visualization, multiple views","AminerCitationCount_02-2020":108,"AminerCitationCount_06-2020":147,"XploreCitationCount - 2020-01":64,"PubsCited":24,"Award":"TT","image":"4389006-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"SpiralView: Towards Security Policies Assessment through Visual Correlation of Network Resources with Evolution of Alarms","DOI":"10.1109/VAST.2007.4389007","Link":"http://dx.doi.org/10.1109/VAST.2007.4389007","FirstPage":139,"LastPage":146,"PaperType":"C","Abstract":"This article presents SpiralView, a visualization tool for helping system administrators to assess network policies. The tool is meant to be a complementary support to the routine activity of network monitoring, enabling a retrospective view on the alarms generated during and extended period of time. The tool permits to reason about how alarms distribute over time and how they correlate with network resources (e.g., users, IPs, applications, etc.), supporting the analysts in understanding how the network evolves and thus in devising new security policies for the future. The spiral visualization plots alarms in time, and, coupled with interactive bar charts and a users/applications graph view, is used to present network data and perform queries. The user is able to segment the data in meaningful subsets, zoom on specific related information, and inspect for relationships between alarms, users, and applications. In designing the visualizations and their interaction, and through tests with security experts, several ameliorations over the standard techniques have been provided.","AuthorNames-Deduped":"Enrico Bertini;Patrick Hertzog;Denis Lalanne","AuthorNames":"Enrico Bertini;Patrick Hertzog;Denis Lalanne","AuthorAffiliation":"University of Fribourg. enrico.bertini@unifr.ch;NEXThink S.A. patrick.hertzog@nexthink.com;University of Fribourg. denis.lalanne@unifr.ch","InternalReferences":"10.1109/INFVIS.2001.963273;10.1109/INFVIS.2001.963279;10.1109/INFVIS.2003.1249020","AuthorKeywords":"Network security, Intrusion Detection, Visualization, Data Exploration","AminerCitationCount_02-2020":28,"AminerCitationCount_06-2020":34,"XploreCitationCount - 2020-01":19,"PubsCited":25,"Award":null,"image":"4389007-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2007,"Title":"Session Viewer: Visual Exploratory Analysis of Web Session Logs","DOI":"10.1109/VAST.2007.4389008","Link":"http://dx.doi.org/10.1109/VAST.2007.4389008","FirstPage":147,"LastPage":154,"PaperType":"C","Abstract":"Large-scale session log analysis typically includes statistical methods and detailed log examinations. While both methods have merits, statistical methods can miss previously unknown sub- populations in the data and detailed analyses may have selection biases. We therefore built Session Viewer, a visualization tool to facilitate and bridge between statistical and detailed analyses. Taking a multiple-coordinated view approach, Session Viewer shows multiple session populations at the Aggregate, Multiple, and Detail data levels to support different analysis styles. To bridge between the statistical and the detailed analysis levels, Session Viewer provides fluid traversal between data levels and side-by-side comparison at all data levels. We describe an analysis of a large-scale web usage study to demonstrate the use of Session Viewer, where we quantified the importance of grouping sessions based on task type.","AuthorNames-Deduped":"Heidi Lam;Daniel M. Russell;Diane Tang;Tamara Munzner","AuthorNames":"Heidi Lam;Daniel Russell;Diane Tang;Tamara Munzner","AuthorAffiliation":"University of British Columbia, Google, Inc. e-mail: hllam@cs.ubc.ca;Google, Inc. e-mail: drussell@google.com;Google, Inc. e-mail: diane@google.com;University of British Columbia. e-mail: tmm@cs.ubc.ca","InternalReferences":"10.1109/INFVIS.1998.729553;10.1109/INFVIS.2003.1249006;10.1109/INFVIS.2004.2;10.1109/INFVIS.1996.559227","AuthorKeywords":"Web session log analysis, visual exploratory data analysis, information visualization","AminerCitationCount_02-2020":43,"AminerCitationCount_06-2020":51,"XploreCitationCount - 2020-01":25,"PubsCited":34,"Award":null,"image":"4389008-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"WireVis: Visualization of Categorical, Time-Varying Data From Financial Transactions","DOI":"10.1109/VAST.2007.4389009","Link":"http://dx.doi.org/10.1109/VAST.2007.4389009","FirstPage":155,"LastPage":162,"PaperType":"C","Abstract":"Large financial institutions such as Bank of America handle hundreds of thousands of wire transactions per day. Although most transactions are legitimate, these institutions have legal and financial obligations in discovering those that are suspicious. With the methods of fraudulent activities ever changing, searching on predefined patterns is often insufficient in detecting previously undiscovered methods. In this paper, we present a set of coordinated visualizations based on identifying specific keywords within the wire transactions. The different views used in our system depict relationships among keywords and accounts over time. Furthermore, we introduce a search-by-example technique which extracts accounts that show similar transaction patterns. In collaboration with the Anti-Money Laundering division at Bank of America, we demonstrate that using our tool, investigators are able to detect accounts and transactions that exhibit suspicious behaviors.","AuthorNames-Deduped":"Remco Chang;Mohammad Ghoniem;Robert Kosara;William Ribarsky;Jing Yang 0001;Evan A. Suma;Caroline Ziemkiewicz;Daniel A. Kern;Agus Sudjianto","AuthorNames":"Remco Chang;Mohammad Ghoniem;Robert Kosara;William Ribarsky;Jing Yang;Evan Suma;Caroline Ziemkiewicz;Daniel Kern;Agus Sudjianto","AuthorAffiliation":"UNC Charlotte. e-mail: rchang@uncc.edu;UNC Charlotte. e-mail: mghoniem@uncc.edu;UNC Charlotte. e-mail: rkosara@uncc.edu;UNC Charlotte. e-mail: ribarsky@uncc.edu;UNC Charlotte. e-mail: jyang13@uncc.edu;UNC Charlotte. e-mail: easuma@uncc.edu;UNC Charlotte. e-mail: caziemki@uncc.edu;Bank of America. e-mail: daniel.c.kern@bankofamerica.com;Bank of America. e-mail: agus.sudjianto@bankofamerica.com","InternalReferences":"10.1109/INFVIS.1999.801851;10.1109/INFVIS.1995.528686;10.1109/TVCG.2006.160;10.1109/INFVIS.2004.46","AuthorKeywords":"Fraud detection, financial data visualization, categorial and time-varying data","AminerCitationCount_02-2020":73,"AminerCitationCount_06-2020":85,"XploreCitationCount - 2020-01":56,"PubsCited":21,"Award":null,"image":"4389009-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"Us vs. Them: Understanding Social Dynamics in Wikipedia with Revert Graph Visualizations","DOI":"10.1109/VAST.2007.4389010","Link":"http://dx.doi.org/10.1109/VAST.2007.4389010","FirstPage":163,"LastPage":170,"PaperType":"C","Abstract":"Wikipedia is a wiki-based encyclopedia that has become one of the most popular collaborative on-line knowledge systems. As in any large collaborative system, as Wikipedia has grown, conflicts and coordination costs have increased dramatically. Visual analytic tools provide a mechanism for addressing these issues by enabling users to more quickly and effectively make sense of the status of a collaborative environment. In this paper we describe a model for identifying patterns of conflicts in Wikipedia articles. The model relies on users' editing history and the relationships between user edits, especially revisions that void previous edits, known as \"reverts\". Based on this model, we constructed Revert Graph, a tool that visualizes the overall conflict patterns between groups of users. It enables visual analysis of opinion groups and rapid interactive exploration of those relationships via detail drill- downs. We present user patterns and case studies that show the effectiveness of these techniques, and discuss how they could generalize to other systems.","AuthorNames-Deduped":"Bongwon Suh;Ed H. Chi;Bryan A. Pendleton;Aniket Kittur","AuthorNames":"Bongwon Suh;Ed H. Chi;Bryan A. Pendleton;Aniket Kittur","AuthorAffiliation":"Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304. Email: suh@parc.com;Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304. Email: echi@parc.com;Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304. Email: bp@parc.com;Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304. Email: nkittur@ucla.edu","InternalReferences":"10.1109/TVCG.2006.122;10.1109/INFVIS.2005.1532126","AuthorKeywords":"Wikipedia, wiki, revert, graph, collaboration, user model, visualization","AminerCitationCount_02-2020":65,"AminerCitationCount_06-2020":71,"XploreCitationCount - 2020-01":22,"PubsCited":47,"Award":null,"image":"4389010-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"Design Considerations for Collaborative Visual Analytics","DOI":"10.1109/VAST.2007.4389011","Link":"http://dx.doi.org/10.1109/VAST.2007.4389011","FirstPage":171,"LastPage":178,"PaperType":"C","Abstract":"Information visualization leverages the human visual system to support the process of sensemaking, in which information is collected, organized, and analyzed to generate knowledge and inform action. Though most research to date assumes a single-user focus on perceptual and cognitive processes, in practice, sensemaking is often a social process involving parallelization of effort, discussion, and consensus building. This suggests that to fully support sensemaking, interactive visualization should also support social interaction. However, the most appropriate collaboration mechanisms for supporting this interaction are not immediately clear. In this article, we present design considerations for asynchronous collaboration in visual analysis environments, highlighting issues of work parallelization, communication, and social organization. These considerations provide a guide for the design and evaluation of collaborative visualization systems.","AuthorNames-Deduped":"Jeffrey Heer;Maneesh Agrawala","AuthorNames":"Jeffrey Heer;Maneesh Agrawala","AuthorAffiliation":"University of California, Soda Hall, UC Berkeley, Berkeley, CA 94720-1776. E-Mail: jheer@cs.berkeley.edu;University of California, Soda Hall, UC Berkeley, Berkeley, CA 94720-1776. E-Mail: maneesh@cs.berkeley.edu","InternalReferences":"10.1109/VISUAL.1991.175820;10.1109/TVCG.2006.178;10.1109/TVCG.2006.202;10.1109/VAST.2006.261439","AuthorKeywords":"visualization, analysis, collaboration, design, computer-supported cooperative work","AminerCitationCount_02-2020":184,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":24,"PubsCited":53,"Award":null,"image":"4389011-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"Visual Analysis of Controversy in User-generated Encyclopedias","DOI":"10.1109/VAST.2007.4389012","Link":"http://dx.doi.org/10.1109/VAST.2007.4389012","FirstPage":179,"LastPage":186,"PaperType":"C","Abstract":"Wikipedia is a large and rapidly growing Web-based collaborative authoring environment, where anyone on the Internet can create, modify, and delete pages about encyclopedic topics. A remarkable property of some Wikipedia pages is that they are written by up to thousands of authors who may have contradicting opinions. In this paper we show that a visual analysis of the \"who revises whom\"- network gives deep insight into controversies. We propose a set of analysis and visualization techniques that reveal the dominant authors of a page, the roles they play, and the alters they confront. Thereby we provide tools to understand how Wikipedia authors collaborate in the presence of controversy.","AuthorNames-Deduped":"Ulrik Brandes;Jürgen Lerner","AuthorNames":"Ulrik Brandes;Jurgen Lerner","AuthorAffiliation":"Department of Computer & Information Science, University of Konstanz. e-mail: Ulrik.Brandes@uni-konstanz.de;Department of Computer & Information Science, University of Konstanz. e-mail: lerner@inf.uni-konstanz.de","InternalReferences":"10.1109/VAST.2006.261431","AuthorKeywords":"Wikipedia, social network analysis, controversy","AminerCitationCount_02-2020":57,"AminerCitationCount_06-2020":63,"XploreCitationCount - 2020-01":5,"PubsCited":23,"Award":null,"image":"4389012-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"DataMeadow: A Visual Canvas for Analysis of Large-Scale Multivariate Data","DOI":"10.1109/VAST.2007.4389013","Link":"http://dx.doi.org/10.1109/VAST.2007.4389013","FirstPage":187,"LastPage":194,"PaperType":"C","Abstract":"Supporting visual analytics of multiple large-scale multidimensional datasets requires a high degree of interactivity and user control beyond the conventional challenges of visualizing such datasets. We present the DataMeadow, a visual canvas providing rich interaction for constructing visual queries using graphical set representations called DataRoses. A DataRose is essentially a starplot of selected columns in a dataset displayed as multivariate visualizations with dynamic query sliders integrated into each axis. The purpose of the DataMeadow is to allow users to create advanced visual queries by iteratively selecting and filtering into the multidimensional data. Furthermore, the canvas provides a clear history of the analysis that can be annotated to facilitate dissemination of analytical results to outsiders. Towards this end, the DataMeadow has a direct manipulation interface for selection, filtering, and creation of sets, subsets, and data dependencies using both simple and complex mouse gestures. We have evaluated our system using a qualitative expert review involving two researchers working in the area. Results from this review are favorable for our new method.","AuthorNames-Deduped":"Niklas Elmqvist;John T. Stasko;Philippas Tsigas","AuthorNames":"Niklas Elmqvist;John Stasko;Philippas Tsigas","AuthorAffiliation":"INRIA/LRI, Univ. Paris-Sud. e-mail: elm@lri.fr;Georgia Institute of Technology. e-mail: stasko@cc.gatech.edu;Chalmers University of Technology. e-mail: tsigas@cs.chalmers.se","InternalReferences":"10.1109/INFVIS.2000.885086;10.1109/VISUAL.1990.146386;10.1109/VISUAL.1991.175815;10.1109/INFVIS.2003.1249026;10.1109/VAST.2006.261439;10.1109/INFVIS.2005.1532139;10.1109/VAST.2006.261424;10.1109/VAST.2006.261452;10.1109/INFVIS.2005.1532136;10.1109/INFVIS.1997.636793;10.1109/VAST.2006.261422;10.1109/VAST.2006.261430;10.1109/INFVIS.2003.1249016;10.1109/VISUAL.1999.809866;10.1109/VISUAL.1990.146375","AuthorKeywords":"Multivariate data, visual analytics, parallel coordinates, dynamic queries, iterative analysis, starplot, small multiples","AminerCitationCount_02-2020":69,"AminerCitationCount_06-2020":72,"XploreCitationCount - 2020-01":18,"PubsCited":37,"Award":null,"image":"4389013-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"VAST to Knowledge: Combining tools for exploration and mining","DOI":"10.1109/VAST.2007.4389015","Link":"http://dx.doi.org/10.1109/VAST.2007.4389015","FirstPage":197,"LastPage":198,"PaperType":"M","Abstract":"The investigation of the VAST Contest collection provided a valuable test for text mining techniques. Our group has focused on creating analytical tools to unveil relevant patterns and to aid with the content navigation in such text collections. Our results show how such an approach, in combination with visualization techniques, can ease the discovery process especially when multiple tools founded on the same approach to data mining are used in complement to and in concert with one another.","AuthorNames-Deduped":"Loretta Auvil;Xavier Llorà;Duane Searsmith;Kelly Searsmith","AuthorNames":"Loretta Auvil;Xavier Llora;Duane Searsmith;Kelly Searsmith","AuthorAffiliation":"Automated Learning Group, National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign. e-mail: lauvil@uiuc.edu;Data Intensive Technologies and Applications, National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign. e-mail: xllora@ncsa.uiuc.edu;Automated Learning Group, National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign. e-mail: dsears@ncsa.uiuc.edu;Automated Learning Group, National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign. e-mail: ksearsmi@ncsa.uiuc.edu","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":1,"PubsCited":4,"Award":null,"image":"4389015-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"VAST 2007 Contest Interactive Poster: Data Analysis Using NdCore and REGGAE","DOI":"10.1109/VAST.2007.4389016","Link":"http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4389016","FirstPage":199,"LastPage":200,"PaperType":"M","Abstract":"ATS intelligent discovery analyzed the VAST 2007 contest data set using two of its proprietary applications, NdCore and REGGAE (relationship generating graph analysis engine). The paper describes these tools and how they were used to discover the contest's scenarios of wildlife law enforcement, endangered species issues, and ecoterrorism.","AuthorNames-Deduped":"Lynn Schwendiman;Jonathan McLean;Jonathan Larson","AuthorNames":"Lynn Schwendiman;Jonathan McLean;Jonathan Larson","AuthorAffiliation":"ATS Intelligent Discovery, 3505 NW Anderson Hill Road, Suite 200, Silverdale, WA 98383. lynn.schwendiman@atsid.com;ATS Intelligent Discovery, 3505 NW Anderson Hill Road, Suite 200, Silverdale, WA 98383. jonathan.mclean@atsid.com;ATS Intelligent Discovery, 3505 NW Anderson Hill Road, Suite 200, Silverdale, WA 98383. jonathan.larson@atsid.com","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":0,"Award":null,"image":"4389016-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"Visual Analytics with Jigsaw","DOI":"10.1109/VAST.2007.4389017","Link":"http://dx.doi.org/10.1109/VAST.2007.4389017","FirstPage":201,"LastPage":202,"PaperType":"M","Abstract":"This article briefly introduces the Jigsaw system and describes how we used it in analysis activities for the VAST '07 Contest. Jigsaw is a visual analytic system that provides multiple coordinated views to show connections between entities that are extracted from a collection of documents.","AuthorNames-Deduped":"Carsten Görg;Zhicheng Liu;Neel Parekh;Kanupriya Singhal;John T. Stasko","AuthorNames":"Carsten Gorg;Zhicheng Liu;Neel Parekh;Kanupriya Singhal;John Stasko","AuthorAffiliation":"School of Interactive Computing & GVU Center, Georgia Institute of Technology. e-mail: goerg@cc.gatech.edu;School of Interactive Computing & GVU Center, Georgia Institute of Technology. e-mail: zcliu@cc.gatech.edu;School of Interactive Computing & GVU Center, Georgia Institute of Technology. e-mail: justneel@gmail.com;School of Interactive Computing & GVU Center, Georgia Institute of Technology. e-mail: ksinghal@cc.gatech.edu;School of Interactive Computing & GVU Center, Georgia Institute of Technology. e-mail: stasko@cc.gatech.edu","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":9,"AminerCitationCount_06-2020":10,"XploreCitationCount - 2020-01":2,"PubsCited":3,"Award":null,"image":"4389017-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2007,"Title":"Something's \"Fishy\" at Global Ways and Gill Breeders - Analysis with nSpace and GeoTime","DOI":"10.1109/VAST.2007.4389018","Link":"http://dx.doi.org/10.1109/VAST.2007.4389018","FirstPage":203,"LastPage":204,"PaperType":"M","Abstract":"GeoTime and nSpace are two interactive visual analytics tools that support the process of analyzing massive and complex datasets. The two tools were used to examine and interpret the 2007 VAST contest dataset. This poster paper describes how the capabilities of the tools were used to facilitate and expedite every stage of an analyst workflow.","AuthorNames-Deduped":"Lynn Chien;Annie Tat;William Wright","AuthorNames":"Lynn Chien;Annie Tat;William Wright","AuthorAffiliation":"Oculus Info Inc.;Oculus Info Inc.;Oculus Info Inc.","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":3,"Award":null,"image":"4389018-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2007,"Title":"TextPlorer: An application supporting text analysis","DOI":"10.1109/VAST.2007.4389019","Link":"http://dx.doi.org/10.1109/VAST.2007.4389019","FirstPage":205,"LastPage":206,"PaperType":"M","Abstract":"TexPlorer is an integrated system for exploring and analyzing large amounts of text documents. The data processing modules of TexPlorer consist of named entity extraction, entity relation extraction, hierarchical clustering, and text summarization tools. Using a timeline tool, tree-view, table-view, and concept maps, TexPlorer provides an analytical interface for exploring a set of text documents from different perspectives and allows users to explore vast amount of text documents efficiently.","AuthorNames-Deduped":"Chi-Chun Pan;Anuj R. Jaiswal;Junyan Luo;Anthony C. Robinson","AuthorNames":"Chi-Chun Pan;Anuj R. Jaiswal;Junyan Luo;Anthony Robinson","AuthorAffiliation":"The Pennsylvania State University. e-mail: julianpan@psu.edu;The Pennsylvania State University. e-mail: arj135@psu.edu;The Pennsylvania State University. e-mail: jluo@psu.edu;The Pennsylvania State University. e-mail: acr181@psu.edu","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":3,"Award":null,"image":"4389019-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2007,"Title":"University of British Columbia & Simon Fraser University - The Bricolage","DOI":"10.1109/VAST.2007.4389020","Link":"http://dx.doi.org/10.1109/VAST.2007.4389020","FirstPage":207,"LastPage":208,"PaperType":"M","Abstract":"This abstract presents a<i>bricolage</i>approach to the 2007 VAST contest. The analytical process we used is presented across four stages of sensemaking. Several tools were used throughout our approach, and we present their strengths and weaknesses for specific aspects of the analytical process. In addition, we review the details of both individual and collaborative techniques for solving visual analytics problems.","AuthorNames-Deduped":"William Chao;Daniel Ha;Kevin I.-J. Ho;Linda T. Kaastra;Minjung Kim;Andrew Wade","AuthorNames":"William Chao;Daniel Ha;Kevin Ho;Linda Kaastra;Minjung Kim;Andrew Wade;Brian Fisher","AuthorAffiliation":"Simon Fraser University, SIAT; University of British Columbia, MAGIC;Simon Fraser University, SIAT; University of British Columbia, MAGIC;Simon Fraser University, SIAT; University of British Columbia, MAGIC;Simon Fraser University, SIAT; University of British Columbia, MAGIC;Simon Fraser University, SIAT; University of British Columbia, MAGIC;Simon Fraser University, SIAT; University of British Columbia, MAGIC. e-mail: andrewwade@ubcviscog.com;Faculty Sponsor, Simon Fraser University, SIAT; University of British Columbia, MAGIC","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":3,"Award":null,"image":""},{"Conference":"VAST","Year":2007,"Title":"VisPad: Integrating Visualization, Navigation and Synthesis","DOI":"10.1109/VAST.2007.4389021","Link":"http://dx.doi.org/10.1109/VAST.2007.4389021","FirstPage":209,"LastPage":210,"PaperType":"M","Abstract":"We present a new framework - VisPad - to support the user to revisit the visual exploration process, and to synthesize and disseminate information. It offers three integrated views. The data view allows the user to interactively explore the data. The navigation view captures the exploration process. It enables the user to revisit any particular state and reuse it. The knowledge view enables the user to record his/her findings and the relations between these findings.","AuthorNames-Deduped":"Yedendra Babu Shrinivasan;Jarke J. van Wijk","AuthorNames":"Yedendra B. Shrinivasan;Jarke J. van Wijk","AuthorAffiliation":"Dept. Mathematics and Computer Science, Technische Universiteit Eindhoven. e-mail: y.b.shrinivasan@tue.nl;Dept. Mathematics and Computer Science, Technische Universiteit Eindhoven. e-mail: vanwijk@win.tue.nl","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":2,"PubsCited":6,"Award":null,"image":"4389021-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"C-GROUP: A Visual Analytic Tool for Pairwise Analysis of Dynamic Group Membership","DOI":"10.1109/VAST.2007.4389022","Link":"http://dx.doi.org/10.1109/VAST.2007.4389022","FirstPage":211,"LastPage":212,"PaperType":"M","Abstract":"C-GROUP is a tool for analyzing dynamic group membership in social networks over time. Unlike most network visualization tools, which show the group structure within an entire network, or the group membership for a single actor, C-GROUP allows users to focus their analysis on a pair of individuals of interest. And unlike most dynamic social network visualization tools, which focus on the addition and deletion of nodes (actors) and edges (relationships) over time, C-GROUP focuses on changing group memberships over time. C-GROUP provides users with a flexible interface for defining (and redefining) groups interactively, and allows users to view the changing group memberships for the pair over time. This helps to highlight the similarities and differences between the individuals and their evolving group memberships. C-GROUP allows users to dynamically select the time granularity of the temporal evolution and supports two novel visual representations of the evolving group memberships. This flexibility gives users alternate views that are appropriate for different network sizes and provides users with different insights into the grouping behavior.","AuthorNames-Deduped":"Hyunmo Kang;Lise Getoor;Lisa Singh","AuthorNames":"Hyunmo Kang;Lise Getoor;Lisa Singh","AuthorAffiliation":"University of Maryland, College Park, MD 20742. kang@cs.umd.edu;University of Maryland, College Park, MD 20742. getoor@cs.umd.edu;Georgetown University, Washington, DC 20052. singh@cs.georgetown.edu","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":6,"AminerCitationCount_06-2020":6,"XploreCitationCount - 2020-01":2,"PubsCited":1,"Award":null,"image":"4389022-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2007,"Title":"Situation Awareness Tool for Global Argus","DOI":"10.1109/VAST.2007.4389023","Link":"http://dx.doi.org/10.1109/VAST.2007.4389023","FirstPage":213,"LastPage":214,"PaperType":"M","Abstract":"We present a visualization tool to enhance situation awareness for Global Argus, a system that tracks and detects indications and warnings of biological events in near real time. Because Global Argus generates massive amounts of data daily, its analysts often struggle to interpret the information. To overcome this problem, we have developed the Global Argus situation awareness tool (GASAT) using the InteleView/World Wind geographical information system. This tool allows users to visualize current and past events in a particular region, and thus to understand how events evolve over time. Combined with the other tools that we are developing, GASAT will contribute to enhanced situation awareness in the tracking and detection of biological events.","AuthorNames-Deduped":"Jae Choi;Sang-joon Lee;Sarah Gigitashvilli;James M. Wilson V","AuthorNames":"Jae Choi;Sang-joon Lee;Sarah Gigitashvilli;James Wilson","AuthorAffiliation":"Georgetown University. e-mail: choi@isis.georgetown.edu;Viznox, Inc. e-mail: sjlee@viznox.com;Georgetown University. e-mail: gigitashvili@isis.georgetown.edu;Georgetown University. e-mail: wilson@isis.georgetown.edu","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":1,"PubsCited":2,"Award":null,"image":"4389023-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"Spectra transformed for model-testing and visual exploration","DOI":"10.1109/VAST.2007.4389024","Link":"http://dx.doi.org/10.1109/VAST.2007.4389024","FirstPage":215,"LastPage":216,"PaperType":"M","Abstract":"The presence of highly tangled patterns in spectra and other serial data exacerbates the difficulty of performing visual comparison between a test model for a particular pattern and the data. The use of a simple map that plants peaks in the data directly onto their corresponding position in a residual plot with respect to a chosen test model not only retrieves the advantages of dynamic regression plotting, but in practical cases also causes patterns in the data to congregate in meaningful ways with respect to more than one reference curve in the plane. The technique is demonstrated on a polyphonic music signal.","AuthorNames-Deduped":"Palmyra Catravas","AuthorNames":"Palmyra Catravas","AuthorAffiliation":"ECE Department, Union College, Schenectady, NY. e-mail: catravap@union.edu","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":15,"Award":null,"image":"4389024-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"Formalizing Analytical Discourse in Visual Analytics","DOI":"10.1109/VAST.2007.4389025","Link":"http://dx.doi.org/10.1109/VAST.2007.4389025","FirstPage":217,"LastPage":218,"PaperType":"M","Abstract":"This paper presents a theory of analytical discourse and a formal model of the intentional structure of visual analytic reasoning process. Our model rests on the theory of collaborative discourse, and allows for cooperative human-machine communication in visual interactive dialogues. Using a sample discourse from a crisis management scenario, we demonstrated the utility of our theory in characterizing the discourse context and collaboration. In particular, we view analytical discourse as plans consisting of complex mental attitude towards analytical tasks and issues. Under this view, human reasoning and computational analysis become integral part of the collaborative plan that evolves through discourse.","AuthorNames-Deduped":"Guoray Cai","AuthorNames":"Guoray Cai","AuthorAffiliation":"College of Information Sciences and Technology, Penn State University, University Park, PA. Email: cai@ist.psu.edu","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":2,"PubsCited":6,"Award":null,"image":"4389025-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"Sunfall: A Collaborative Visual Analytics System for Astrophysics","DOI":"10.1109/VAST.2007.4389026","Link":"http://dx.doi.org/10.1109/VAST.2007.4389026","FirstPage":219,"LastPage":220,"PaperType":"M","Abstract":"Computational and experimental sciences produce and collect ever- larger and complex datasets, often in large-scale, multi-institution projects. The inability to gain insight into complex scientific phenomena using current software tools is a bottleneck facing virtually all endeavors of science. In this paper, we introduce Sunfall, a collaborative visual analytics system developed for the Nearby Supernova Factory, an international astrophysics experiment and the largest data volume supernova search currently in operation. Sunfall utilizes novel interactive visualization and analysis techniques to facilitate deeper scientific insight into complex, noisy, high-dimensional, high-volume, time-critical data. The system combines novel image processing algorithms, statistical analysis, and machine learning with highly interactive visual interfaces to enable collaborative, user-driven scientific exploration of supernova image and spectral data. Sunfall is currently in operation at the Nearby Supernova Factory; it is the first visual analytics system in production use at a major astrophysics project.","AuthorNames-Deduped":"Cecilia R. Aragon;Stephen J. Bailey;Sarah S. Poon;Karl J. Runge;Rollin C. Thomas","AuthorNames":"Cecilia R. Aragon;Stephen J. Bailey;Sarah Poon;Karl J. Runge;Rollin C. Thomas","AuthorAffiliation":"Lawrence Berkeley National Laboratory. E-Mail: aragon@hpcrd.lbl.gov;Lawrence Berkeley National Laboratory. E-Mail: sjbailey@lbl.gov;Space Sciences Laboratory. E-Mail: sspoon@lbl.gov;Space Sciences Laboratory. E-Mail: kjrunge@lbl.gov;Lawrence Berkeley National Laboratory. E-Mail: rcthomas@lbl.gov","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":11,"AminerCitationCount_06-2020":8,"XploreCitationCount - 2020-01":1,"PubsCited":4,"Award":null,"image":"4389026-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"Visual Analysis of Dynamic Networks with Geological Clustering","DOI":"10.1109/VAST.2007.4389027","Link":"http://dx.doi.org/10.1109/VAST.2007.4389027","FirstPage":221,"LastPage":222,"PaperType":"M","Abstract":"Many dynamic networks have associated geological information. Here we present two complementing visual analysis methods for such networks. The first one provides an overview with summerized information while the second one presents a more detailed view. The geological information is encoded in the network layout, which is designed to help maintain user's mental map. We also combined visualization with social network analysis to facilitate knowledge discovery, especially to understand network changes in the context overall evolution. Both methods are applied to the \"History of the FIFA World Cup Competition\" data set.","AuthorNames-Deduped":"Adel Ahmed;Xiaoyan Fu;Seok-Hee Hong;Quan Hoang Nguyen;Kai Xu 0003","AuthorNames":"Adel Ahmed;Xiaoyan Fu;Seok-Hee Hong;Quan Hoang Nguyen;Kai Xu","AuthorAffiliation":"School of Information Technologies, University of Sydney, Australia; National ICT Australia, Australia. e-mail: adel.ahmed@nicta.com.au;National ICT Australia, Australia. e-mail: xiaoyan.fu@nicta.com.au;School of Information Technologies, University of Sydney, Australia; National ICT Australia, Australia. e-mail: shhong@it.usyd.edu.au;School of Computer Sciences and Engineering, University of NSW, Australia. e-mail: quanhn@cse.unsw.edu.au;National ICT Australia, Australia. e-mail: kai.xu@nicta.com.au","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":1,"PubsCited":7,"Award":null,"image":"4389027-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"From Tasks to Tools: A Field Study in Collaborative Visual Analytics","DOI":"10.1109/VAST.2007.4389028","Link":"http://dx.doi.org/10.1109/VAST.2007.4389028","FirstPage":223,"LastPage":224,"PaperType":"M","Abstract":"This poster presents an exploratory field study of a VAST 2007 contest entry. We applied cognitive task analysis (CTA), grounded theory (GT), and activity theory (AT), to analysis of field notes and interviews from participants. Our results are described in the context of activity theory and sensemaking, two theoretical perspectives that we have found to be particularly useful in understanding analytic tasks.","AuthorNames-Deduped":"Daniel Ha;Minjung Kim;Andrew Wade;William Chao;Kevin I.-J. Ho;Linda T. Kaastra;Brian D. Fisher;John Dill","AuthorNames":"Daniel Ha;Minjung Kim;Andrew Wade;William O. Chao;Kevin Ho;Linda Kaastra;Brian Fisher;John Dill","AuthorAffiliation":"Simon Fraser University, SIAT; University of British Columbia, MAGIC. email: dhal@sfu.ca;Simon Fraser University, SIAT; University of British Columbia, MAGIC;Simon Fraser University, SIAT; University of British Columbia, MAGIC;Simon Fraser University, SIAT; University of British Columbia, MAGIC;Simon Fraser University, SIAT; University of British Columbia, MAGIC;Simon Fraser University, SIAT; University of British Columbia, MAGIC;Simon Fraser University, SIAT; University of British Columbia, MAGIC;Simon Fraser University, SIAT; University of British Columbia, MAGIC","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":0,"PubsCited":5,"Award":null,"image":""},{"Conference":"VAST","Year":2007,"Title":"Outlook for Visual Analytics Research Funding","DOI":"10.1109/VAST.2007.4389030","Link":"http://dx.doi.org/10.1109/VAST.2007.4389030","FirstPage":227,"LastPage":227,"PaperType":"M","Abstract":"Visual Analytics has become a rapidly growing field of study. It is also a field that is addressing very significant real world problems in homeland security, business analytics, emergency management, genetics and bioinformatics, investigative analysis, medical analytics, and other areas. For both these reasons, it is attracting new funding and will continue to do so in the future. Visual analytics has also become an international field, with significant research efforts in Canada, Europe, and Australia, as well as the U.S. There is significant new research funding in Canada and Germany with other efforts being discussed, including a major program sponsored by the European Union. The contributors to this panel are some of the primary thought leaders providing research funding or involved in setting up the funding apparatus. We have asked them to present their needs, funding programs, and expectations from the research community. They all come from different perspectives, different missions, and different expectations. They will present their views of the range of activity in both the U.S. and internationally and discuss what is coming. Come learn about these programs, initiatives, and plans, and how you can contribute.","AuthorNames-Deduped":"James J. Thomas;Daniel A. Keim;Joe Kielman;Larry Rosenblum","AuthorNames":"Jim Thomas;Daniel Keim;Joe Kielman;Larry Rosenblum","AuthorAffiliation":"Pacific Northwest National Laboratory;University of Konstanz;Department of Homeland Security;National Science Foundation","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":0,"Award":null,"image":""},{"Conference":"VAST","Year":2007,"Title":"VAST 2007 Contest - Blue Iguanodon","DOI":"10.1109/VAST.2007.4389032","Link":"http://dx.doi.org/10.1109/VAST.2007.4389032","FirstPage":231,"LastPage":232,"PaperType":"M","Abstract":"Visual analytics experts realize that one effective way to push the field forward and to develop metrics for measuring the performance of various visual analytics components is to hold an annual competition. The second visual analytics science and technology (VAST) contest was held in conjunction with the 2007 IEEE VAST symposium. In this contest participants were to use visual analytic tools to explore a large heterogeneous data collection to construct a scenario and find evidence buried in the data of illegal and terrorist activities that were occurring. A synthetic data set was made available as well as tasks. In this paper we describe some of the advances we have made from the first competition held in 2006.","AuthorNames-Deduped":"Georges G. Grinstein;Catherine Plaisant;Sharon J. Laskowski;Theresa A. O'Connell;Jean Scholtz;Mark A. Whiting","AuthorNames":"Georges Grinstein;Catherine Plaisant;Sharon Laskowski;Theresa O'Connell;Jean Scholtz;Mark Whiting","AuthorAffiliation":"University of Massachusetts, Lowell. grinstein@cs.uml.edu;University of Maryland. plaisant@cs.umd.edu;National Institute of Standards and Technology. sharon.laskowski@nist.gov;National Institute of Standards and Technology. toconnell@nist.gov;Pacific Northwest National Laboratory. jean.scholtz@pnl.com;Pacific Northwest National Laboratory. mark.a.whiting@pnl.gov","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":10,"AminerCitationCount_06-2020":11,"XploreCitationCount - 2020-01":7,"PubsCited":6,"Award":null,"image":"4389032-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"VAST 2007 Contest - Analysis with nSpace and GeoTime","DOI":"10.1109/VAST.2007.4389033","Link":"http://dx.doi.org/10.1109/VAST.2007.4389033","FirstPage":233,"LastPage":234,"PaperType":"M","Abstract":"GeoTime and nSpace are two interactive visual analytics tools that support the process of analyzing massive and complex datasets. The two tools were used to examine and interpret the 2007 VAST contest dataset. This paper describes how the capabilities of the tools were used to facilitate and expedite every stage of the analysis.","AuthorNames-Deduped":"Lynn Chien;Annie Tat;Thomas Kapler;Patricia Enns;Winniefried Kuan;William Wright","AuthorNames":"Lynn Chien;Annie Tat;Thomas Kapler;Patricia Enns;Winniefried Kuan;William Wright","AuthorAffiliation":"Oculus Info Inc.;Oculus Info Inc.;Oculus Info Inc.;Oculus Info Inc.;Oculus Info Inc.;Oculus Info Inc.","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":3,"Award":null,"image":"4389033-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"Jigsaw meets Blue Iguanodon - The VAST 2007 Contest","DOI":"10.1109/VAST.2007.4389034","Link":"http://dx.doi.org/10.1109/VAST.2007.4389034","FirstPage":235,"LastPage":236,"PaperType":"M","Abstract":"This article describes our use of the Jigsaw system in working on the VAST 2007 contest. Jigsaw provides multiple views of a document collection and the individual entities within those documents, with a particular focus on exposing connections between entities. We describe how we refined the identified entities in order to better facilitate Jigsaw's use and how the different views helped us to uncover key parts of the underlying plot.","AuthorNames-Deduped":"Carsten Görg;Zhicheng Liu;Neel Parekh;Kanupriyah Singhal;John T. Stasko","AuthorNames":"Carsten Gorg;Zhicheng Liu;Neel Parekh;Kanupriya Singhal;John Stasko","AuthorAffiliation":"School of Interactive Computing & GVU Center, Georgia Institute of Technology. e-mail: goerg@cc.gatech.edu;School of Interactive Computing & GVU Center, Georgia Institute of Technology. e-mail: zcliu@cc.gatech.edu;School of Interactive Computing & GVU Center, Georgia Institute of Technology. e-mail: justneel@gmail.com;School of Interactive Computing & GVU Center, Georgia Institute of Technology. e-mail: ksinghal@cc.gatech.edu;School of Interactive Computing & GVU Center, Georgia Institute of Technology. e-mail: stasko@cc.gatech.edu","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":12,"AminerCitationCount_06-2020":13,"XploreCitationCount - 2020-01":9,"PubsCited":2,"Award":null,"image":"4389034-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"VAST to Knowledge: Combining tools for exploration and mining","DOI":"10.1109/VAST.2007.4389035","Link":"http://dx.doi.org/10.1109/VAST.2007.4389035","FirstPage":237,"LastPage":238,"PaperType":"M","Abstract":"The investigation of the VAST Contest collection provided a valuable test for text mining techniques. Our group has focused on creating analytical tools to unveil relevant patterns and to aid with the content navigation in such text collections. Our results show how such an approach, in combination with visualization techniques, can ease the discovery process especially when multiple tools founded on the same approach to data mining are used in complement to and in concert with one another.","AuthorNames-Deduped":"Loretta Auvil;Xavier Llorà;Duane Searsmith;Kelly Searsmith","AuthorNames":"Loretta Auvil;Xavier Llora;Duane Searsmith;Kelly Searsmith","AuthorAffiliation":"Automated Learning Group, National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign. e-mail: lauvil@uiuc.edu;Data Intensive Technologies and Applications, National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign. e-mail: xllora@ncsa.uiuc.edu;Automated Learning Group, National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign. e-mail: dsears@ncsa.uiuc.edu;Automated Learning Group, National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign. e-mail: ksearsmi@ncsa.uiuc.edu","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":0,"PubsCited":4,"Award":null,"image":"4389035-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2007,"Title":"Intelligence Analysis Using Titan","DOI":"10.1109/VAST.2007.4389036","Link":"http://dx.doi.org/10.1109/VAST.2007.4389036","FirstPage":241,"LastPage":242,"PaperType":"M","Abstract":"The open source Titan informatics toolkit project, which extends the visualization toolkit (VTK) to include information visualization capabilities, is being developed by Sandia National Laboratories in collaboration with Kitware. The VAST Contest provided us with an opportunity to explore various ideas for constructing an analysis tool, while allowing us to exercise our architecture in the solution of a complex problem. As amateur analysts, we found the experience both enlightening and fun.","AuthorNames-Deduped":"Patricia Crossno;Brian N. Wylie;Andrew T. Wilson;John A. Greenfield;Eric T. Stanton;Timothy M. Shead;Lisa G. Ice;Kenneth Moreland;Jeffrey Baumes;Berk Geveci","AuthorNames":"Patricia Crossno;Brian Wylie;Andrew Wilson;John Greenfield;Eric Stanton;Timothy Shead;Lisa Ice;Kenneth Moreland;Jeffrey Baumes;Berk Geveci","AuthorAffiliation":"Sandia National Laboratories. Email: pjcross@sandia.gov;Sandia National Laboratories. Email: bnwylie@sandia.gov;Sandia National Laboratories. Email: atwilso@sandia.gov;Sandia National Laboratories. Email: jagreen@sandia.gov;Sandia National Laboratories. Email: etstant@sandia.gov;Sandia National Laboratories. Email: tshead@sandia.gov;Sandia National Laboratories. Email: lgice@sandia.gov;Sandia National Laboratories. Email: kmorel@sandia.gov;Kitware Inc. jeff.baumes@kitware.com;Kitware Inc. berk.geveci@kitware.com","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":3,"PubsCited":3,"Award":null,"image":"4389036-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2007,"Title":"VAST 2007 Contest TexPlorer","DOI":"10.1109/VAST.2007.4389037","Link":"http://dx.doi.org/10.1109/VAST.2007.4389037","FirstPage":243,"LastPage":244,"PaperType":"M","Abstract":"TexPlorer is an integrated system for exploring and analyzing vast amount of text documents. The data processing modules of TexPlorer consist of named entity extraction, entity relation extraction, hierarchical clustering, and text summarization tools. Using time line tool, tree-view, table-view, and concept maps, TexPlorer provides visualizations from different aspects and allows analysts to explore vast amount of text documents efficiently.","AuthorNames-Deduped":"Chi-Chun Pan;Anuj R. Jaiswal;Junyan Luo;Anthony C. Robinson;Prasenjit Mitra;Alan M. MacEachren;Ian Turton","AuthorNames":"Chi-Chun Pan;Anuj R. Jaiswal;Junyan Luo;Anthony Robinson;Prasenjit Mitra;Alan M. MacEachren;Ian Turton","AuthorAffiliation":"The Pennsylvania State University. e-mail: julianpan@psu.edu;The Pennsylvania State University. e-mail: arj135@psu.edu;The Pennsylvania State University. e-mail: jluo@psu.edu;The Pennsylvania State University. e-mail: acr181@psu.edu;The Pennsylvania State University. e-mail: pmitra@ist.psu.edu;The Pennsylvania State University. e-mail: maceachren@psu.edu;The Pennsylvania State University. e-mail: ijt1@psu.edu","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":0,"PubsCited":3,"Award":null,"image":"4389037-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2007,"Title":"VAST 2007 Contest Data Analysis Using NdCore and REGGAE","DOI":"10.1109/VAST.2007.4389038","Link":"http://dx.doi.org/10.1109/VAST.2007.4389038","FirstPage":245,"LastPage":246,"PaperType":"M","Abstract":"ATS Intelligent Discovery analyzed the VAST 2007 contest data set using two of its proprietary applications, NdCore and REGGAE (Relationship Generating Graph Analysis Engine). The paper describes these tools and how they were used to discover the contest's scenarios of wildlife law enforcement, endangered species issues, and ecoterrorism.","AuthorNames-Deduped":"Lynn Schwendiman;Jonathan McLean;Jonathan Larson","AuthorNames":"Lynn Schwendiman;Jonathan McLean;Jonathan Larson","AuthorAffiliation":"ATS Intelligent Discovery, 3505 NW Anderson Hill Road, Suite 200, Silverdale, WA 98383. lynn.schwendiman@atsid.com;ATS Intelligent Discovery, 3505 NW Anderson Hill Road, Suite 200, Silverdale, WA 98383. jonathan.mclean@atsid.com;ATS Intelligent Discovery, 3505 NW Anderson Hill Road, Suite 200, Silverdale, WA 98383. jonathan.larson@atsid.com","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":0,"PubsCited":1,"Award":null,"image":"4389038-fig-1-source-large.gif"},{"Conference":"VAST","Year":2007,"Title":"University of British Columbia & Simon Fraser University - The Bricolage","DOI":"10.1109/VAST.2007.4470207","Link":"http://dx.doi.org/10.1109/VAST.2007.4470207","FirstPage":239,"LastPage":240,"PaperType":"M","Abstract":null,"AuthorNames-Deduped":"William Chao;Daniel Ha;Kevin I.-J. Ho;Linda T. Kaastra;Minjung Kim;Andrew Wade;Brian D. Fisher","AuthorNames":"William Chao;Daniel Ha;Kevin Ho;Linda Kaastra;Minjung Kim;Andrew Wade;Brian Fisher","AuthorAffiliation":null,"InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":3,"Award":null,"image":""},{"Conference":"VAST","Year":2008,"Title":"Visual cluster analysis of trajectory data with interactive Kohonen Maps","DOI":"10.1109/VAST.2008.4677350","Link":"http://dx.doi.org/10.1109/VAST.2008.4677350","FirstPage":3,"LastPage":10,"PaperType":"C","Abstract":"Visual-interactive cluster analysis provides valuable tools for effectively analyzing large and complex data sets. Due to desirable properties and an inherent predisposition for visualization, the Kohonen Feature Map (or self-organizing map, or SOM) algorithm is among the most popular and widely used visual clustering techniques. However, the unsupervised nature of the algorithm may be disadvantageous in certain applications. Depending on initialization and data characteristics, cluster maps (cluster layouts) may emerge that do not comply with user preferences, expectations, or the application context. Considering SOM-based analysis of trajectory data, we propose a comprehensive visual-interactive monitoring and control framework extending the basic SOM algorithm. The framework implements the general Visual Analytics idea to effectively combine automatic data analysis with human expert supervision. It provides simple, yet effective facilities for visually monitoring and interactively controlling the trajectory clustering process at arbitrary levels of detail. The approach allows the user to leverage existing domain knowledge and user preferences, arriving at improved cluster maps. We apply the framework on a trajectory clustering problem, demonstrating its potential in combining both unsupervised (machine) and supervised (human expert) processing, in producing appropriate cluster results.","AuthorNames-Deduped":"Tobias Schreck;Jürgen Bernard;Tatiana von Landesberger;Jörn Kohlhammer","AuthorNames":"Tobias Schreck;Jurgen Bernard;Tatiana Tekusova;Jorn Kohlhammer","AuthorAffiliation":"Interactive Graphics Systems Group, TU Darmstadt, Germany;Interactive Graphics Systems Group, TU Darmstadt, Germany;Interactive Graphics Systems Group, TU Darmstadt, Germany;Fraunhofer Institute for Computer Graphics IGD, Darmstadt, Germany","InternalReferences":"10.1109/TVCG.2007.70621","AuthorKeywords":null,"AminerCitationCount_02-2020":65,"AminerCitationCount_06-2020":70,"XploreCitationCount - 2020-01":25,"PubsCited":21,"Award":null,"image":"4677350-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Crystal structures classifier for an evolutionary algorithm structure predictor","DOI":"10.1109/VAST.2008.4677351","Link":"http://dx.doi.org/10.1109/VAST.2008.4677351","FirstPage":11,"LastPage":18,"PaperType":"C","Abstract":"USPEX is a crystal structure predictor based on an evolutionary algorithm. Every USPEX run produces hundreds or thousands of crystal structures, some of which may be identical. To ease the extraction of unique and potentially interesting structures we applied usual high-dimensional classification concepts to the unusual field of crystallography. We experimented with various crystal structure descriptors, distinct distance measures and tried different clustering methods to identify groups of similar structures. These methods are already applied in combinatorial chemistry to organic molecules for a different goal and in somewhat different forms, but are not widely used for crystal structures classification. We adopted a visual design and validation method in the development of a library (CrystalFp) and an end-user application to select and validate method choices, to gain userspsila acceptance and to tap into their domain expertise. The use of the classifier has already accelerated the analysis of USPEX output by at least one order of magnitude, promoting some new crystallographic insight and discovery. Furthermore the visual display of key algorithm indicators has led to diverse, unexpected discoveries that will improve the USPEX algorithms.","AuthorNames-Deduped":"Mario Valle;Artem R. Oganov","AuthorNames":"Mario Valle;Artem R. Oganov","AuthorAffiliation":"Data Analysis and Visualization Services, Swiss National Supercomputing Centre (CSCS), Switzerland;Laboratory of Crystallography, Department of Materials, ETH Zürich, Switzerland","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":8,"XploreCitationCount - 2020-01":6,"PubsCited":31,"Award":null,"image":"4677351-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Model-driven Visual Analytics","DOI":"10.1109/VAST.2008.4677352","Link":"http://dx.doi.org/10.1109/VAST.2008.4677352","FirstPage":19,"LastPage":26,"PaperType":"C","Abstract":"We describe a visual analytics (VA) infrastructure, rooted on techniques in machine learning and logic-based deductive reasoning that will assist analysts to make sense of large, complex data sets by facilitating the generation and validation of models representing relationships in the data. We use logic programming (LP) as the underlying computing machinery to encode the relations as rules and facts and compute with them. A unique aspect of our approach is that the LP rules are automatically learned, using Inductive Logic Programming, from examples of data that the analyst deems interesting when viewing the data in the high-dimensional visualization interface. Using this system, analysts will be able to construct models of arbitrary relationships in the data, explore the data for scenarios that fit the model, refine the model if necessary, and query the model to automatically analyze incoming (future) data exhibiting the encoded relationships. In other words it will support both model-driven data exploration, as well as data-driven model evolution. More importantly, by basing the construction of models on techniques from machine learning and logic-based deduction, the VA process will be both flexible in terms of modeling arbitrary, user-driven relationships in the data as well as readily scale across different data domains.","AuthorNames-Deduped":"Supriya Garg;Julia Eunju Nam;I. V. Ramakrishnan;Klaus Mueller","AuthorNames":"Supriya Garg;Julia Eunju Nam;I.V. Ramakrishnan;Klaus Mueller","AuthorAffiliation":"Computer Science Department, Stony Brook University, USA;Computer Science Department, Stony Brook University, USA;Computer Science Department, Stony Brook University, USA;Computer Science Department, Stony Brook University, USA","InternalReferences":"10.1109/VAST.2006.261437;10.1109/VAST.2007.4388999;10.1109/VAST.2007.4388998;10.1109/VAST.2007.4389003;10.1109/VAST.2007.4389000;10.1109/VAST.2006.261425;10.1109/VAST.2006.261436;10.1109/VAST.2007.4388992;10.1109/TVCG.2007.70581;10.1109/VISUAL.1990.146402;10.1109/VISUAL.1990.146386","AuthorKeywords":"Visual Analytics, Knowledge Discovery, Visual Clustering, Machine Learning, Grand Tour, High-dimensional Data, Network Security","AminerCitationCount_02-2020":22,"AminerCitationCount_06-2020":24,"XploreCitationCount - 2020-01":19,"PubsCited":27,"Award":null,"image":"4677352-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Using visual analytics to maintain situation awareness in astrophysics","DOI":"10.1109/VAST.2008.4677353","Link":"http://dx.doi.org/10.1109/VAST.2008.4677353","FirstPage":27,"LastPage":34,"PaperType":"C","Abstract":"We present a novel collaborative visual analytics application for cognitively overloaded users in the astrophysics domain. The system was developed for scientists needing to analyze heterogeneous, complex data under time pressure, and then make predictions and time-critical decisions rapidly and correctly under a constant influx of changing data. The Sunfall Data Taking system utilizes several novel visualization and analysis techniques to enable a team of geographically distributed domain specialists to effectively and remotely maneuver a custom-built instrument under challenging operational conditions. Sunfall Data Taking has been in use for over eighteen months by a major international astrophysics collaboration (the largest data volume supernova search currently in operation), and has substantially improved the operational efficiency of its users. We describe the system design process by an interdisciplinary team, the system architecture, and the results of an informal usability evaluation of the production system by domain experts in the context of Endsleypsilas three levels of situation awareness.","AuthorNames-Deduped":"Cecilia R. Aragon;Sarah S. Poon;Gregory S. Aldering;Rollin C. Thomas;Robert Quimby","AuthorNames":"Cecilia R. Aragon;Sarah S. Poon;Gregory S. Aldering;Rollin C. Thomas;Robert Quimby","AuthorAffiliation":"Lawrence Berkeley National Laboratory, CA 94720, USA;Space Sciences Laboratory, Berkeley, CA 94720, USA;Lawrence Berkeley National Laboratory, CA 94720, USA;Lawrence Berkeley National Laboratory, CA 94720, USA;California Institute of Technology, Pasadena, 91125, USA","InternalReferences":"10.1109/VAST.2007.4388997;10.1109/VAST.2007.4388998;10.1109/VAST.2007.4388993;10.1109/VAST.2006.261416;10.1109/VAST.2007.4388991;10.1109/VAST.2007.4388996;10.1109/VAST.2007.4388994;10.1109/VAST.2006.261434;10.1109/TVCG.2006.176;10.1109/INFVIS.2004.27","AuthorKeywords":"Data and knowledge visualization, scientific visualization, visual analytics, situation awareness, astrophysics","AminerCitationCount_02-2020":9,"AminerCitationCount_06-2020":14,"XploreCitationCount - 2020-01":5,"PubsCited":39,"Award":null,"image":"4677353-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Understanding syndromic hotspots - a visual analytics approach","DOI":"10.1109/VAST.2008.4677354","Link":"http://dx.doi.org/10.1109/VAST.2008.4677354","FirstPage":35,"LastPage":42,"PaperType":"C","Abstract":"When analyzing syndromic surveillance data, health care officials look for areas with unusually high cases of syndromes. Unfortunately, many outbreaks are difficult to detect because their signal is obscured by the statistical noise. Consequently, many detection algorithms have a high false positive rate. While many false alerts can be easily filtered by trained epidemiologists, others require health officials to drill down into the data, analyzing specific segments of the population and historical trends over time and space. Furthermore, the ability to accurately recognize meaningful patterns in the data becomes more challenging as these data sources increase in volume and complexity. To facilitate more accurate and efficient event detection, we have created a visual analytics tool that provides analysts with linked geo-spatiotemporal and statistical analytic views. We model syndromic hotspots by applying a kernel density estimation on the population sample. When an analyst selects a syndromic hotspot, temporal statistical graphs of the hotspot are created. Similarly, regions in the statistical plots may be selected to generate geospatial features specific to the current time period. Demographic filtering can then be combined to determine if certain populations are more affected than others. These tools allow analysts to perform real-time hypothesis testing and evaluation.","AuthorNames-Deduped":"Ross Maciejewski;Stephen Rudolph;Ryan Hafen;Ahmad M. Abusalah;Mohamed Yakout;Mourad Ouzzani;William S. Cleveland;Shaun J. Grannis;Michael Wade;David S. Ebert","AuthorNames":"Ross Maciejewski;Stephen Rudolph;Ryan Hafen;Ahmad Abusalah;Mohamed Yakout;Mourad Ouzzani;William S. Cleveland;Shaun J. Grannis;Michael Wade;David S. Ebert","AuthorAffiliation":"Purdue University Regional Visualization and Analytics Center (PURVAC), USA;Purdue University Regional Visualization and Analytics Center (PURVAC), USA;Purdue University Regional Visualization and Analytics Center (PURVAC), USA;Purdue University Regional Visualization and Analytics Center (PURVAC), USA;Purdue University Regional Visualization and Analytics Center (PURVAC), USA;Purdue University Regional Visualization and Analytics Center (PURVAC), USA;Purdue University Regional Visualization and Analytics Center (PURVAC), USA;Regenstrief Institute and Indiana University School of Medicine, USA;Indiana State Department of Health, USA;Purdue University Regional Visualization and Analytics Center (PURVAC), USA","InternalReferences":"10.1109/INFVIS.2001.963294;10.1109/VAST.2007.4388991;10.1109/INFVIS.1998.729563;10.1109/VISUAL.1995.485139;10.1109/VAST.2007.4388993","AuthorKeywords":null,"AminerCitationCount_02-2020":20,"AminerCitationCount_06-2020":25,"XploreCitationCount - 2020-01":13,"PubsCited":26,"Award":null,"image":"4677354-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Configurable Spaces: Temporal analysis in diagrammatic contexts","DOI":"10.1109/VAST.2008.4677355","Link":"http://dx.doi.org/10.1109/VAST.2008.4677355","FirstPage":43,"LastPage":50,"PaperType":"C","Abstract":"Social network graphs, concept maps, and process charts are examples of diagrammatic representations employed by intelligence analysts to understand complex systems. Unfortunately, these 2D representations currently do not easily convey the flow, sequence, tempo and other important dynamic behaviors within these systems. In this paper we present Configurable Spaces, a novel analytical method for visualizing patterns of activity over time in complex diagrammatically- represented systems. Configurable Spaces extends GeoTime's X, Y, T coordinate workspace space for temporal analysis to any arbitrary diagrammatic work space by replacing a geographic map with a diagram. This paper traces progress from concept to prototype, and discusses how diagrams can be created, transformed and leveraged for analysis, including generating diagrams from knowledge bases, visualizing temporal concept maps, and the use of linked diagrams for exploring complex, multi-dimensional, sequences of events. An evaluation of the prototype by the National Institute of Standards and Technology showed intelligence analysts believed they were able to attain an increased level of insight, were able to explore data more efficiently, and that Configurable Spaces would help them work faster.","AuthorNames-Deduped":"Thomas Kapler;Ryan Eccles;Robert Harper 0002;William Wright","AuthorNames":"Thomas Kapler;Ryan Eccles;Robert Harper;William Wright","AuthorAffiliation":"Oculus Info Inc., USA;Oculus Info Inc., USA;Oculus Info Inc., USA;Oculus Info Inc., USA","InternalReferences":"10.1109/VAST.2006.261450;10.1109/INFVIS.2001.963281;10.1109/INFVIS.2004.27;10.1109/INFVIS.2002.1173160","AuthorKeywords":"human information interaction, visual analytics, graph visualization, geo-temporal analysis, concept maps","AminerCitationCount_02-2020":12,"AminerCitationCount_06-2020":12,"XploreCitationCount - 2020-01":7,"PubsCited":28,"Award":null,"image":"4677355-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2008,"Title":"Spatio-temporal aggregation for visual analysis of movements","DOI":"10.1109/VAST.2008.4677356","Link":"http://dx.doi.org/10.1109/VAST.2008.4677356","FirstPage":51,"LastPage":58,"PaperType":"C","Abstract":"Data about movements of various objects are collected in growing amounts by means of current tracking technologies. Traditional approaches to visualization and interactive exploration of movement data cannot cope with data of such sizes. In this research paper we investigate the ways of using aggregation for visual analysis of movement data. We define aggregation methods suitable for movement data and find visualization and interaction techniques to represent results of aggregations and enable comprehensive exploration of the data. We consider two possible views of movement, traffic-oriented and trajectory-oriented. Each view requires different methods of analysis and of data aggregation. We illustrate our argument with example data resulting from tracking multiple cars in Milan and example analysis tasks from the domain of city traffic management.","AuthorNames-Deduped":"Gennady L. Andrienko;Natalia V. Andrienko","AuthorNames":"Gennady Andrienko;Natalia Andrienko","AuthorAffiliation":"Fraunhofer Institute IAIS (Intelligent Analysis and Information Systems), Sankt Augustin, Germany;Fraunhofer Institute IAIS (Intelligent Analysis and Information Systems), Sankt Augustin, Germany","InternalReferences":null,"AuthorKeywords":"Movement data, spatio-temporal data, aggregation, scalable visualization, geovisualization","AminerCitationCount_02-2020":120,"AminerCitationCount_06-2020":137,"XploreCitationCount - 2020-01":106,"PubsCited":17,"Award":null,"image":"4677356-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Maintaining interactivity while exploring massive time series","DOI":"10.1109/VAST.2008.4677357","Link":"http://dx.doi.org/10.1109/VAST.2008.4677357","FirstPage":59,"LastPage":66,"PaperType":"C","Abstract":"The speed of data retrieval qualitatively affects how analysts visually explore and analyze their data. To ensure smooth interactions in massive time series datasets, one needs to address the challenges of computing &lt;i&gt;ad&lt;/i&gt; &lt;i&gt;hoc&lt;/i&gt; queries, distributing query load, and hiding system latency. In this paper, we present ATLAS, a visualization tool for temporal data that addresses these issues using a combination of high performance database technology, predictive caching, and level of detail management. We demonstrate ATLAS using commodity hardware on a network traffic dataset of more than a billion records.","AuthorNames-Deduped":"Sye-Min Chan;Ling Xiao;John Gerth;Pat Hanrahan","AuthorNames":"Sye-Min Chan;Ling Xiao;John Gerth;Pat Hanrahan","AuthorAffiliation":"Stanford University, USA;Stanford University, USA;Stanford University, USA;Stanford University, USA","InternalReferences":"10.1109/VAST.2006.261437;10.1109/VAST.2007.4388998","AuthorKeywords":null,"AminerCitationCount_02-2020":35,"AminerCitationCount_06-2020":51,"XploreCitationCount - 2020-01":10,"PubsCited":30,"Award":null,"image":"4677357-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Collaborative synthesis of visual analytic results","DOI":"10.1109/VAST.2008.4677358","Link":"http://dx.doi.org/10.1109/VAST.2008.4677358","FirstPage":67,"LastPage":74,"PaperType":"C","Abstract":"Visual analytic tools allow analysts to generate large collections of useful analytical results. We anticipate that analysts in most real world situations will draw from these collections when working together to solve complicated problems. This indicates a need to understand how users synthesize multiple collections of results. This paper reports the results of collaborative synthesis experiments conducted with expert geographers and disease biologists. Ten participants were worked in pairs to complete a simulated real-world synthesis task using artifacts printed on cards on a large, paper-covered workspace. Experiment results indicate that groups use a number of different approaches to collaborative synthesis, and that they employ a variety of organizational metaphors to structure their information. It is further evident that establishing common ground and role assignment are critical aspects of collaborative synthesis. We conclude with a set of general design guidelines for collaborative synthesis support tools.","AuthorNames-Deduped":"Anthony C. Robinson","AuthorNames":"Anthony C. Robinson","AuthorAffiliation":"GeoVISTA Center, Department of Geography, The Pennsylvania State University, USA","InternalReferences":"10.1109/VAST.2007.4389011;10.1109/TVCG.2007.70594;10.1109/TVCG.2007.70568","AuthorKeywords":"Movement data, spatio-temporal data, aggregation, scalable visualization, geovisualization","AminerCitationCount_02-2020":65,"AminerCitationCount_06-2020":71,"XploreCitationCount - 2020-01":41,"PubsCited":17,"Award":null,"image":"4677358-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Visual evaluation of text features for document summarization and analysis","DOI":"10.1109/VAST.2008.4677359","Link":"http://dx.doi.org/10.1109/VAST.2008.4677359","FirstPage":75,"LastPage":82,"PaperType":"C","Abstract":"Thanks to the Web-related and other advanced technologies, textual information is increasingly being stored in digital form and posted online. Automatic methods to analyze such textual information are becoming inevitable. Many of those methods are based on quantitative text features. Analysts face the challenge to choose the most appropriate features for their tasks. This requires effective approaches for evaluation and feature-engineering.","AuthorNames-Deduped":"Daniela Oelke;Peter Bak;Daniel A. Keim;Mark Last;Guy Danon","AuthorNames":"Daniela Oelke;Peter Bak;Daniel A. Keim;Mark Last;Guy Danon","AuthorAffiliation":"University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;Ben-Gurion University of the Negev, Israel;Ben-Gurion University of the Negev, Israel","InternalReferences":"10.1109/VISUAL.1993.398863;10.1109/INFVIS.1995.528686;10.1109/VAST.2007.4389004","AuthorKeywords":null,"AminerCitationCount_02-2020":19,"AminerCitationCount_06-2020":19,"XploreCitationCount - 2020-01":9,"PubsCited":27,"Award":null,"image":"4677359-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Evaluating the relationship between user interaction and financial visual analysis","DOI":"10.1109/VAST.2008.4677360","Link":"http://dx.doi.org/10.1109/VAST.2008.4677360","FirstPage":83,"LastPage":90,"PaperType":"C","Abstract":"It has been widely accepted that interactive visualization techniques enable users to more effectively form hypotheses and identify areas for more detailed investigation. There have been numerous empirical user studies testing the effectiveness of specific visual analytical tools. However, there has been limited effort in connecting a userpsilas interaction with his reasoning for the purpose of extracting the relationship between the two. In this paper, we present an approach for capturing and analyzing user interactions in a financial visual analytical tool and describe an exploratory user study that examines these interaction strategies. To achieve this goal, we created two visual tools to analyze raw interaction data captured during the user session. The results of this study demonstrate one possible strategy for understanding the relationship between interaction and reasoning both operationally and strategically.","AuthorNames-Deduped":"Dong Hyun Jeong;Wenwen Dou;Heather Lipford;Felesia Stukes;Remco Chang;William Ribarsky","AuthorNames":"Dong Hyun Jeong;Wenwen Dou;Heather Richter Lipford;Felesia Stukes;Remco Chang;William Ribarsky","AuthorAffiliation":"UNC Charlotte, Viscenter, USA;UNC Charlotte, Viscenter, USA;UNC Charlotte, HCI Lab, USA;UNC Charlotte, HCI Lab, USA;UNC Charlotte, Viscenter, USA;UNC Charlotte, Viscenter, USA","InternalReferences":"10.1109/VAST.2007.4389009","AuthorKeywords":null,"AminerCitationCount_02-2020":18,"AminerCitationCount_06-2020":19,"XploreCitationCount - 2020-01":13,"PubsCited":15,"Award":null,"image":"4677360-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2008,"Title":"Visual analytics for complex concepts using a human cognition model","DOI":"10.1109/VAST.2008.4677361","Link":"http://dx.doi.org/10.1109/VAST.2008.4677361","FirstPage":91,"LastPage":98,"PaperType":"C","Abstract":"As the information being visualized and the process of understanding that information both become increasingly complex, it is necessary to develop new visualization approaches that facilitate the flow of human reasoning. In this paper, we endeavor to push visualization design a step beyond current user models by discussing a modeling framework of human ldquohigher cognition.rdquo Based on this cognition model, we present design guidelines for the development of visual interfaces designed to maximize the complementary cognitive strengths of both human and computer. Some of these principles are already being reflected in the better visual analytics designs, while others have not yet been applied or fully applied. But none of the guidelines have explained the deeper rationale that the model provides. Lastly, we discuss and assess these visual analytics guidelines through the evaluation of several visualization examples.","AuthorNames-Deduped":"Tera Marie Green;William Ribarsky;Brian D. Fisher","AuthorNames":"Tera Marie Green;William Ribarsky;Brian Fisher","AuthorAffiliation":"Charlotte Visualization Center, University of North Carolina, USA;Charlotte Visualization Center, University of North Carolina, USA;School of Interactive Arts and Technology, Simon Fraser University, Canada","InternalReferences":"10.1109/VISUAL.2005.1532781;10.1109/VAST.2006.261425;10.1109/TVCG.2007.70574;10.1109/VAST.2007.4389006;10.1109/VAST.2007.4389005;10.1109/VAST.2007.4389009;10.1109/INFVIS.1995.528686","AuthorKeywords":"visual analytics, cognition and perception theory, embodied cognition, visualization taxonomies and models","AminerCitationCount_02-2020":55,"AminerCitationCount_06-2020":61,"XploreCitationCount - 2020-01":43,"PubsCited":34,"Award":null,"image":"4677361-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Entity-based collaboration tools for intelligence analysis","DOI":"10.1109/VAST.2008.4677362","Link":"http://dx.doi.org/10.1109/VAST.2008.4677362","FirstPage":99,"LastPage":106,"PaperType":"C","Abstract":"Software tools that make it easier for analysts to collaborate as a natural part of their work will lead to better analysis that is informed by more perspectives. We are interested to know if software tools can be designed that support collaboration even as they allow analysts to find documents and organize information (including evidence, schemas, and hypotheses). We have modified the Entity Workspace system, described previously, to test such designs. We have evaluated the resulting design in both a laboratory study and a study where it is situated with an analysis team. In both cases, effects on collaboration appear to be positive. Key aspects of the design include an evidence notebook optimized for organizing entities (rather than text characters), information structures that can be collapsed and expanded, visualization of evidence that emphasizes events and documents (rather than emphasizing the entity graph), and a notification system that finds entities of mutual interest to multiple analysts.","AuthorNames-Deduped":"Eric A. Bier;Stuart K. Card;John W. Bodnar","AuthorNames":"Eric A. Bier;Stuart K. Card;John W. Bodnar","AuthorAffiliation":"Palo Alto Research Center, Inc., 3333 Coyote Hill Road, California, 94304, USA;Palo Alto Research Center, Inc., 3333 Coyote Hill Road, California, 94304, USA;Science Applications International Corporation, 1710 SAIC Drive, McLean, VA 22102, USA","InternalReferences":"10.1109/VAST.2006.261427;10.1109/VAST.2007.4389006","AuthorKeywords":"sensemaking, information foraging, collective intelligence, exploratory search, information workspace, entity-based, collaboration, intelligence analysis, visualization, semantic notebook, argumentation marshalling, visual analytics","AminerCitationCount_02-2020":38,"AminerCitationCount_06-2020":45,"XploreCitationCount - 2020-01":28,"PubsCited":17,"Award":"BP","image":"4677362-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Applied visual analytics for economic decision-making","DOI":"10.1109/VAST.2008.4677363","Link":"http://dx.doi.org/10.1109/VAST.2008.4677363","FirstPage":107,"LastPage":114,"PaperType":"C","Abstract":"This paper introduces the application of visual analytics techniques as a novel approach for improving economic decision making. Particularly, we focus on two known problems where subjectspsila behavior consistently deviates from the optimal, the Winnerpsilas and Loserpsilas Curse. According to economists, subjects fail to recognize the profit-maximizing decision strategy in both the Winnerpsilas and Loserpsilas curse because they are unable to properly consider all the available information. As such, we have created a visual analytics tool to aid subjects in decision making under the Acquiring a Company framework common in many economic experiments. We demonstrate the added value of visual analytics in the decision making process through a series of user studies comparing standard visualization methods with interactive visual analytics techniques. Our work presents not only a basis for development and evaluation of economic visual analytic research, but also empirical evidence demonstrating the added value of applying visual analytics to general decision making tasks.","AuthorNames-Deduped":"Anya Samak;Ross Maciejewski;David S. Ebert","AuthorNames":"Anya Savikhin;Ross Maciejewski;David S. Ebert","AuthorAffiliation":"Purdue University Department of Economics, USA;Purdue University Regional Visualization and Analytics Center (PURVAC), USA;Purdue University Regional Visualization and Analytics Center (PURVAC), USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":26,"AminerCitationCount_06-2020":29,"XploreCitationCount - 2020-01":15,"PubsCited":18,"Award":null,"image":"4677363-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Narratives: A visualization to track narrative events as they develop","DOI":"10.1109/VAST.2008.4677364","Link":"http://dx.doi.org/10.1109/VAST.2008.4677364","FirstPage":115,"LastPage":122,"PaperType":"C","Abstract":"Analyzing unstructured text streams can be challenging. One popular approach is to isolate specific themes in the text, and to visualize the connections between them. Some existing systems, like ThemeRiver, provide a temporal view of changes in themes; other systems, like In-Spire, use clustering techniques to help an analyst identify the themes at a single point in time. Narratives combines both of these techniques; it uses a temporal axis to visualize ways that concepts have changed over time, and introduces several methods to explore how those concepts relate to each other. Narratives is designed to help the user place news stories in their historical and social context by understanding how the major topics associated with them have changed over time. Users can relate articles through time by examining the topical keywords that summarize a specific news event. By tracking the attention to a news article in the form of references in social media (such as weblogs), a user discovers both important events and measures the social relevance of these stories.","AuthorNames-Deduped":"Danyel Fisher;Aaron Hoff;George G. Robertson;Matthew Hurst","AuthorNames":"Danyel Fisher;Aaron Hoff;George Robertson;Matthew Hurst","AuthorAffiliation":"Microsoft Research, USA;Microsoft Research, USA;Microsoft Research, USA;Microsoft Live Labs, Switzerland","InternalReferences":"10.1109/INFVIS.2005.1532122;10.1109/INFVIS.1999.801851","AuthorKeywords":"blogs, events, trends, time series, topic detection and tracking","AminerCitationCount_02-2020":44,"AminerCitationCount_06-2020":46,"XploreCitationCount - 2020-01":20,"PubsCited":16,"Award":null,"image":"4677364-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Characterizing users' visual analytic activity for insight provenance","DOI":"10.1109/VAST.2008.4677365","Link":"http://dx.doi.org/10.1109/VAST.2008.4677365","FirstPage":123,"LastPage":130,"PaperType":"C","Abstract":"Insight provenance - a historical record of the process and rationale by which an insight is derived - is an essential requirement in many visual analytics applications. While work in this area has relied on either manually recorded provenance (e.g., user notes) or automatically recorded event-based insight provenance (e.g., clicks, drags, and key-presses), both approaches have fundamental limitations. Our aim is to develop a new approach that combines the benefits of both approaches while avoiding their deficiencies. Toward this goal, we characterize userspsila visual analytic activity at multiple levels of granularity. Moreover, we identify a critical level of abstraction, Actions, that can be used to represent visual analytic activity with a set of general but semantically meaningful behavior types. In turn, the action types can be used as the semantic building blocks for insight provenance. We present a catalog of common actions identified through observations of several different visual analytic systems. In addition, we define a taxonomy to categorize actions into three major classes based on their semantic intent. The concept of actions has been integrated into our labpsilas prototype visual analytic system, HARVEST, as the basis for its insight provenance capabilities.","AuthorNames-Deduped":"David Gotz;Michelle X. Zhou","AuthorNames":"David Gotz;Michelle X. Zhou","AuthorAffiliation":"IBM T.J. Watson Research Center, USA;IBM T.J. Watson Research Center, USA","InternalReferences":"10.1109/INFVIS.2004.2;10.1109/INFVIS.1996.559213;10.1109/TVCG.2007.70577;10.1109/VISUAL.2005.1532788;10.1109/VAST.2007.4388992;10.1109/INFVIS.2005.1532136;10.1109/INFVIS.1998.729560;10.1109/INFVIS.2004.10;10.1109/VAST.2006.261430;10.1109/INFVIS.2000.885092;10.1109/VISUAL.1990.146375;10.1109/VISUAL.2002.1183791","AuthorKeywords":"Taxonomy, Information Visualization, Analytic Activity, Visual Analytics, Insight Provenance","AminerCitationCount_02-2020":125,"AminerCitationCount_06-2020":89,"XploreCitationCount - 2020-01":34,"PubsCited":29,"Award":null,"image":"4677365-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"The Scalable Reasoning System: Lightweight visualization for distributed analytics","DOI":"10.1109/VAST.2008.4677366","Link":"http://dx.doi.org/10.1109/VAST.2008.4677366","FirstPage":131,"LastPage":138,"PaperType":"C","Abstract":"A central challenge in visual analytics is the creation of accessible, widely distributable analysis applications that bring the benefits of visual discovery to as broad a user base as possible. Moreover, to support the role of visualization in the knowledge creation process, it is advantageous to allow users to describe the reasoning strategies they employ while interacting with analytic environments. We introduce an application suite called the scalable reasoning system (SRS), which provides Web-based and mobile interfaces for visual analysis. The service-oriented analytic framework that underlies SRS provides a platform for deploying pervasive visual analytic environments across an enterprise. SRS represents a ldquolightweightrdquo approach to visual analytics whereby thin client analytic applications can be rapidly deployed in a platform-agnostic fashion. Client applications support multiple coordinated views while giving analysts the ability to record evidence, assumptions, hypotheses and other reasoning artifacts. We describe the capabilities of SRS in the context of a real-world deployment at a regional law enforcement organization.","AuthorNames-Deduped":"William A. Pike;Joe Bruce;Bob Baddeley;Daniel M. Best;Lyndsey Franklin;Richard May;Douglas M. Rice;Roderick M. Riensche;Katarina Younkin","AuthorNames":"William A. Pike;Joe Bruce;Bob Baddeley;Daniel Best;Lyndsey Franklin;Richard May;Douglas M. Rice;Rick Riensche;Katarina Younkin","AuthorAffiliation":"Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA","InternalReferences":"10.1109/TVCG.2007.70577;10.1109/TVCG.2006.142;10.1109/VAST.2007.4388996;10.1109/INFVIS.2005.1532133;10.1109/VISUAL.1993.398874;10.1109/VAST.2007.4389006;10.1109/VAST.2007.4388991","AuthorKeywords":"Web visualization, mobile visualization, analytic reasoning, law enforcement, multiple views","AminerCitationCount_02-2020":22,"AminerCitationCount_06-2020":12,"XploreCitationCount - 2020-01":8,"PubsCited":21,"Award":null,"image":"4677366-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Generating hypotheses of trends in high-dimensional data skeletons","DOI":"10.1109/VAST.2008.4677367","Link":"http://dx.doi.org/10.1109/VAST.2008.4677367","FirstPage":139,"LastPage":146,"PaperType":"C","Abstract":"We seek an information-revealing representation for high-dimensional data distributions that may contain local trends in certain subspaces. Examples are data that have continuous support in simple shapes with identifiable branches. Such data can be represented by a graph that consists of segments of locally fit principal curves or surfaces summarizing each identifiable branch. We describe a new algorithm to find the optimal paths through such a principal graph. The paths are optimal in the sense that they represent the longest smooth trends through the data set, and jointly they cover the data set entirely with minimum overlap. The algorithm is suitable for hypothesizing trends in high-dimensional data, and can assist exploratory data analysis and visualization.","AuthorNames-Deduped":"Chandan K. Reddy;Snehal Pokharkar;Tin Kam Ho","AuthorNames":"Chandan K. Reddy;Snehal Pokharkar;Tin Kam Ho","AuthorAffiliation":"Department of Computer Science, Wayne State University, USA;Department of Computer Science, Wayne State University, USA;Bell Labs, Alcatel-Lucent, USA","InternalReferences":"10.1109/VAST.2007.4388999","AuthorKeywords":null,"AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":3,"PubsCited":13,"Award":null,"image":"4677367-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Multivariate visual explanation for high dimensional datasets","DOI":"10.1109/VAST.2008.4677368","Link":"http://dx.doi.org/10.1109/VAST.2008.4677368","FirstPage":147,"LastPage":154,"PaperType":"C","Abstract":"Understanding multivariate relationships is an important task in multivariate data analysis. Unfortunately, existing multivariate visualization systems lose effectiveness when analyzing relationships among variables that span more than a few dimensions. We present a novel multivariate visual explanation approach that helps users interactively discover multivariate relationships among a large number of dimensions by integrating automatic numerical differentiation techniques and multidimensional visualization techniques. The result is an efficient workflow for multivariate analysis model construction, interactive dimension reduction, and multivariate knowledge discovery leveraging both automatic multivariate analysis and interactive multivariate data visual exploration. Case studies and a formal user study with a real dataset illustrate the effectiveness of this approach.","AuthorNames-Deduped":"Scott Barlowe;Tianyi Zhang;Yujie Liu;Jing Yang 0001;Donald J. Jacobs","AuthorNames":"Scott Barlowe;Tianyi Zhang;Yujie Liu;Jing Yang;Donald Jacobs","AuthorAffiliation":"Dept of Computer Science, University of North Carolina at Charlotte, USA;Dept of Computer Science, University of North Carolina at Charlotte, USA;Dept of Computer Science, University of North Carolina at Charlotte, USA;Dept of Computer Science, University of North Carolina at Charlotte, USA;Dept of Physics and Optical Science, University of North Carolina at Charlotte, USA","InternalReferences":"10.1109/INFVIS.2005.1532142;10.1109/INFVIS.2004.10;10.1109/VISUAL.1995.485140;10.1109/VISUAL.1995.485139;10.1109/INFVIS.2004.3;10.1109/INFVIS.2004.71","AuthorKeywords":"visual analysis, multivariate analysis, dimension reduction, multivariate model construction, multivariate visualization","AminerCitationCount_02-2020":14,"AminerCitationCount_06-2020":18,"XploreCitationCount - 2020-01":11,"PubsCited":30,"Award":null,"image":"4677368-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Visual mining of multimedia data for social and behavioral studies","DOI":"10.1109/VAST.2008.4677369","Link":"http://dx.doi.org/10.1109/VAST.2008.4677369","FirstPage":155,"LastPage":162,"PaperType":"C","Abstract":"With advances in computing techniques, a large amount of high-resolution high-quality multimedia data (video and audio, etc.) has been collected in research laboratories in various scientific disciplines, particularly in social and behavioral studies. How to automatically and effectively discover new knowledge from rich multimedia data poses a compelling challenge since state-of-the-art data mining techniques can most often only search and extract pre-defined patterns or knowledge from complex heterogeneous data. In light of this, our approach is to take advantages of both the power of human perception system and the power of computational algorithms. More specifically, we propose an approach that allows scientists to use data mining as a first pass, and then forms a closed loop of visual analysis of current results followed by more data mining work inspired by visualization, the results of which can be in turn visualized and lead to the next round of visual exploration and analysis. In this way, new insights and hypotheses gleaned from the raw data and the current level of analysis can contribute to further analysis. As a first step toward this goal, we implement a visualization system with three critical components: (1) A smooth interface between visualization and data mining. The new analysis results can be automatically loaded into our visualization tool. (2) A flexible tool to explore and query temporal data derived from raw multimedia data. We represent temporal data into two forms - continuous variables and event variables. We have developed various ways to visualize both temporal correlations and statistics of multiple variables with the same type, and conditional and high-order statistics between continuous and event variables. (3) A seamless interface between raw multimedia data and derived data. Our visualization tool allows users to explore, compare, and analyze multi-stream derived variables and simultaneously switch to access raw multimedia data. We demonstrate various functions in our visualization program using a set of multimedia data including video, audio and motion tracking data.","AuthorNames-Deduped":"Chen Yu;Yiwen Zhong;Thomas G. Smith 0002;Ikhyun Park;Weixia Huang","AuthorNames":"Chen Yu;Yiwen Zhong;Thomas Smith;Ikhyun Park;Weixia Huang","AuthorAffiliation":"Indiana University, USA;Indiana University, USA;Indiana University, USA;Indiana University, USA;Indiana University, USA","InternalReferences":"10.1109/INFVIS.2001.963273;10.1109/INFVIS.1999.801851","AuthorKeywords":"visual data mining, multimedia data","AminerCitationCount_02-2020":8,"AminerCitationCount_06-2020":11,"XploreCitationCount - 2020-01":0,"PubsCited":12,"Award":null,"image":"4677369-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Multidimensional visual analysis using cross-filtered views","DOI":"10.1109/VAST.2008.4677370","Link":"http://dx.doi.org/10.1109/VAST.2008.4677370","FirstPage":163,"LastPage":170,"PaperType":"C","Abstract":"Analysis of multidimensional data often requires careful examination of relationships across dimensions. Coordinated multiple view approaches have become commonplace in visual analysis tools because they directly support expression of complex multidimensional queries using simple interactions. However, generating such tools remains difficult because of the need to map domain-specific data structures and semantics into the idiosyncratic combinations of interdependent data and visual abstractions needed to reveal particular patterns and distributions in cross-dimensional relationships. This paper describes: (1) a method for interactively expressing sequences of multidimensional set queries by cross-filtering data values across pairs of views, and (2) design strategies for constructing coordinated multiple view interfaces for cross-filtered visual analysis of multidimensional data sets. Using examples of cross-filtered visualizations of data from several different domains, we describe how cross-filtering can be modularized and reused across designs, flexibly customized with respect to data types across multiple dimensions, and incorporated into more wide-ranging multiple view designs. The demonstrated analytic utility of these examples suggest that cross-filtering is a suitable design pattern for instantiation in a wide variety of visual analysis tools.","AuthorNames-Deduped":"Chris Weaver","AuthorNames":"Chris Weaver","AuthorAffiliation":"The GeoVISTA Center and Department of Geography, The Pennsylvania State University, USA","InternalReferences":"10.1109/TVCG.2006.178;10.1109/VAST.2006.261428;10.1109/INFVIS.2003.1249024;10.1109/INFVIS.2000.885086;10.1109/VISUAL.1994.346302;10.1109/INFVIS.1998.729560;10.1109/TVCG.2007.70594;10.1109/VAST.2007.4389006","AuthorKeywords":null,"AminerCitationCount_02-2020":26,"AminerCitationCount_06-2020":27,"XploreCitationCount - 2020-01":15,"PubsCited":27,"Award":null,"image":"4677370-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2008,"Title":"Interactive poster: Visual analytic techniques for CO₂ emissions and concentrations in the United States","DOI":"10.1109/VAST.2008.4677372","Link":"http://dx.doi.org/10.1109/VAST.2008.4677372","FirstPage":173,"LastPage":174,"PaperType":"M","Abstract":"Climate change has emerged as one of the grand global challenges facing humanity. The dominant anthropogenic greenhouse gas that seems to be contributing to the climate change problem, carbon dioxide (CO<sub>2</sub>), has a complex cycle through the atmosphere, oceans and biosphere. The combustion of fossil fuels (power production, transportation, etc.) remains the largest source of anthropogenic CO<sub>2</sub> to the Earthpsilas atmosphere. Up until very recently, the quantification of fossil fuel CO<sub>2</sub> was understood only at coarse space and time scales. A recent research effort has greatly improved this space/time quantification resulting in source data at a resolution of less than 10 km<sup>2</sup>/hr at the surface of North America. By providing visual tools to examine this new, high resolution CO<sub>2</sub> data, we can better understand the way that CO<sub>2</sub> is transmitted within the atmosphere and how it is exchanged with other components of the Earth System. We have developed interactive visual analytic tools, which allows for easy data manipulation, analysis, and extraction. The visualization system is aimed for a wide range of users which include researchers and political leaders. The goal is to help assist these people in analyzing data and enabling new policy options in mitigation of fossil fuel CO<sub>2</sub> emissions in the U.S.","AuthorNames-Deduped":"Nathan Andrysco;Bedrich Benes;Kevin R. Gurney","AuthorNames":"Nathan Andrysco;Bedrich Benes;Kevin Gurney","AuthorAffiliation":"Department of Computer Science, Purdue University, USA;Department of Computer Graphics Technology, Purdue University, USA;Department of Atmospheric Sciences, Purdue University, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":null,"XploreCitationCount - 2020-01":0,"PubsCited":3,"Award":null,"image":"4677372-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Envisioning user models for adaptive visualization","DOI":"10.1109/VAST.2008.4677373","Link":"http://dx.doi.org/10.1109/VAST.2008.4677373","FirstPage":175,"LastPage":176,"PaperType":"M","Abstract":"Adaptive search systems apply user models to provide better separation of relevant and non-relevant documents in a list of results. This paper presents our attempt to leverage this ability of user models in the context of visual information analysis. We developed an adaptive visualization approach for presentation and exploration of search results. We simulated a visual intelligence search/analysis scenario with log data extracted from an adaptive information foraging study and were able to verify that our method can improve the ability of traditional relevance visualization to separate relevant and irrelevant information.","AuthorNames-Deduped":"Jae-wook Ahn;Peter Brusilovsky","AuthorNames":"Jae-wook Ahn;Peter Brusilovsky","AuthorAffiliation":"School of Information Sciences, University of Pittsburgh, USA;School of Information Sciences, University of Pittsburgh, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":2,"PubsCited":6,"Award":null,"image":"4677373-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"A compound approach for interactive visualization of time-oriented data","DOI":"10.1109/VAST.2008.4677374","Link":"http://dx.doi.org/10.1109/VAST.2008.4677374","FirstPage":177,"LastPage":178,"PaperType":"M","Abstract":"Many real-world visual analytics applications involve time-oriented data. I am working in a research project related to this challenge where I am responsible for the interactive visualization part. My goal are interactive visualizations to explore such time-oriented data according to the user tasks while considering the structure of time. Time is composed of many granularities that are likely to have crucial influence on the formation of the data. The challenge is to integrate the granularities into a detailed compound view on the data, like the compound eye of insects integrates many images into one view. Other members of our team are experts in temporal data mining and user centered design. The goal is to combine our research topics to an integrated system that helps domain experts to get more insight from their time-oriented data.","AuthorNames-Deduped":"Tim Lammarsch","AuthorNames":"Tim Lammarsch","AuthorAffiliation":"Department of Information and Knowledge Engineering (ike), Danube University Krems, Austria","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":10,"Award":null,"image":""},{"Conference":"VAST","Year":2008,"Title":"Interactive poster - SocialRank: An ego- and time-centric workflow for relationship identification","DOI":"10.1109/VAST.2008.4677375","Link":"http://dx.doi.org/10.1109/VAST.2008.4677375","FirstPage":179,"LastPage":180,"PaperType":"M","Abstract":"From instant messaging and email to wikis and blogs, millions of individuals are generating content that reflects their relationships with others in the world, both online and offline. Since communication artifacts are recordings of life events, we can gain insights into the social attributes and structures of the people within this communication history. In this paper, we describe SocialRank, an ego- and time-centric workflow for identifying social relationships in an email corpus. This workflow includes four high-level tasks: discovery, validation, annotation and dissemination. SocialRank combines relationship ranking algorithms with timeline, social network diagram, and multidimensional scaling visualization techniques to support these tasks.","AuthorNames-Deduped":"Jaime Montemayor;Christopher P. Diehl;Michael Pekala;David Patrone","AuthorNames":"Jaime Montemayor;Chris Diehl;Mike Pekala;David Patrone","AuthorAffiliation":"Milton Eisenhower Research Center, The Johns Hopkins University Applied Physics Laboratory, USA;Milton Eisenhower Research Center, The Johns Hopkins University Applied Physics Laboratory, USA;Milton Eisenhower Research Center, The Johns Hopkins University Applied Physics Laboratory, USA;Milton Eisenhower Research Center, The Johns Hopkins University Applied Physics Laboratory, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":2,"PubsCited":1,"Award":null,"image":"4677375-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2008,"Title":"Visual analysis for mutual fund performance","DOI":"10.1109/VAST.2008.4677376","Link":"http://dx.doi.org/10.1109/VAST.2008.4677376","FirstPage":181,"LastPage":182,"PaperType":"M","Abstract":"Mutual funds are one of the most important investment instruments available. However, choosing among mutual funds is not an easy task because they vary in many different dimensions, such as asset size, turnover and fee structure, and these characteristics may affect fund returns. It is thus important to understand the relation between fund performance and these properties. In this work, we use a new visual analytical tool, the density-based distribution map, to assist in this task. By visualizing various important fund characteristics from a real-world database of the US stock funds, our new visual representations greatly help understand the relation between fund characteristics and returns.","AuthorNames-Deduped":"Ye Zhao;Jamal Alsakran;Xinlei Zhao","AuthorNames":"Ye Zhao;Jamal Alsakran;Xinlei Zhao","AuthorAffiliation":"Kent State University, USA;Kent State University, USA;Kent State University, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":0,"PubsCited":4,"Award":null,"image":"4677376-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"An information visualisation system for the understanding of web data","DOI":"10.1109/VAST.2008.4677377","Link":"http://dx.doi.org/10.1109/VAST.2008.4677377","FirstPage":183,"LastPage":184,"PaperType":"M","Abstract":"Internet has become one of the best communication and marketing tools. Hence, designing well-structured Web sites with the information or products that users look for is a crucial mission. For this reason, understanding Web data is a decisive task to assure the success of a Website. In that sense, data mining techniques provide many metrics and statistics useful to automatically discover the structure, contents and usage of a site. This research aims at proving the usefulness of a set of information visualisation techniques in order to analyse Web data, using a visual Web mining tool that allows the combination, coordination and exploration of visualisations to get insight on Web data. The tool, named WET, provides a set of visual metaphors that represent the structure of the Websites where Web metrics are overlaid.","AuthorNames-Deduped":"Victor Pascual-Cid","AuthorNames":"Victor Pascual-Cid","AuthorAffiliation":"Web Research Group, Universitat Pompeu Fabra and Fundació Barcelona Media, Spain","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":14,"AminerCitationCount_06-2020":23,"XploreCitationCount - 2020-01":9,"PubsCited":4,"Award":null,"image":"4677377-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2008,"Title":"Supporting exploration awareness for visual analytics","DOI":"10.1109/VAST.2008.4677378","Link":"http://dx.doi.org/10.1109/VAST.2008.4677378","FirstPage":185,"LastPage":186,"PaperType":"M","Abstract":"While exploring data using information visualization, analysts try to make sense of the data, build cases, and present them to others. However, if the exploration is long or done in multiple sessions, it can be hard for analysts to remember all interesting visualizations and the relationships among them they have seen. Often, they will see the same or similar visualizations, and are unable to recall when, why and how they have seen something similar. Recalling and retrieving interesting visualizations are important tasks for the analysis processes such as problem solving, reasoning, and conceptualization. In this paper, we argue that offering support for thinking based on past analysis processes is important, and present a solution for this.","AuthorNames-Deduped":"Yedendra Babu Shrinivasan;Jarke J. van Wijk","AuthorNames":"Yedendra B. Shrinivasan;Jarke J. van Wijk","AuthorAffiliation":"Eindhoven University of Technology, The Netherlands;Eindhoven University of Technology, The Netherlands","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":6,"XploreCitationCount - 2020-01":3,"PubsCited":4,"Award":null,"image":"4677378-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2008,"Title":"Interactive poster: Visual data mining of unevenly-spaced event sequences","DOI":"10.1109/VAST.2008.4677379","Link":"http://dx.doi.org/10.1109/VAST.2008.4677379","FirstPage":187,"LastPage":188,"PaperType":"M","Abstract":"We present a process for the exploration and analysis of large databases of events. A typical database is characterized by the sequential actions of a number of individual entities. These entities can be compared by their similarities in sequence and changes in sequence over time. The correlation of two sequences can provide important clues as to the possibility of a connection between the responsible entities, but an analyst might not be able to specify the type of connection sought prior to examination. Our process incorporates extensive automated calculation and data mining but permits diversity of analysis by providing visualization of results at multiple levels, taking advantage of human intuition and visual processing to generate avenues of inquiry.","AuthorNames-Deduped":"Alex Godwin;Remco Chang;Robert Kosara;William Ribarsky","AuthorNames":"Alex Godwin;Remco Chang;Robert Kosara;William Ribarsky","AuthorAffiliation":"Visualization Center, University of North Carolina at Charlotte, USA;Visualization Center, University of North Carolina at Charlotte, USA;Visualization Center, University of North Carolina at Charlotte, USA;Visualization Center, University of North Carolina at Charlotte, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":0,"PubsCited":4,"Award":null,"image":"4677379-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"A 3D treemap approach for analyzing the classificatory distribution in patent portfolios","DOI":"10.1109/VAST.2008.4677380","Link":"http://dx.doi.org/10.1109/VAST.2008.4677380","FirstPage":189,"LastPage":190,"PaperType":"M","Abstract":"Due to the complexity of the patent domain and the huge amount of data, advanced interactive visual techniques are needed to support the analysis of large patent collections and portfolios. In this paper we present a new approach for visualizing the classificatory distribution of patent collections among the International Patent Classification (IPC) - todaypsilas most important internationally agreed patent classification system with about 70.000 categories. Our approach is based on an interactive three-dimensional treemap overlaid with adjacency edge bundles.","AuthorNames-Deduped":"Mark Giereth;Harald Bosch;Thomas Ertl","AuthorNames":"Mark Giereth;Harald Bosch;Thomas Ertl","AuthorAffiliation":"Visualization and Interactive Systems Institute (VIS), University of Stuttgart, Germany;Visualization and Interactive Systems Institute (VIS), University of Stuttgart, Germany;Visualization and Interactive Systems Institute (VIS), University of Stuttgart, Germany","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":8,"XploreCitationCount - 2020-01":4,"PubsCited":8,"Award":null,"image":"4677380-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Visual analysis of seismic simulation data","DOI":"10.1109/VAST.2008.4677381","Link":"http://dx.doi.org/10.1109/VAST.2008.4677381","FirstPage":191,"LastPage":192,"PaperType":"M","Abstract":"Seismic simulations use finite element methods to describe ground motion. The results of such numerical simulations are often difficult to interpret for decision makers. We describe a terrain rendering engine that uses photorealistic metaphors to represent typical terrain properties without representing an actual terrain. In the context of ground motion, a simulation of the effects of various types of earthquakes on buildings has been conducted. Usually, such structural response simulations are carried out independently and are being visualized separate from the ground motion simulation. We combine the results from both simulations in an interactive, hybrid visualization so that decision makers (first responders and emergency management agencies) are provided with a photo-realistic, simulated view of various earthquake scenarios, enabling them to study the effect of various earthquakes on buildings typical for a rural or urban area. We present a method for visually analyzing large-scale simulation data from different sources (ground motion simulation and structural response simulation) using photorealistic metaphors. We have implemented an intuitive, interactive system for visual analysis and inspection of possible effects of various types of earthquakes on an inventory of buildings typical for a particular area. The underlying rendering system can be easily adapted for other simulations, such as smoke plumes or biohazards.","AuthorNames-Deduped":"Florian Jürgen Gerhardt;Joerg Meyer","AuthorNames":"Florian Juergen Gerhardt;Joerg Meyer","AuthorAffiliation":"Technical University of Kaiserslautern, Germany;Electrical Engineering and Computer Science Department, University of California, Irvine, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":1,"Award":null,"image":"4677381-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"VAST 2008 Challenge: Introducing mini-challenges","DOI":"10.1109/VAST.2008.4677383","Link":"http://dx.doi.org/10.1109/VAST.2008.4677383","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"Visual analytics experts realize that one effective way to push the field forward and to develop metrics for measuring the performance of various visual analytics components is to hold an annual competition. The VAST 2008 Challenge is the third year that such a competition was held in conjunction with the IEEE Visual Analytics Science and Technology (VAST) symposium. The authors restructured the contest format used in 2006 and 2007 to reduce the barriers to participation and offered four mini-challenges and a Grand Challenge. Mini Challenge participants were to use visual analytic tools to explore one of four heterogeneous data collections to analyze specific activities of a fictitious, controversial movement. Questions asked in the Grand Challenge required the participants to synthesize data from all four data sets. In this paper we give a brief overview of the data sets, the tasks, the participation, the judging, and the results.","AuthorNames-Deduped":"Georges G. Grinstein;Catherine Plaisant;Sharon J. Laskowski;Teresa O'Connell;Jean Scholtz;Mark A. Whiting","AuthorNames":"Georges Grinstein;Catherine Plaisant;Sharon Laskowski;Theresa O'Connell;Jean Scholtz;Mark Whiting","AuthorAffiliation":"University of Massachusetts, Lowell, USA;University of Maryland, USA;National Institute of Standards and Technology, USA;National Institute of Standards and Technology, USA;Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":31,"AminerCitationCount_06-2020":42,"XploreCitationCount - 2020-01":22,"PubsCited":6,"Award":null,"image":""},{"Conference":"VAST","Year":2008,"Title":"Grand challenge award: Data integration visualization and collaboration in the VAST 2008 Challenge","DOI":"10.1109/VAST.2008.4677384","Link":"http://dx.doi.org/10.1109/VAST.2008.4677384","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"The VAST 2008 Challenge consisted of four heterogeneous synthetic data sets each organized into separate mini-challenges. The Grand Challenge required integrating the raw data from these four data sets as well as integrating results and findings from team members working on specific mini-challenges. Modeling the problem with a semantic network provided a means for integrating both the raw data and the subjective findings.","AuthorNames-Deduped":"Donald A. Pellegrino;Chi-Chun Pan;Anthony C. Robinson;Michael Stryker;Junyan Luo;Chris Weaver;Prasenjit Mitra;Chaomei Chen;Ian Turton;Alan M. MacEachren","AuthorNames":"Donald Pellegrino;Chi-Chun Pan;Anthony Robinson;Michael Stryker;Junyan Luo;Chris Weaver;Prasenjit Mitra;Chaomei Chen;Ian Turton;Alan MacEachren","AuthorAffiliation":"Drexel University, USA;Penn State University, USA;Penn State University, USA;Penn State University, USA;Penn State University, USA;Penn State University, USA;Penn State University, USA;Drexel University, USA;Penn State University, USA;Penn State University, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":1,"PubsCited":7,"Award":null,"image":""},{"Conference":"VAST","Year":2008,"Title":"Grand challenge award 2008: Support for diverse analytic techniques - nSpace2 and GeoTime visual analytics","DOI":"10.1109/VAST.2008.4677385","Link":"http://dx.doi.org/10.1109/VAST.2008.4677385","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"GeoTime and nSpace2 are interactive visual analytics tools that were used to examine and interpret all four of the 2008 VAST Challenge datasets. GeoTime excels in visualizing event patterns in time and space, or in time and any abstract landscape, while nSpace2 is a web-based analytical tool designed to support every step of the analytical process. nSpace2 is an integrating analytic environment. This paper highlights the VAST analytical experience with these tools that contributed to the success of these tools and this team for the third consecutive year.","AuthorNames-Deduped":"Lynn Chien;Annie Tat;Pascale Proulx;Adeel Khamisa;William Wright","AuthorNames":"Lynn Chien;Annie Tat;Pascale Proulx;Adeel Khamisa;William Wright","AuthorAffiliation":"Oculus Info Inc., USA;Oculus Info Inc., USA;Oculus Info Inc., USA;Oculus Info Inc., USA;Oculus Info Inc., USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":3,"PubsCited":4,"Award":null,"image":"4677385-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Grand challenge award: Interactive visual analytics palantir: The future of analysis","DOI":"10.1109/VAST.2008.4677386","Link":"http://dx.doi.org/10.1109/VAST.2008.4677386","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"Palantir is a world-class analytic platform used worldwide by governmental and financial analysts. This paper provides an introduction to the platform contextualized by its application to the 2008 IEEE VAST contest. In this challenge, we explored a notional dataset about a fabricated religious movement, Catalanopsilas Paraiso Manifesto Movement.","AuthorNames-Deduped":"Jason Payne;Jake Solomon;Ravi Sankar;Bob McGrew","AuthorNames":"Jason Payne;Jake Solomon;Ravi Sankar;Bob McGrew","AuthorAffiliation":"100 Hamilton Ave Suite 300 Palo Alto CA 94301, USA;100 Hamilton Ave Suite 300 Palo Alto CA 94301, USA;100 Hamilton Ave Suite 300 Palo Alto CA 94301, USA;100 Hamilton Ave Suite 300 Palo Alto CA 94301, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":8,"AminerCitationCount_06-2020":11,"XploreCitationCount - 2020-01":3,"PubsCited":0,"Award":null,"image":"4677386-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Migrant boat mini challenge award: Simple and effective integrated display geo-temporal analysis of migrant boats","DOI":"10.1109/VAST.2008.4677387","Link":"http://dx.doi.org/10.1109/VAST.2008.4677387","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"We provide a description of the tools and techniques used in our analysis of the VAST 2008 Challenge dealing with mass movement of persons departing Isla Del Sue.no on boats for the United States during 2005-2007. We used visual analytics to explore migration patterns, characterize the choice and evolution of landing sites, characterize the geographical patterns of interdictions and determine the successful landing rate. Our ComVis tool, in connection with some helper applications and Google Earth, allowed us to explore geo-temporal characteristics of the data set and answer the challenge questions. The ComVis project file captures the visual analysis context and facilitates better collaboration among team members.","AuthorNames-Deduped":"Ranko Miklin;Tomislav Lipic;Zoltan Konyha;Mario Beric;Wolfgang Freiler;Kresimir Matkovic;Denis Gracanin","AuthorNames":"R. Miklin;T. Lipic;Z. Konyha;M. Beric;W. Freiler;K. Matkovic;D. Gracanin","AuthorAffiliation":"Dept. of Telecom.¿FER, Univ. of Zagreb, Croatia;Dept. of Telecom.¿FER, Univ. of Zagreb, Croatia;VRVis Research, Vienna, Austria;Dept. of Telecom.¿FER, Univ. of Zagreb, Croatia;VRVis Research, Vienna, Austria;VRVis Research, Vienna, Austria;Virginia Tech, Blacksburg, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":3,"Award":null,"image":"4677387-fig-1-source-large.gif","hasViewed":false},{"Conference":"VAST","Year":2008,"Title":"Evacuation trace Mini Challenge award: Tool integration analysis of movements with Geospatial Visual Analytics Toolkit","DOI":"10.1109/VAST.2008.4677388","Link":"http://dx.doi.org/10.1109/VAST.2008.4677388","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"The Geospatial Visual Analytics Toolkit intended for exploratory analysis of spatial and spatio-temporal data has been recently enriched with specific visual and computational techniques supporting analysis of data about movement. We applied these and other techniques to the data and tasks of Mini Challenge 4, where it was necessary to analyze tracks of moving people.CR Categories and Subject Descriptors: H.1.2 [User/Machine Systems]: Human information processing - Visual Analytics; 1.6.9 [Visualization]: information visualization.","AuthorNames-Deduped":"Natalia V. Andrienko;Gennady L. Andrienko","AuthorNames":"Natalia Andrienko;Gennady Andrienko","AuthorAffiliation":"Fraunhofer Institute IAIS (Intelligent Analysis and Information Systems), Sankt Augustin, Germany;Fraunhofer Institute IAIS (Intelligent Analysis and Information Systems), Sankt Augustin, Germany","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":0,"PubsCited":3,"Award":null,"image":"4677388-fig-1-source-large.gif","hasViewed":false},{"Conference":"VAST","Year":2008,"Title":"Cell phone mini challenge award: Social network accuracy - exploring temporal communication in mobile call graphs","DOI":"10.1109/VAST.2008.4677389","Link":"http://dx.doi.org/10.1109/VAST.2008.4677389","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"In the mobile call mini challenge of VAST 2008 contest, we explored the temporal communication patterns of Catalano/Vidro social network which is reflected in the mobile call data. We focus on detecting the hierarchy of the social network and try to get the important actors in it. We present our tools and methods in this summary. By using the visual analytic approaches, we can find out not only the temporal communication patterns in the social network but also the hierarchy of it.","AuthorNames-Deduped":"Qi Ye;Tian Zhu 0001;Deyong Hu;Bin Wu 0001;Nan Du;Bai Wang 0001","AuthorNames":"Qi Ye;Tian Zhu;Deyong Hu;Bin Wu;Nan Du;Bai Wang","AuthorAffiliation":"Beijing University of Posts and Telecommunications, Beijing Key Lab of Intelligent Telecommunications Software and Multimedia, China;Beijing University of Posts and Telecommunications, Beijing Key Lab of Intelligent Telecommunications Software and Multimedia, China;Beijing University of Posts and Telecommunications, Beijing Key Lab of Intelligent Telecommunications Software and Multimedia, China;Beijing University of Posts and Telecommunications, Beijing Key Lab of Intelligent Telecommunications Software and Multimedia, China;Beijing University of Posts and Telecommunications, Beijing Key Lab of Intelligent Telecommunications Software and Multimedia, China;Beijing University of Posts and Telecommunications, Beijing Key Lab of Intelligent Telecommunications Software and Multimedia, China","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":23,"AminerCitationCount_06-2020":26,"XploreCitationCount - 2020-01":5,"PubsCited":3,"Award":null,"image":"4677389-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Evacuation traces mini challenge: User testing to obtain consensus discovering the terrorist","DOI":"10.1109/VAST.2008.4677390","Link":"http://dx.doi.org/10.1109/VAST.2008.4677390","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"The adoption of visual analytics methodologies in security applications is an approach that could lead to interesting results. Usually, the data that has to be analyzed finds in a graphical representation its preferred nature, such as spatial or temporal relationships. Due to the nature of these applications, it is very important that key-details are made easy to identify. In the context of the VAST 2008 Challenge, we developed a visualization tool that graphically displays the movement of 82 employees of the Miami Department of Health (USA). We also asked 13 users to identify potential suspects and observe what happened during an evacuation of the building caused by an explosion. In this paper we explain the results of the user testing we conducted and how the users interpreted the event taken into account.","AuthorNames-Deduped":"Adalberto L. Simeone;Paolo Buono","AuthorNames":"Adalberto L. Simeone;Buono Paolo","AuthorAffiliation":"University of Bari, Italy;University of Bari, Italy","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":4,"Award":null,"image":"4677390-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Cell phone mini challenge award: Intuitive social network graphs visual analytics of cell phone data using mobivis and ontovis","DOI":"10.1109/VAST.2008.4677391","Link":"http://dx.doi.org/10.1109/VAST.2008.4677391","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"MobiVis is a visual analytics tools to aid in the process of processing and understanding complex relational data, such as social networks. At the core of these tools is the ability to filter complex networks structurally and semantically, which helps us discover clusters and patterns in the organization of social networks. Semantic filtering is obtained via an ontology graph, based on another visual analytics tool, called OntoVis. In this summary, we describe how these tools where used to analyze one of the mini-challenges of the 2008 VAST challenge.","AuthorNames-Deduped":"Carlos D. Correa;Tarik Crnovrsanin;Chris Muelder;Zeqian Shen;Ryan Armstrong;James Shearer;Kwan-Liu Ma","AuthorNames":"Carlos D. Correa;Tarik Crnovrsanin;Christopher Muelder;Zeqian Shen;Ryan Armstrong;James Shearer;Kwan-Liu Ma","AuthorAffiliation":"Visualization and Interface Design Innovation (VIDI) Group, University of California, Davis, USA;Visualization and Interface Design Innovation (VIDI) Group, University of California, Davis, USA;Visualization and Interface Design Innovation (VIDI) Group, University of California, Davis, USA;Visualization and Interface Design Innovation (VIDI) Group, University of California, Davis, USA;Visualization and Interface Design Innovation (VIDI) Group, University of California, Davis, USA;Visualization and Interface Design Innovation (VIDI) Group, University of California, Davis, USA;Visualization and Interface Design Innovation (VIDI) Group, University of California, Davis, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":6,"XploreCitationCount - 2020-01":2,"PubsCited":2,"Award":null,"image":"4677391-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2008,"Title":"Using SocialAction to uncover structure in social networks over time","DOI":"10.1109/VAST.2008.4677392","Link":"http://dx.doi.org/10.1109/VAST.2008.4677392","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"I describe how SocialAction was used to find insights in an evolving social structure VAST Challenge 2008psilas Mini-Challenge 3. This analysis and SocialAction were given the award, ldquoCell Phone Mini Challenge Award: Time Visualizations of Cell Phone Activityrdquo.","AuthorNames-Deduped":"Adam Perer","AuthorNames":"Adam Perer","AuthorAffiliation":"Human-Computer Interaction Lab & Department of Computer Science, University of Maryland, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":0,"PubsCited":3,"Award":null,"image":"4677392-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Cell phone Mini Challenge: Node-link animation award animating multivariate dynamic social networks","DOI":"10.1109/VAST.2008.4677393","Link":"http://dx.doi.org/10.1109/VAST.2008.4677393","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"This article describes the visualization tool developed for analysing a dynamic social network of phone calls, for the VAST 2008 mini challenge. The tool was designed to highlight temporal changes in the network, by animating different network visual representations. We also explain how animating these network representations, helped to identify key events in the mini challenge problem scenario. Finally, we make some suggestions for future research and development in the area.","AuthorNames-Deduped":"Michael Farrugia;Aaron J. Quigley","AuthorNames":"Michael Farrugia;Aaron Quigley","AuthorAffiliation":"University College Dublin, Ireland;University College Dublin, Ireland","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":27,"AminerCitationCount_06-2020":31,"XploreCitationCount - 2020-01":5,"PubsCited":6,"Award":null,"image":"4677393-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Migrant boat mini challenge award: Analysis summary a geo-temporal analysis of the migrant boat dataset","DOI":"10.1109/VAST.2008.4677394","Link":"http://dx.doi.org/10.1109/VAST.2008.4677394","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"The SPADAC team used various visual analytics tools and methods to find geo-temporal patterns of migration from a Caribbean island from 2005-2007. In this paper, we describe the tools and methods used in the analysis. These methods included generating temporal variograms, dendrograms, and proportionally weighted migration maps, using tools such as the R statistical software package and Signature Analysttrade. We found that there is a significant positive space-time correlation with the boat encounters (especially the landings), with a migratory shift further away from the point of departure over time.","AuthorNames-Deduped":"Benjamin Holland;Lisa Kuchy;Jason Dalton","AuthorNames":"Benjamin Holland;Lisa Kuchy;Jason Dalton","AuthorAffiliation":"SPADAC, Inc., USA;SPADAC, Inc., USA;SPADAC, Inc., USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":3,"Award":null,"image":"4677394-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Evacuation Traces Mini Challenge award: Innovative trace visualization staining for information discovery","DOI":"10.1109/VAST.2008.4677395","Link":"http://dx.doi.org/10.1109/VAST.2008.4677395","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"Staining is a technique for categorizing time-varying spatial data; that is, data of things moving through space over time. In Staining, a stain is applied in either time or space, and the objects which move through the stain become marked. This technique and a research prototype demonstrating the technique were developed in response to the VAST 2008 Contest Mini-challenge: Evacuation Traces.","AuthorNames-Deduped":"Dennis J. Bouvier;Britian Oates","AuthorNames":"Dennis J. Bouvier;Britain Oates","AuthorAffiliation":"Southern Illinois University Edwardsville, USA;Southern Illinois University Edwardsville, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":8,"AminerCitationCount_06-2020":10,"XploreCitationCount - 2020-01":5,"PubsCited":1,"Award":null,"image":"4677395-fig-1-source-large.gif"},{"Conference":"VAST","Year":2008,"Title":"Award: Efficient toolkit integration solving the cell phone calls challenge with the Prajna Project","DOI":"10.1109/VAST.2008.4677396","Link":"http://dx.doi.org/10.1109/VAST.2008.4677396","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"The Prajna Project is a Java toolkit designed to provide various capabilities for visualization, knowledge representation, geographic displays, semantic reasoning, and data fusion. Rather than attempt to recreate the significant capabilities provided in other tools, Prajna instead provides software bridges to incorporate other toolkits where appropriate. This challenge required the development of a custom application for visual analysis. By applying the utilities within the Prajna project, I developed a robust and diverse set of capabilities to solve the analytical challenge.","AuthorNames-Deduped":"Edward Swing","AuthorNames":"Edward Swing","AuthorAffiliation":"Vision Systems & Technology, Inc., USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":6,"XploreCitationCount - 2020-01":2,"PubsCited":5,"Award":null,"image":"4677396-graphic-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Merging visual analysis with automated reasoning: Using Prajna to solve the traffic challenge","DOI":"10.1109/VAST.2009.5332481","Link":"http://dx.doi.org/10.1109/VAST.2009.5332481","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"The Internet traffic challenge required the development of a custom application to analyze internet traffic patterns coupled with building access records. To solve this challenge, the author applied the Prajna Project, an open-source Java toolkit designed to provide various capabilities for visualization, knowledge representation, semantic reasoning, and data fusion. By applying some of the capabilities of Prajna to this challenge, the author could quickly develop a custom application for visual analysis. The author determined that he could solve some of the analytical components of this challenge using automated reasoning techniques. Prajna includes interfaces to incorporate automated reasoners into visual applications. By blending the automated reasoning processes with visual analysis, the author could design a flexible, useful application to solve this challenge.","AuthorNames-Deduped":"Edward Swing","AuthorNames":"Edward Swing","AuthorAffiliation":"Vision Systems & Technology, Inc., USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":4,"Award":null,"image":""},{"Conference":"VAST","Year":2009,"Title":"Professional analysts using a large, high-resolution display","DOI":"10.1109/VAST.2009.5332485","Link":"http://dx.doi.org/10.1109/VAST.2009.5332485","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"Professional cyber analysts were observed as they attempted to solve the VAST 2009 Traffic Mini Challenge using basic visualization tools and a large, high-resolution display. We discuss some of the lessons we learned about how analysts actually work and potential roles for visualization and large, high-resolution displays.","AuthorNames-Deduped":"Alex Endert;Christopher Andrews;Glenn A. Fink;Chris North","AuthorNames":"Alex Endert;Christopher Andrews;Glenn A. Fink;Chris North","AuthorAffiliation":"Virginia Polytechnic Institute and State University, USA;Virginia Polytechnic Institute and State University, USA;Pacific Northwest National Laboratory, USA;Virginia Polytechnic Institute and State University, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":9,"AminerCitationCount_06-2020":9,"XploreCitationCount - 2020-01":6,"PubsCited":2,"Award":null,"image":"5332485-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Interactive visual clustering of large collections of trajectories","DOI":"10.1109/VAST.2009.5332584","Link":"http://dx.doi.org/10.1109/VAST.2009.5332584","FirstPage":3,"LastPage":10,"PaperType":"C","Abstract":"One of the most common operations in exploration and analysis of various kinds of data is clustering, i.e. discovery and interpretation of groups of objects having similar properties and/or behaviors. In clustering, objects are often treated as points in multi-dimensional space of properties. However, structurally complex objects, such as trajectories of moving entities and other kinds of spatio-temporal data, cannot be adequately represented in this manner. Such data require sophisticated and computationally intensive clustering algorithms, which are very hard to scale effectively to large datasets not fitting in the computer main memory. We propose an approach to extracting meaningful clusters from large databases by combining clustering and classification, which are driven by a human analyst through an interactive visual interface.","AuthorNames-Deduped":"Gennady L. Andrienko;Natalia V. Andrienko;Salvatore Rinzivillo;Mirco Nanni;Dino Pedreschi;Fosca Giannotti","AuthorNames":"Gennady Andrienko;Natalia Andrienko;Salvatore Rinzivillo;Mirco Nanni;Dino Pedreschi;Fosca Giannotti","AuthorAffiliation":"Fraunhofer Institute IAIS (Intelligent Analysis and Information Systems), Sankt Augustin, Germany;Fraunhofer Institute IAIS (Intelligent Analysis and Information Systems), Sankt Augustin, Germany;KDD Lab ISTI CNR, Pisa, Italy;KDD Lab ISTI CNR, Pisa, Italy;University of Pisa, Italy;KDD Lab ISTI CNR, Pisa, Italy","InternalReferences":"10.1109/VAST.2008.4677356;10.1109/VAST.2007.4388999","AuthorKeywords":"Spatio-temporal data, movement data, trajectories, clustering, classification, scalable visualization, geovisualization","AminerCitationCount_02-2020":132,"AminerCitationCount_06-2020":148,"XploreCitationCount - 2020-01":93,"PubsCited":26,"Award":null,"image":"5332584-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Guided analysis of hurricane trends using statistical processes integrated with interactive parallel coordinates","DOI":"10.1109/VAST.2009.5332586","Link":"http://dx.doi.org/10.1109/VAST.2009.5332586","FirstPage":19,"LastPage":26,"PaperType":"C","Abstract":"This paper demonstrates the promise of augmenting interactive multivariate representations with information from statistical processes in the domain of weather data analysis. Statistical regression, correlation analysis, and descriptive statistical calculations are integrated via graphical indicators into an enhanced parallel coordinates system, called the Multidimensional Data eXplorer (MDX). These statistical indicators, which highlight significant associations in the data, are complemented with interactive visual analysis capabilities. The resulting system allows a smooth, interactive, and highly visual workflow. The system's utility is demonstrated with an extensive hurricane climate study that was conducted by a hurricane expert. In the study, the expert used a new data set of environmental weather data, composed of 28 independent variables, to predict annual hurricane activity. MDX shows the Atlantic Meridional Mode increases the explained variance of hurricane seasonal activity by 7-15% and removes less significant variables used in earlier studies. The findings and feedback from the expert (1) validate the utility of the data set for hurricane prediction, and (2) indicate that the integration of statistical processes with interactive parallel coordinates, as implemented in MDX, addresses both deficiencies in traditional weather data analysis and exhibits some of the expected benefits of visual data analysis.","AuthorNames-Deduped":"Chad A. Steed;J. Edward Swan;T. J. Jankun-Kelly;Patrick J. Fitzpatrick","AuthorNames":"Chad A. Steed;J. Edward Swan;T.J. Jankun-Kelly;Patrick J. Fitzpatrick","AuthorAffiliation":"Naval Research Laboratory, USA;Mississippi State University, USA;Mississippi State University, USA;Mississippi State University, USA","InternalReferences":"10.1109/TVCG.2007.70523;10.1109/INFVIS.2005.1532138;10.1109/VAST.2006.261452;10.1109/INFVIS.2004.68;10.1109/VISUAL.1995.485139;10.1109/INFVIS.2002.1173157;10.1109/VISUAL.1999.809866;10.1109/TVCG.2006.170","AuthorKeywords":"Climate study, multivariate data, correlation, regression, interaction, statistical analysis, visual analytics","AminerCitationCount_02-2020":24,"AminerCitationCount_06-2020":26,"XploreCitationCount - 2020-01":18,"PubsCited":32,"Award":null,"image":"5332586-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2009,"Title":"Proximity-based visualization of movement trace data","DOI":"10.1109/VAST.2009.5332593","Link":"http://dx.doi.org/10.1109/VAST.2009.5332593","FirstPage":11,"LastPage":18,"PaperType":"C","Abstract":"The increasing availability of motion sensors and video cameras in living spaces has made possible the analysis of motion patterns and collective behavior in a number of situations. The visualization of this movement data, however, remains a challenge. Although maintaining the actual layout of the data space is often desirable, direct visualization of movement traces becomes cluttered and confusing as the spatial distribution of traces may be disparate and uneven. We present proximity-based visualization as a novel approach to the visualization of movement traces in an abstract space rather than the given spatial layout. This abstract space is obtained by considering proximity data, which is computed as the distance between entities and some number of important locations. These important locations can range from a single fixed point, to a moving point, several points, or even the proximities between the entities themselves. This creates a continuum of proximity spaces, ranging from the fixed absolute reference frame to completely relative reference frames. By combining these abstracted views with the concrete spatial views, we provide a way to mentally map the abstract spaces back to the real space. We demonstrate the effectiveness of this approach, and its applicability to visual analytics problems such as hazard prevention, migration patterns, and behavioral studies.","AuthorNames-Deduped":"Tarik Crnovrsanin;Chris Muelder;Carlos D. Correa;Kwan-Liu Ma","AuthorNames":"Tarik Crnovrsanin;Chris Muelder;Carlos Correa;Kwan-Liu Ma","AuthorAffiliation":"University of California, Davis, USA;University of California, Davis, USA;University of California, Davis, USA;University of California, Davis, USA","InternalReferences":"10.1109/INFVIS.2004.27;10.1109/VISUAL.1990.146402;10.1109/TVCG.2007.70621;10.1109/TVCG.2007.70558","AuthorKeywords":"Spatio-temporal visualization, proximity, linked views, principal component analysis, temporal trajectories, movement patterns","AminerCitationCount_02-2020":50,"AminerCitationCount_06-2020":56,"XploreCitationCount - 2020-01":42,"PubsCited":33,"Award":null,"image":"5332593-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Finding comparable temporal categorical records: A similarity measure with an interactive visualization","DOI":"10.1109/VAST.2009.5332595","Link":"http://dx.doi.org/10.1109/VAST.2009.5332595","FirstPage":27,"LastPage":34,"PaperType":"C","Abstract":"An increasing number of temporal categorical databases are being collected: Electronic Health Records in healthcare organizations, traffic incident logs in transportation systems, or student records in universities. Finding similar records within these large databases requires effective similarity measures that capture the searcher's intent. Many similarity measures exist for numerical time series, but temporal categorical records are different. We propose a temporal categorical similarity measure, the M&amp;M (Match &amp; Mismatch) measure, which is based on the concept of aligning records by sentinel events, then matching events between the target and the compared records. The M&amp;M measure combines the time differences between pairs of events and the number of mismatches. To accom-modate customization of parameters in the M&amp;M measure and results interpretation, we implemented Similan, an interactive search and visualization tool for temporal categorical records. A usability study with 8 participants demonstrated that Similan was easy to learn and enabled them to find similar records, but users had difficulty understanding the M&amp;M measure. The usability study feedback, led to an improved version with a continuous timeline, which was tested in a pilot study with 5 participants.","AuthorNames-Deduped":"Krist Wongsuphasawat;Ben Shneiderman","AuthorNames":"Krist Wongsuphasawat;Ben Shneiderman","AuthorAffiliation":"Department of Computer Science & Human-Computer Interaction Lab, University of Maryland, College Park, 20742, USA;Department of Computer Science & Human-Computer Interaction Lab, University of Maryland, College Park, 20742, USA","InternalReferences":"10.1109/VAST.2006.261421","AuthorKeywords":"Similan, M&M Measure, Similarity Search, Temporal Categorical Records","AminerCitationCount_02-2020":56,"AminerCitationCount_06-2020":66,"XploreCitationCount - 2020-01":43,"PubsCited":27,"Award":null,"image":"5332595-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2009,"Title":"A visual analytics system for radio frequency fingerprinting-based localization","DOI":"10.1109/VAST.2009.5332596","Link":"http://dx.doi.org/10.1109/VAST.2009.5332596","FirstPage":35,"LastPage":42,"PaperType":"C","Abstract":"Radio frequency (RF) fingerprinting-based techniques for localization are a promising approach for ubiquitous positioning systems, particularly indoors. By finding unique fingerprints of RF signals received at different locations within a predefined area beforehand, whenever a similar fingerprint is subsequently seen again, the localization system will be able to infer a user's current location. However, developers of these systems face the problem of finding reliable RF fingerprints that are unique enough and adequately stable over time. We present a visual analytics system that enables developers of these localization systems to visually gain insight on whether their collected datasets and chosen fingerprint features have the necessary properties to enable a reliable RF fingerprinting-based localization system. The system was evaluated by testing and debugging an existing localization system.","AuthorNames-Deduped":"Yi Han 0005;Erich P. Stuntebeck;John T. Stasko;Gregory D. Abowd","AuthorNames":"Yi Han;Erich P. Stuntebeck;John T. Stasko;Gregory D. Abowd","AuthorAffiliation":"School of Interactive Computing & GVU Center, Georgia Institute of Technology, USA;School of Interactive Computing & GVU Center, Georgia Institute of Technology, USA;School of Interactive Computing & GVU Center, Georgia Institute of Technology, USA;School of Interactive Computing & GVU Center, Georgia Institute of Technology, USA","InternalReferences":"10.1109/INFVIS.1997.636793","AuthorKeywords":null,"AminerCitationCount_02-2020":10,"AminerCitationCount_06-2020":10,"XploreCitationCount - 2020-01":6,"PubsCited":15,"Award":null,"image":"5332596-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Geovisual analytics for self-organizing network data","DOI":"10.1109/VAST.2009.5332610","Link":"http://dx.doi.org/10.1109/VAST.2009.5332610","FirstPage":43,"LastPage":50,"PaperType":"C","Abstract":"Cellular radio networks are continually growing in both node count and complexity. It therefore becomes more difficult to manage the networks and necessary to use time and cost effective automatic algorithms to organize the networks neighbor cell relations. There have been a number of attempts to develop such automatic algorithms. Network operators, however, may not trust them because they need to have an understanding of their behavior and of their reliability and performance, which is not easily perceived. This paper presents a novel Web-enabled geovisual analytics approach to exploration and understanding of self-organizing network data related to cells and neighbor cell relations. A demonstrator and case study are presented in this paper, developed in close collaboration with the Swedish telecom company Ericsson and based on large multivariate, time-varying and geospatial data provided by the company. It allows the operators to follow, interact with and analyze the evolution of a self-organizing network and enhance their understanding of how an automatic algorithm configures locally-unique physical cell identities and organizes neighbor cell relations of the network. The geovisual analytics tool is tested with a self-organizing network that is operated by the automatic neighbor relations (ANR) algorithm. The demonstrator has been tested with positive results by a group of domain experts from Ericsson and will be tested in production.","AuthorNames-Deduped":"Ho Van Quan;Tobias Åström;Mikael Jern","AuthorNames":"Van Quan Ho;Tobias Astrom;Mikael Jern","AuthorAffiliation":"National Center for Visual Analytics, Dept. of Science and Technology, Linköping University, Sweden;National Center for Visual Analytics, Dept. of Science and Technology, Linköping University, Sweden;National Center for Visual Analytics, Dept. of Science and Technology, Linköping University, Sweden","InternalReferences":"10.1109/VISUAL.1999.809930","AuthorKeywords":"Geovisual analytics, visualization, self-organizing network, multi-layer, multi-dimensional, time-varying, geospatial data sets","AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":0,"PubsCited":22,"Award":null,"image":"5332610-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"A framework for uncertainty-aware visual analytics","DOI":"10.1109/VAST.2009.5332611","Link":"http://dx.doi.org/10.1109/VAST.2009.5332611","FirstPage":51,"LastPage":58,"PaperType":"C","Abstract":"Visual analytics has become an important tool for gaining insight on large and complex collections of data. Numerous statistical tools and data transformations, such as projections, binning and clustering, have been coupled with visualization to help analysts understand data better and faster. However, data is inherently uncertain, due to error, noise or unreliable sources. When making decisions based on uncertain data, it is important to quantify and present to the analyst both the aggregated uncertainty of the results and the impact of the sources of that uncertainty. In this paper, we present a new framework to support uncertainty in the visual analytics process, through statistic methods such as uncertainty modeling, propagation and aggregation. We show that data transformations, such as regression, principal component analysis and k-means clustering, can be adapted to account for uncertainty. This framework leads to better visualizations that improve the decision-making process and help analysts gain insight on the analytic process itself.","AuthorNames-Deduped":"Carlos D. Correa;Yu-Hsuan Chan;Kwan-Liu Ma","AuthorNames":"Carlos D. Correa;Yu-Hsuan Chan;Kwan-Liu Ma","AuthorAffiliation":"University of California at Davis, USA;University of California at Davis, USA;University of California at Davis, USA","InternalReferences":"10.1109/VAST.2008.4677368;10.1109/VAST.2007.4389000","AuthorKeywords":"Uncertainty, Data Transformations, Principal Component Analysis, Model fitting","AminerCitationCount_02-2020":60,"AminerCitationCount_06-2020":76,"XploreCitationCount - 2020-01":51,"PubsCited":35,"Award":null,"image":"5332611-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Combining automated analysis and visualization techniques for effective exploration of high-dimensional data","DOI":"10.1109/VAST.2009.5332628","Link":"http://dx.doi.org/10.1109/VAST.2009.5332628","FirstPage":59,"LastPage":66,"PaperType":"C","Abstract":"Visual exploration of multivariate data typically requires projection onto lower-dimensional representations. The number of possible representations grows rapidly with the number of dimensions, and manual exploration quickly becomes ineffective or even unfeasible. This paper proposes automatic analysis methods to extract potentially relevant visual structures from a set of candidate visualizations. Based on features, the visualizations are ranked in accordance with a specified user task. The user is provided with a manageable number of potentially useful candidate visualizations, which can be used as a starting point for interactive data analysis. This can effectively ease the task of finding truly useful visualizations and potentially speed up the data exploration task. In this paper, we present ranking measures for class-based as well as non class-based Scatterplots and Parallel Coordinates visualizations. The proposed analysis methods are evaluated on different datasets.","AuthorNames-Deduped":"Andrada Tatu;Georgia Albuquerque;Martin Eisemann;Jörn Schneidewind;Holger Theisel;Marcus A. Magnor;Daniel A. Keim","AuthorNames":"Andrada Tatu;Georgia Albuquerque;Martin Eisemann;Jorn Schneidewind;Holger Theisel;Marcus Magnork;Daniel Keim","AuthorAffiliation":"University of Konstanz, Germany;TU Braunschweig, Germany;TU Braunschweig, Germany;Telefonica o2 Business, Intelligence Center, Germany;University of Magdeburg, Germany;TU Braunschweig, Germany;University of Konstanz, Germany","InternalReferences":"10.1109/INFVIS.2005.1532142;10.1109/INFVIS.1998.729559;10.1109/INFVIS.2003.1249017;10.1109/VISUAL.1994.346302;10.1109/VAST.2006.261423","AuthorKeywords":null,"AminerCitationCount_02-2020":83,"AminerCitationCount_06-2020":98,"XploreCitationCount - 2020-01":76,"PubsCited":25,"Award":null,"image":"5332628-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Two-stage framework for visualization of clustered high dimensional data","DOI":"10.1109/VAST.2009.5332629","Link":"http://dx.doi.org/10.1109/VAST.2009.5332629","FirstPage":67,"LastPage":74,"PaperType":"C","Abstract":"In this paper, we discuss dimension reduction methods for 2D visualization of high dimensional clustered data. We propose a two-stage framework for visualizing such data based on dimension reduction methods. In the first stage, we obtain the reduced dimensional data by applying a supervised dimension reduction method such as linear discriminant analysis which preserves the original cluster structure in terms of its criteria. The resulting optimal reduced dimension depends on the optimization criteria and is often larger than 2. In the second stage, the dimension is further reduced to 2 for visualization purposes by another dimension reduction method such as principal component analysis. The role of the second-stage is to minimize the loss of information due to reducing the dimension all the way to 2. Using this framework, we propose several two-stage methods, and present their theoretical characteristics as well as experimental comparisons on both artificial and real-world text data sets.","AuthorNames-Deduped":"Jaegul Choo;Shawn Bohn;Haesun Park","AuthorNames":"Jaegul Choo;Shawn Bohn;Haesun Park","AuthorAffiliation":"College of Computing, Georgia Institute of Technology, 266 Ferst Drive, Atlanta, 30332, USA;National Visualization and Analytics Center, Pacific Northwest National Laboratory, 902 Battelle Blvd, Richland, WA 99354, USA;College of Computing, Georgia Institute of Technology, 266 Ferst Drive, Atlanta, 30332, USA","InternalReferences":"10.1109/INFVIS.2003.1249017","AuthorKeywords":"dimension reduction, linear discriminant analysis, principal component analysis, orthogonal centroid method, 2D projection, clustered data, regularization, generalized singular value decomposition","AminerCitationCount_02-2020":36,"AminerCitationCount_06-2020":39,"XploreCitationCount - 2020-01":28,"PubsCited":21,"Award":null,"image":"5332629-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Capturing and supporting the analysis process","DOI":"10.1109/VAST.2009.5333020","Link":"http://dx.doi.org/10.1109/VAST.2009.5333020","FirstPage":131,"LastPage":138,"PaperType":"C","Abstract":"Visual analytics tools provide powerful visual representations in order to support the sense-making process. In this process, analysts typically iterate through sequences of steps many times, varying parameters each time. Few visual analytics tools support this process well, nor do they provide support for visualizing and understanding the analysis process itself. To help analysts understand, explore, reference, and reuse their analysis process, we present a visual analytics system named CzSaw (See-Saw) that provides an editable and re-playable history navigation channel in addition to multiple visual representations of document collections and the entities within them (in a manner inspired by Jigsaw). Conventional history navigation tools range from basic undo and redo to branching timelines of user actions. In CzSaw's approach to this, first, user interactions are translated into a script language that drives the underlying scripting-driven propagation system. The latter allows analysts to edit analysis steps, and ultimately to program them. Second, on this base, we build both a history view showing progress and alternative paths, and a dependency graph showing the underlying logic of the analysis and dependency relations among the results of each step. These tools result in a visual model of the sense-making process, providing a way for analysts to visualize their analysis process, to reinterpret the problem, explore alternative paths, extract analysis patterns from existing history, and reuse them with other related analyses.","AuthorNames-Deduped":"Nazanin Kadivar;Victor Y. Chen;Dustin Dunsmuir;Eric Lee;Cheryl Z. Qian;John Dill;Chris Shaw 0002;Robert F. Woodbury","AuthorNames":"Nazanin Kadivar;Victor Chen;Dustin Dunsmuir;Eric Lee;Cheryl Qian;John Dill;Christopher Shaw;Robert Woodbury","AuthorAffiliation":"School of Interactive Arts and Technology, Simon Fraser University, Canada;School of Interactive Arts and Technology, Simon Fraser University, Canada;School of Interactive Arts and Technology, Simon Fraser University, Canada;School of Interactive Arts and Technology, Simon Fraser University, Canada;School of Interactive Arts and Technology, Simon Fraser University, Canada;School of Interactive Arts and Technology, Simon Fraser University, Canada;School of Interactive Arts and Technology, Simon Fraser University, Canada;School of Interactive Arts and Technology, Simon Fraser University, Canada","InternalReferences":"10.1109/INFVIS.2005.1532136;10.1109/VAST.2008.4677362;10.1109/VAST.2007.4388992;10.1109/TVCG.2008.137;10.1109/VAST.2007.4389006;10.1109/VAST.2008.4677365;10.1109/INFVIS.2004.2;10.1109/VAST.2007.4389002;10.1109/TVCG.2007.70515;10.1109/VAST.2007.4389001","AuthorKeywords":"Visual Analytics, Sense-making, Analysis Process, Visual History","AminerCitationCount_02-2020":54,"AminerCitationCount_06-2020":57,"XploreCitationCount - 2020-01":33,"PubsCited":28,"Award":null,"image":"5333020-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Connecting the dots in visual analysis","DOI":"10.1109/VAST.2009.5333023","Link":"http://dx.doi.org/10.1109/VAST.2009.5333023","FirstPage":123,"LastPage":130,"PaperType":"C","Abstract":"During visual analysis, users must often connect insights discovered at various points of time. This process is often called ldquoconnecting the dots.rdquo When analysts interactively explore complex datasets over multiple sessions, they may uncover a large number of findings. As a result, it is often difficult for them to recall the past insights, views and concepts that are most relevant to their current line of inquiry. This challenge is even more difficult during collaborative analysis tasks where they need to find connections between their own discoveries and insights found by others. In this paper, we describe a context-based retrieval algorithm to identify notes, views and concepts from users' past analyses that are most relevant to a view or a note based on their line of inquiry. We then describe a related notes recommendation feature that surfaces the most relevant items to the user as they work based on this algorithm. We have implemented this recommendation feature in HARVEST, a Web based visual analytic system. We evaluate the related notes recommendation feature of HARVEST through a case study and discuss the implications of our approach.","AuthorNames-Deduped":"Yedendra Babu Shrinivasan;David Gotz;Jie Lu","AuthorNames":"Yedendra B. Shrinivasan;David Gotzy;Jie Lu","AuthorAffiliation":"Eindhoven University of Technology, The Netherlands;IBM Research, USA;IBM Research, USA","InternalReferences":"10.1109/VAST.2008.4677362;10.1109/VAST.2007.4389006;10.1109/VAST.2007.4389011;10.1109/VAST.2006.261432;10.1109/VAST.2008.4677365","AuthorKeywords":null,"AminerCitationCount_02-2020":28,"AminerCitationCount_06-2020":28,"XploreCitationCount - 2020-01":12,"PubsCited":20,"Award":null,"image":"5333023-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Interactive poster: Interactive multiobjective optimization - a new application area for visual analytics","DOI":"10.1109/VAST.2009.5333081","Link":"http://dx.doi.org/10.1109/VAST.2009.5333081","FirstPage":237,"LastPage":238,"PaperType":"M","Abstract":"The poster introduces interactive multiobjective optimization (IMO) as a field offering new application possibilities and challenges for visual analytics (VA), and aims at inspiring collaboration between the two fields. Our aim is to collect new ideas in order to be able to utilize VA techniques more effectively in our user interface development. Simulation-based IMO methods are developed for complex problem solving, where the expert decision maker (analyst) should be supported during the iterative process of eliciting preference information and examining the resulting output data. IMO is a subfield of multiple criteria decision making (MCDM). In simulation-based IMO, the optimization task is formulated in a mathematical model containing several conflicting objectives and constraints depending on decision variables. While using IMO methods the analyst progressively provides preference information in order to find the most satisfactory compromise between the conflicting objectives. In the poster, the implementations of two new IMO methods are used as examples to demonstrate concrete challenges of interaction design. One of them is described in this summary.","AuthorNames-Deduped":"Suvi Tarkkanen;Kaisa Miettinen;Jussi Hakanen","AuthorNames":"Suvi Tarkkanen;Kaisa Miettinen;Jussi Hakanen","AuthorAffiliation":"Department of Mathematical Information Technology, University of Jyväskylä, Finland;Department of Mathematical Information Technology, University of Jyväskylä, Finland;Department of Mathematical Information Technology, University of Jyväskylä, Finland","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":5,"XploreCitationCount - 2020-01":5,"PubsCited":6,"Award":null,"image":"5333081-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Poster: Icexplorer: Studying Great Lakes Ice cover","DOI":"10.1109/VAST.2009.5333082","Link":"http://dx.doi.org/10.1109/VAST.2009.5333082","FirstPage":239,"LastPage":240,"PaperType":"M","Abstract":"IceXplorer is a tool for analyzing variations in ice cover on Lake Erie. It enhances the data and pre-packaged analysis currently available in the great lakes ice atlas and serves as an example of a small, focused application where simple but carefully-chosen visualizations, interaction techniques, and automated data analysis are combined to create an effective tool for advancing scientific research.","AuthorNames-Deduped":"Stina S. Bridgeman","AuthorNames":"Stina Bridgeman","AuthorAffiliation":"Hobart and William Smith Colleges, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":6,"Award":null,"image":"5333082-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2009,"Title":"Articulate: a conversational interface for visual analytics","DOI":"10.1109/VAST.2009.5333099","Link":"http://dx.doi.org/10.1109/VAST.2009.5333099","FirstPage":233,"LastPage":234,"PaperType":"M","Abstract":"While many visualization tools exist that offer sophisticated functions for charting complex data, they still expect users to possess a high degree of expertise in wielding the tools to create an effective visualization. This poster presents Articulate, an attempt at a semi-automated visual analytic model that is guided by a conversational user interface. The goal is to relieve the user of the physical burden of having to directly craft a visualization through the manipulation of a complex user-interface, by instead being able to verbally articulate what the user wants to see, and then using natural language processing and heuristics to semi-automatically create a suitable visualization.","AuthorNames-Deduped":"Yiwen Sun;Jason Leigh;Andrew E. Johnson;Dennis Chau","AuthorNames":"Yiwen Sun;Jason Leigh;Andrew Johnson;Dennis Chau","AuthorAffiliation":"Electronic Visualization Laboratory, University of Illinois at Chicago, USA;Electronic Visualization Laboratory, University of Illinois at Chicago, USA;Electronic Visualization Laboratory, University of Illinois at Chicago, USA;Electronic Visualization Laboratory, University of Illinois at Chicago, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":6,"XploreCitationCount - 2020-01":1,"PubsCited":9,"Award":null,"image":"5333099-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"VAST contest dataset use in education","DOI":"10.1109/VAST.2009.5333245","Link":"http://dx.doi.org/10.1109/VAST.2009.5333245","FirstPage":115,"LastPage":122,"PaperType":"C","Abstract":"The IEEE Visual Analytics Science and Technology (VAST) Symposium has held a contest each year since its inception in 2006. These events are designed to provide visual analytics researchers and developers with analytic challenges similar to those encountered by professional information analysts. The VAST contest has had an extended life outside of the symposium, however, as materials are being used in universities and other educational settings, either to help teachers of visual analytics-related classes or for student projects. We describe how we develop VAST contest datasets that results in products that can be used in different settings and review some specific examples of the adoption of the VAST contest materials in the classroom. The examples are drawn from graduate and undergraduate courses at Virginia Tech and from the Visual Analytics ldquoSummer Camprdquo run by the National Visualization and Analytics Center in 2008. We finish with a brief discussion on evaluation metrics for education.","AuthorNames-Deduped":"Mark A. Whiting;Chris North;Alex Endert;Jean Scholtz;Jereme Haack;Carrie Varley;James J. Thomas","AuthorNames":"Mark A. Whiting;Chris North;Alex Endert;Jean Scholtz;Jereme Haack;Carrie Varley;Jim Thomas","AuthorAffiliation":"Pacific Northwest National Laboratory, USA;Virginia Tech, Department of Computer Science, USA;Virginia Tech, Department of Computer Science, USA;Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA","InternalReferences":"10.1109/VAST.2006.261416","AuthorKeywords":"education, evaluation, synthetic data","AminerCitationCount_02-2020":8,"AminerCitationCount_06-2020":8,"XploreCitationCount - 2020-01":3,"PubsCited":24,"Award":null,"image":"5333245-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"What's being said near \"Martha\"? Exploring name entities in literary text collections","DOI":"10.1109/VAST.2009.5333248","Link":"http://dx.doi.org/10.1109/VAST.2009.5333248","FirstPage":107,"LastPage":114,"PaperType":"C","Abstract":"A common task in literary analysis is to study characters in a novel or collection. Automatic entity extraction, text analysis and effective user interfaces facilitate character analysis. Using our interface, called POSvis, the scholar uses word clouds and self-organizing graphs to review vocabulary, to filter by part of speech, and to explore the network of characters located near characters under review. Further, visualizations show word usages within an analysis window (i.e. a book chapter), which can be compared with a reference window (i.e. the whole book). We describe the interface and report on an early case study with a humanities scholar.","AuthorNames-Deduped":"Romain Vuillemot;Tanya E. Clement;Catherine Plaisant;Amit Kumar","AuthorNames":"Romain Vuillemot;Tanya Clement;Catherine Plaisant;Amit Kumar","AuthorAffiliation":"Université de Lyon, France;University of Maryland, USA;University of Maryland, USA;University of Illinois, Urbana Champaign, USA","InternalReferences":"10.1109/TVCG.2008.172;10.1109/TVCG.2007.70577;10.1109/VAST.2008.4677359;10.1109/VAST.2007.4389004;10.1109/VAST.2007.4389006","AuthorKeywords":"Visual Analytics, Design, Experimentation, Human Factors","AminerCitationCount_02-2020":32,"AminerCitationCount_06-2020":45,"XploreCitationCount - 2020-01":24,"PubsCited":19,"Award":null,"image":"5333248-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2009,"Title":"BEADS: High dimensional data cluster visualization","DOI":"10.1109/VAST.2009.5333417","Link":"http://dx.doi.org/10.1109/VAST.2009.5333417","FirstPage":235,"LastPage":236,"PaperType":"M","Abstract":"In this poster paper, we present BEADS, a high dimensional data cluster visualization by having a 2-D representation of shape and spread of the cluster. The Cluster Division component, the Bead Shape Identification and Cluster Shape Composition form the core of the system. BEADS visualization consists of a 2-D plot, standard 2-D shapes which are used as metaphors to represent corresponding high-dimensional shapes of beads. The final resulting images convey the relative placement of beads with respect to the cluster center, the shape of the beads. We give a textual summary of the beads and their 2-D placement on the Beads plot in tabular format along with the image.","AuthorNames-Deduped":"Soujanya Vadapalli;Kamalakar Karlapalem","AuthorNames":"Soujanya Vadapalli;Kamalakar Karlapalem","AuthorAffiliation":"Centre for Data Engineering, International Institute of Information Technology-Hyderabad, INDIA;Centre for Data Engineering, International Institute of Information Technology-Hyderabad, INDIA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":4,"Award":null,"image":"5333417-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Poster: Visual prediction of time series","DOI":"10.1109/VAST.2009.5333420","Link":"http://dx.doi.org/10.1109/VAST.2009.5333420","FirstPage":229,"LastPage":230,"PaperType":"M","Abstract":"Many well-known time series prediction methods have been used daily by analysts making decisions. To reach a good prediction, we introduce several new visual analysis techniques of smoothing, multi-scaling, and weighted average with the involvement of human expert knowledge. We combine them into a well-fitted method to perform prediction. We have applied this approach to predict resource consumption in data center for next day planning.","AuthorNames-Deduped":"Ming C. Hao;Halldór Janetzko;Ratnesh K. Sharma;Umeshwar Dayal;Daniel A. Keim;Malú Castellanos","AuthorNames":"Ming C. Hao;Halldor Janetzko;Ratnesh K. Sharma;Umeshwar Dayal;Daniel A. Keim;Malu Castellanos","AuthorAffiliation":"Hewlett-Packard Labs, USA;University of Konstanz, Germany;Hewlett-Packard Labs, USA;Hewlett-Packard Labs, USA;University of Konstanz, Germany;Hewlett-Packard Labs, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":7,"XploreCitationCount - 2020-01":2,"PubsCited":4,"Award":null,"image":"5333420-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"ProcessLine: Visualizing time-series data in process industry","DOI":"10.1109/VAST.2009.5333421","Link":"http://dx.doi.org/10.1109/VAST.2009.5333421","FirstPage":231,"LastPage":232,"PaperType":"M","Abstract":"In modern process industry, it is often difficult to analyze a manufacture process due to its numerous time-series data. Analysts wish to not only interpret the evolution of data over time in a working procedure, but also examine the changes in the whole production process through time. To meet such analytic requirements, we have developed ProcessLine, an interactive visualization tool for a large amount of time-series data in process industry. The data are displayed in a fisheye timeline. ProcessLine provides good overviews for the whole production process and details for the focused working procedure. A preliminary user study using beer industry production data has shown that the tool is effective.","AuthorNames-Deduped":"XiongFei Luo;Hongan Wang;Feng Tian 0001;Wei Liu 0023;Dongxing Teng;Guozhong Dai","AuthorNames":"Xiongfei Luo;Hongan Wang;Feng Tian;Wei Liu;Dongxing Teng;Guozhong Dai","AuthorAffiliation":"Graduate University, Chinese Academy of Sciences, Beijing, China;Institute of Software, Chinese Academy of Sciences, Beijing, China;Institute of Software, Chinese Academy of Sciences, Beijing, China;Graduate University, Chinese Academy of Sciences, Beijing, China;Institute of Software, Chinese Academy of Sciences, Beijing, China;Institute of Software, Chinese Academy of Sciences, Beijing, China","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":6,"Award":null,"image":"5333421-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2009,"Title":"LSAView: A tool for visual exploration of latent semantic modeling","DOI":"10.1109/VAST.2009.5333428","Link":"http://dx.doi.org/10.1109/VAST.2009.5333428","FirstPage":83,"LastPage":90,"PaperType":"C","Abstract":"Latent Semantic Analysis (LSA) is a commonly-used method for automated processing, modeling, and analysis of unstructured text data. One of the biggest challenges in using LSA is determining the appropriate model parameters to use for different data domains and types of analyses. Although automated methods have been developed to make rank and scaling parameter choices, these approaches often make choices with respect to noise in the data, without an understanding of how those choices impact analysis and problem solving. Further, no tools currently exist to explore the relationships between an LSA model and analysis methods. Our work focuses on how parameter choices impact analysis and problem solving. In this paper, we present LSAView, a system for interactively exploring parameter choices for LSA models. We illustrate the use of LSAView's small multiple views, linked matrix-graph views, and data views to analyze parameter selection and application in the context of graph layout and clustering.","AuthorNames-Deduped":"Patricia Crossno;Daniel M. Dunlavy;Timothy M. Shead","AuthorNames":"Patricia J. Crossno;Daniel M. Dunlavy;Timothy M. Shead","AuthorAffiliation":"Sandia National Laboratories, USA;Sandia National Laboratories, USA;Sandia National Laboratories, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":15,"AminerCitationCount_06-2020":16,"XploreCitationCount - 2020-01":11,"PubsCited":27,"Award":null,"image":"5333428-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2009,"Title":"Model space visualization for multivariate linear trend discovery","DOI":"10.1109/VAST.2009.5333431","Link":"http://dx.doi.org/10.1109/VAST.2009.5333431","FirstPage":75,"LastPage":82,"PaperType":"C","Abstract":"Discovering and extracting linear trends and correlations in datasets is very important for analysts to understand multivariate phenomena. However, current widely used multivariate visualization techniques, such as parallel coordinates and scatterplot matrices, fail to reveal and illustrate such linear relationships intuitively, especially when more than 3 variables are involved or multiple trends coexist in the dataset. We present a novel multivariate model parameter space visualization system that helps analysts discover single and multiple linear patterns and extract subsets of data that fit a model well. Using this system, analysts are able to explore and navigate in model parameter space, interactively select and tune patterns, and refine the model for accuracy using computational techniques. We build connections between model space and data space visually, allowing analysts to employ their domain knowledge during exploration to better interpret the patterns they discover and their validity. Case studies with real datasets are used to investigate the effectiveness of the visualizations.","AuthorNames-Deduped":"Zhenyu Guo;Matthew O. Ward;Elke A. Rundensteiner","AuthorNames":"Zhenyu Guo;Matthew O. Ward;Elke A. Rundensteiner","AuthorAffiliation":"Computer Science Department, Worcester Polytechnic Institute, USA;Computer Science Department, Worcester Polytechnic Institute, USA;Computer Science Department, Worcester Polytechnic Institute, USA","InternalReferences":"10.1109/VAST.2008.4677350;10.1109/VAST.2007.4389000;10.1109/VAST.2008.4677363;10.1109/VAST.2007.4388999;10.1109/VISUAL.1990.146402;10.1109/VAST.2008.4677352;10.1109/VAST.2008.4677368","AuthorKeywords":"Knowledge Discovery, visual analysis, multivariate linear model construction, model space visualization","AminerCitationCount_02-2020":25,"AminerCitationCount_06-2020":27,"XploreCitationCount - 2020-01":19,"PubsCited":21,"Award":null,"image":"5333431-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Reordered tilebars for visual text exploration","DOI":"10.1109/VAST.2009.5333436","Link":"http://dx.doi.org/10.1109/VAST.2009.5333436","FirstPage":225,"LastPage":226,"PaperType":"M","Abstract":"The classic TileBars paradigm has been used to show distribution information of query terms in full-text documents. However, when the number of query terms becomes large, it is not an easy task for users to comprehend their distribution within certain parts of a document. In this paper, we present a novel approach to improve the visual presentation of TileBars, in which barycenter heuristic for bigraph crossing minimization is used to reorder TileBars elements. The reordered TileBars can be demonstrated to provide users with better focus and navigation while exploring text documents.","AuthorNames-Deduped":"VinhTuan Thai;Siegfried Handschuh","AuthorNames":"VinhTuan Thai;Siegfried Handschuh","AuthorAffiliation":"Digital Enterprise Research Institute, National University of Ireland, Galway, Ireland;Digital Enterprise Research Institute, National University of Ireland, Galway, Ireland","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":9,"Award":null,"image":"5333436-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2009,"Title":"Describing story evolution from dynamic information streams","DOI":"10.1109/VAST.2009.5333437","Link":"http://dx.doi.org/10.1109/VAST.2009.5333437","FirstPage":99,"LastPage":106,"PaperType":"C","Abstract":"Sources of streaming information, such as news syndicates, publish information continuously. Information portals and news aggregators list the latest information from around the world enabling information consumers to easily identify events in the past 24 hours. The volume and velocity of these streams causes information from prior days to quickly vanish despite its utility in providing an informative context for interpreting new information. Few capabilities exist to support an individual attempting to identify or understand trends and changes from streaming information over time. The burden of retaining prior information and integrating with the new is left to the skills, determination, and discipline of each individual. In this paper we present a visual analytics system for linking essential content from information streams over time into dynamic stories that develop and change over multiple days. We describe particular challenges to the analysis of streaming information and present a fundamental visual representation for showing story change and evolution over time.","AuthorNames-Deduped":"Stuart J. Rose;Scott Butner;Wendy Cowley;Michelle L. Gregory;Julia Walker","AuthorNames":"Stuart Rose;Scott Butner;Wendy Cowley;Michelle Gregory;Julia Walker","AuthorAffiliation":"Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA","InternalReferences":"10.1109/INFVIS.2000.885098;10.1109/INFVIS.2005.1532133","AuthorKeywords":null,"AminerCitationCount_02-2020":29,"AminerCitationCount_06-2020":29,"XploreCitationCount - 2020-01":16,"PubsCited":18,"Award":null,"image":"5333437-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Visual knowledge exploration and discovery from different points of view","DOI":"10.1109/VAST.2009.5333438","Link":"http://dx.doi.org/10.1109/VAST.2009.5333438","FirstPage":227,"LastPage":228,"PaperType":"M","Abstract":"Complex scenario analysis requires the exploration of multiple hypotheses and supporting evidence for each argument posed. Knowledge-intensive organisations typically analyse large amounts of inter-related, heterogeneous data to retrieve the knowledge this contains and use it to support effective decision-making. We demonstrate the use of interactive graph visualisation to support hierarchical, task-driven, hypothesis investigation. The visual investigative analysis is guided by task and domain ontologies used to capture the structure of the investigation process and the experience gained and knowledge created in previous, related investigations.","AuthorNames-Deduped":"Aba-Sah Dadzie;Daniela Petrelli","AuthorNames":"Aba-Sah Dadzie;Daniela Petrelli","AuthorAffiliation":"Dept. of Information Studies, The University of Sheffield, UK;Dept. of Information Studies, The University of Sheffield, UK","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":0,"PubsCited":6,"Award":null,"image":"5333438-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Parallel Tag Clouds to explore and analyze faceted text corpora","DOI":"10.1109/VAST.2009.5333443","Link":"http://dx.doi.org/10.1109/VAST.2009.5333443","FirstPage":91,"LastPage":98,"PaperType":"C","Abstract":"Do court cases differ from place to place? What kind of picture do we get by looking at a country's collection of law cases? We introduce parallel tag clouds: a new way to visualize differences amongst facets of very large metadata-rich text corpora. We have pointed parallel tag clouds at a collection of over 600,000 US Circuit Court decisions spanning a period of 50 years and have discovered regional as well as linguistic differences between courts. The visualization technique combines graphical elements from parallel coordinates and traditional tag clouds to provide rich overviews of a document collection while acting as an entry point for exploration of individual texts. We augment basic parallel tag clouds with a details-in-context display and an option to visualize changes over a second facet of the data, such as time. We also address text mining challenges such as selecting the best words to visualize, and how to do so in reasonable time periods to maintain interactivity.","AuthorNames-Deduped":"Christopher Collins 0001;Fernanda B. Viégas;Martin Wattenberg","AuthorNames":"Christopher Collins;Fernanda B. Viegas;Martin Wattenberg","AuthorAffiliation":"University of Toronto, Canada;IBM Research, USA;IBM Research, USA","InternalReferences":"10.1109/INFVIS.1995.528686;10.1109/TVCG.2007.70589;10.1109/TVCG.2008.175;10.1109/TVCG.2008.172;10.1109/VAST.2007.4389006;10.1109/TVCG.2006.166","AuthorKeywords":"Text visualization, corpus visualization, information retrieval, text mining, tag clouds","AminerCitationCount_02-2020":141,"AminerCitationCount_06-2020":176,"XploreCitationCount - 2020-01":109,"PubsCited":35,"Award":"TT","image":"5333443-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"A scalable architecture for visual data exploration","DOI":"10.1109/VAST.2009.5333451","Link":"http://dx.doi.org/10.1109/VAST.2009.5333451","FirstPage":221,"LastPage":222,"PaperType":"M","Abstract":"Intelligence analysts in the areas of defense and homeland security are now faced with the difficult problem of discerning the relevant details amidst massive data stores. We propose a component-based visualization architecture that is built specifically to encourage the flexible exploration of geospatial event databases. The proposed system is designed to deploy on a variety of display layouts, from a single laptop screen to a multi-monitor tiled-display. By utilizing a combination of parallel coordinates, principal components plots, and other data views, analysts may reduce the dimensionality of a data set to its most salient features. Of particular value to our target applications are understanding correlations between data layers, both within a single view and across multiple views. Our proposed system aims to address the limited scalability associated with coordinated multiple views (CMVs) through the implementation of an efficient core application which is extensible by the end-user.","AuthorNames-Deduped":"Jonathan W. Decker;Alex Godwin;Mark A. Livingston;Denise Royle","AuthorNames":"Jonathan Decker;Alex Godwin;Mark A. Livingston;Denise Royle","AuthorAffiliation":"Naval Research Laboratory, USA;Naval Research Laboratory, USA;Naval Research Laboratory, USA;Naval Research Laboratory, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":7,"Award":null,"image":"5333451-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Interactive visual analysis of location reporting patterns","DOI":"10.1109/VAST.2009.5333453","Link":"http://dx.doi.org/10.1109/VAST.2009.5333453","FirstPage":223,"LastPage":224,"PaperType":"M","Abstract":"Interactive visualization methods are often used to aid in the analysis of large datasets. We present a novel interactive visualization technique designed specifically for the analysis of location reporting patterns within large time-series datasets. We use a set of triangles with color coding to indicate the time between location reports. This allows reporting patterns (expected and unexpected) to be easily discerned during interactive analysis. We discuss the details of our method and describe evaluation both from expert opinion and from a user study.","AuthorNames-Deduped":"Derek Overby;John Keyser;Jim Wall","AuthorNames":"Derek Overby;John Keyser;Jim Wall","AuthorAffiliation":"Department of Industrial and Systems Engineering, Texas A&M University, USA;Department of Industrial and Systems Engineering, Texas A&M University, USA;Department of Industrial and Systems Engineering, Texas A&M University, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":0,"PubsCited":4,"Award":null,"image":"5333453-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Working memory load as a novel tool for evaluating visual analytics","DOI":"10.1109/VAST.2009.5333468","Link":"http://dx.doi.org/10.1109/VAST.2009.5333468","FirstPage":217,"LastPage":218,"PaperType":"M","Abstract":"The current visual analytics literature highlights design and evaluation processes that are highly variable and situation dependent, which raises at least two broad challenges. First, lack of a standardized evaluation criterion leads to costly re-designs for each task and specific user community. Second, this inadequacy in criterion validation raises significant uncertainty regarding visualization outputs and their related decisions, which may be especially troubling in high consequence environments like those of the intelligence community. As an attempt to standardize the ldquoapples and orangesrdquo of the extant situation, we propose the creation of standardized evaluation tools using general principles of human cognition. Theoretically, visual analytics enables the user to see information in a way that should attenuate the user's memory load and increase the user's task-available cognitive resources. By using general cognitive abilities like available working memory resources as our dependent measures, we propose to develop standardized evaluative capabilities that can be generalized across contexts, tasks, and user communities.","AuthorNames-Deduped":"Courtney C. Dornburg;Laura E. Matzen;Travis L. Bauer;Laura A. McNamara","AuthorNames":"Courtney C. Dornburg;Laura E. Matzen;Travis L. Bauer;Laura A. McNamara","AuthorAffiliation":"Sandia National Laboratories, USA;Sandia National Laboratories, USA;Sandia National Laboratories, USA;Sandia National Laboratories, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":0,"PubsCited":7,"Award":null,"image":""},{"Conference":"VAST","Year":2009,"Title":"Comparing two interface tools in performing visual analytics tasks","DOI":"10.1109/VAST.2009.5333469","Link":"http://dx.doi.org/10.1109/VAST.2009.5333469","FirstPage":219,"LastPage":220,"PaperType":"M","Abstract":"In visual analytics, menu systems are commonly adopted as supporting tools because of the complex nature of data. However, it is still unknown how much the interaction implicit to the interface impacts the performance of visual analysis. To show the effectiveness of two interface tools, one a floating text-based menu (Floating Menu) and the other a more interactive iconic tool (Interactive-Icon), we evaluated the use and human performance of both tools within one highly interactive visual analytics system. We asked participants to answer similarly constructed, straightforward questions in a genomic visualization, first with one tool, and then the other. During task performance we tracked completion times, task errors, and captured coarse-grained interactive behaviors. Based on the participants accuracy, speed, behaviors and post-task qualitative feedback, we observed that although the Interactive-Icon tool supports continuous interactions, task-oriented user evaluation did not find a significant difference between the two tools because there is a familiarity effect on the performance of solving the task questions with using Floating-Menu interface tool.","AuthorNames-Deduped":"Dong Hyun Jeong;Tera Marie Green;William Ribarsky;Remco Chang","AuthorNames":"Dong Hyun Jeong;Tera Marie Green;William Ribarsky;Remco Chang","AuthorAffiliation":"Charlotte Visualization Center, UNC Charlotte, USA;School of Interactive Arts and Technology, Simon Fraser University, Canada;Charlotte Visualization Center, UNC Charlotte, USA;Charlotte Visualization Center, UNC Charlotte, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":1,"PubsCited":3,"Award":null,"image":"5333469-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Analysis of community-contributed space- and time-referenced data (example of flickr and panoramio photos)","DOI":"10.1109/VAST.2009.5333472","Link":"http://dx.doi.org/10.1109/VAST.2009.5333472","FirstPage":213,"LastPage":214,"PaperType":"M","Abstract":"Space- and time-referenced data published on the Web by general people can be viewed in a dual way: as independent spatio-temporal events and as trajectories of people in the geographical space. These two views suppose different approaches to the analysis, which can yield different kinds of valuable knowledge about places and about people. We define possible types of analysis tasks related to the two views of the data and present several analysis methods appropriate for these tasks. The methods are suited to large amounts of the data.","AuthorNames-Deduped":"Gennady L. Andrienko;Natalia V. Andrienko;Peter Bak;Slava Kisilevich;Daniel A. Keim","AuthorNames":"Gennady Andrienko;Natalia Andrienko;Peter Bak;Slava Kisilevich;Daniel Keim","AuthorAffiliation":"University of Bonn and Fraunhofer Institute IAIS, Germany;University of Bonn and Fraunhofer Institute IAIS, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":17,"AminerCitationCount_06-2020":21,"XploreCitationCount - 2020-01":11,"PubsCited":3,"Award":null,"image":"5333472-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Interactive poster: A proposal for sharing user requirements for visual analytic tools","DOI":"10.1109/VAST.2009.5333474","Link":"http://dx.doi.org/10.1109/VAST.2009.5333474","FirstPage":215,"LastPage":216,"PaperType":"M","Abstract":"Although many in the community have advocated user-centered evaluations for visual analytic environments, a significant barrier exists. The users targeted by the visual analytics community (law enforcement personnel, professional information analysts, financial analysts, health care analysts, etc.) are often inaccessible to researchers. These analysts are extremely busy and their work environments and data are often classified or at least confidential. Furthermore, their tasks often last weeks or even months. It is simply not feasible to do such long-term observations to understand their jobs. How then can we hope to gather enough information about the diverse user populations to understand their needs? Some researchers, including the author, have been successful in getting access to specific end-users. A reasonable approach, therefore, would be to find a way to share user information. This work outlines a proposal for developing a handbook of user profiles for use by researchers, developers, and evaluators.","AuthorNames-Deduped":"Jean Scholtz","AuthorNames":"Jean Scholtz","AuthorAffiliation":"Pacific Northwest National Laboratory, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":8,"Award":null,"image":""},{"Conference":"VAST","Year":2009,"Title":"Iterative integration of visual insights during patent search and analysis","DOI":"10.1109/VAST.2009.5333564","Link":"http://dx.doi.org/10.1109/VAST.2009.5333564","FirstPage":203,"LastPage":210,"PaperType":"C","Abstract":"Patents are an important economic factor in todays globalized markets. Therefore, the analysis of patent information has become an inevitable task for a variety of interest groups. The retrieval of relevant patent information is an integral part of almost every patent analysis scenario. Unfortunately, the complexity of patent material inhibits a straightforward retrieval of all relevant patent documents and leads to iterative, time-consuming approaches in practice. With `PatViz', a new system for interactive analysis of patent information has been developed to leverage iterative query refinement. PatViz supports users in building complex queries visually and in exploring patent result sets interactively. Thereby, the visual query module introduces an abstraction layer that provides uniform access to different retrieval systems and relieves users of the burden to learn different complex query languages. By establishing an integrated environment it allows for interactive reintegration of insights gained from visual result set exploration into the visual query representation. We expect that the approach we have taken is also suitable to improve iterative query refinement in other Visual Analytics systems.","AuthorNames-Deduped":"Steffen Koch;Harald Bosch;Mark Giereth;Thomas Ertl","AuthorNames":"Steffen Koch;Harald Bosch;Mark Giereth;Thomas Ertl","AuthorAffiliation":"Visualization and Interactive Systems Group, Universität Stuttgart, Germany;Visualization and Interactive Systems Group, Universität Stuttgart, Germany;Visualization and Interactive Systems Group, Universität Stuttgart, Germany;Visualization and Interactive Systems Group, Universität Stuttgart, Germany","InternalReferences":"10.1109/INFVIS.2000.885086;10.1109/VAST.2007.4389009;10.1109/VAST.2007.4389006","AuthorKeywords":"Patent retrieval, information visualization, visual analytics, multiple coordinated views","AminerCitationCount_02-2020":22,"AminerCitationCount_06-2020":27,"XploreCitationCount - 2020-01":16,"PubsCited":23,"Award":"BP","image":"5333564-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Evaluating visual analytics systems for investigative analysis: Deriving design principles from a case study","DOI":"10.1109/VAST.2009.5333878","Link":"http://dx.doi.org/10.1109/VAST.2009.5333878","FirstPage":139,"LastPage":146,"PaperType":"C","Abstract":"Despite the growing number of systems providing visual analytic support for investigative analysis, few empirical studies of the potential benefits of such systems have been conducted, particularly controlled, comparative evaluations. Determining how such systems foster insight and sensemaking is important for their continued growth and study, however. Furthermore, studies that identify how people use such systems and why they benefit (or not) can help inform the design of new systems in this area. We conducted an evaluation of the visual analytics system Jigsaw employed in a small investigative sensemaking exercise, and we compared its use to three other more traditional methods of analysis. Sixteen participants performed a simulated intelligence analysis task under one of the four conditions. Experimental results suggest that Jigsaw assisted participants to analyze the data and identify an embedded threat. We describe different analysis strategies used by study participants and how computational support (or the lack thereof) influenced the strategies. We then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative analysis tools based thereon. We conclude with recommendations for metrics and techniques for evaluating other visual analytics investigative analysis tools.","AuthorNames-Deduped":"Youn ah Kang;Carsten Görg;John T. Stasko","AuthorNames":"Youn-ah Kang;Carsten Gorg;John Stasko","AuthorAffiliation":"School of Interactive Computing & GVU Center, Georgia Institute of Technology, USA;School of Interactive Computing & GVU Center, Georgia Institute of Technology, USA;School of Interactive Computing & GVU Center, Georgia Institute of Technology, USA","InternalReferences":"10.1109/VAST.2008.4677362;10.1109/VAST.2008.4677360;10.1109/VAST.2006.261416;10.1109/VAST.2007.4389006;10.1109/VAST.2008.4677358","AuthorKeywords":null,"AminerCitationCount_02-2020":52,"AminerCitationCount_06-2020":62,"XploreCitationCount - 2020-01":35,"PubsCited":17,"Award":null,"image":"5333878-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2009,"Title":"A multi-level middle-out cross-zooming approach for large graph analytics","DOI":"10.1109/VAST.2009.5333880","Link":"http://dx.doi.org/10.1109/VAST.2009.5333880","FirstPage":147,"LastPage":154,"PaperType":"C","Abstract":"This paper presents a working graph analytics model that embraces the strengths of the traditional top-down and bottom-up approaches with a resilient crossover concept to exploit the vast middle-ground information overlooked by the two extreme analytical approaches. Our graph analytics model is co-developed by users and researchers, who carefully studied the functional requirements that reflect the critical thinking and interaction pattern of a real-life intelligence analyst. To evaluate the model, we implement a system prototype, known as GreenHornet, which allows our analysts to test the theory in practice, identify the technological and usage-related gaps in the model, and then adapt the new technology in their work space. The paper describes the implementation of GreenHornet and compares its strengths and weaknesses against the other prevailing models and tools.","AuthorNames-Deduped":"Pak Chung Wong;Patrick Mackey;Kristin A. Cook;Randall M. Rohrer;Harlan Foote;Mark A. Whiting","AuthorNames":"Pak Chung Wong;Patrick Mackey;Kristin A. Cook;Randall M. Rohrer;Harlan Foote;Mark A. Whiting","AuthorAffiliation":"Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA;U.S. Department of Defense, USA;Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA","InternalReferences":"10.1109/VAST.2007.4389006;10.1109/INFVIS.2004.43;10.1109/INFVIS.2004.66;10.1109/TVCG.2007.70582","AuthorKeywords":"Graph analytics, information visualization","AminerCitationCount_02-2020":6,"AminerCitationCount_06-2020":8,"XploreCitationCount - 2020-01":4,"PubsCited":26,"Award":null,"image":"5333880-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Visual analysis of graphs with multiple connected components","DOI":"10.1109/VAST.2009.5333893","Link":"http://dx.doi.org/10.1109/VAST.2009.5333893","FirstPage":155,"LastPage":162,"PaperType":"C","Abstract":"In this paper, we present a system for the interactive visualization and exploration of graphs with many weakly connected components. The visualization of large graphs has recently received much research attention. However, specific systems for visual analysis of graph data sets consisting of many components are rare. In our approach, we rely on graph clustering using an extensive set of topology descriptors. Specifically, we use the self-organizing-map algorithm in conjunction with a user-adaptable combination of graph features for clustering of graphs. It offers insight into the overall structure of the data set. The clustering output is presented in a grid containing clusters of the connected components of the input graph. Interactive feature selection and task-tailored data views allow the exploration of the whole graph space. The system provides also tools for assessment and display of cluster quality. We demonstrate the usefulness of our system by application to a shareholder network analysis problem based on a large real-world data set. While so far our approach is applied to weighted directed graphs only, it can be used for various graph types.","AuthorNames-Deduped":"Tatiana von Landesberger;Melanie Görner;Tobias Schreck","AuthorNames":"Tatiana von Landesberger;Melanie Gorner;Tobias Schreck","AuthorAffiliation":"Interactive Graphics Systems Group, Technische Universität Darmstadt and Fraunhofer IGD, Germany;Interactive Graphics Systems Group, Technische Universität Darmstadt, Germany;Interactive Graphics Systems Group, Technische Universität Darmstadt, Germany","InternalReferences":"10.1109/TVCG.2006.193;10.1109/TVCG.2008.135;10.1109/INFVIS.2003.1249011;10.1109/TVCG.2006.147","AuthorKeywords":null,"AminerCitationCount_02-2020":29,"AminerCitationCount_06-2020":32,"XploreCitationCount - 2020-01":17,"PubsCited":48,"Award":null,"image":"5333893-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"MassVis: Visual analysis of protein complexes using mass spectrometry","DOI":"10.1109/VAST.2009.5333895","Link":"http://dx.doi.org/10.1109/VAST.2009.5333895","FirstPage":163,"LastPage":170,"PaperType":"C","Abstract":"Protein complexes are formed when two or more proteins non-covalently interact to form a larger three dimensional structure with specific biological function. Understanding the composition of such complexes is vital to understanding cell biology at the molecular level. MassVis is a visual analysis tool designed to assist the interpretation of data from a new workflow for detecting the composition of such protein complexes in biological samples. The data generated by the laboratory workflow naturally lends itself to a scatter plot visualization. However, characteristics of this data give rise to some unique aspects not typical of a standard scatter plot. We are able to take the output from tandem mass spectrometry and render the data in such a way that it mimics more traditional two-dimensional gel techniques and at the same time reveals the correlated behavior indicative of protein complexes. By computationally measuring these correlated patterns in the data, membership in putative complexes can be inferred. User interactions are provided to support both an interactive discovery mode as well as an unsupervised clustering of likely complexes. The specific analysis tasks led us to design a unique arrangement of item selection and coordinated detail views in order to simultaneously view different aspects of the selected item.","AuthorNames-Deduped":"Robert Kincaid;Kurt Dejgaard","AuthorNames":"Robert Kincaid;Kurt Dejgaard","AuthorAffiliation":"Agilent Laboratories, USA;McGill University, Canada","InternalReferences":"10.1109/VISUAL.2005.1532827;10.1109/VISUAL.2005.1532828;10.1109/VAST.2007.4389006","AuthorKeywords":"information visualization, visual analysis, correlation analysis, mass spectrometry, proteomics, interactome","AminerCitationCount_02-2020":7,"AminerCitationCount_06-2020":7,"XploreCitationCount - 2020-01":8,"PubsCited":27,"Award":null,"image":"5333895-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"SpRay: A visual analytics approach for gene expression data","DOI":"10.1109/VAST.2009.5333911","Link":"http://dx.doi.org/10.1109/VAST.2009.5333911","FirstPage":179,"LastPage":186,"PaperType":"C","Abstract":"We present a new application, SpRay, designed for the visual exploration of gene expression data. It is based on an extension and adaption of parallel coordinates to support the visual exploration of large and high-dimensional datasets. In particular, we investigate the visual analysis of gene expression data as generated by micro-array experiments; We combine refined visual exploration with statistical methods to a visual analytics approach that proved to be particularly successful in this application domain. We will demonstrate the usefulness on several multidimensional gene expression datasets from different bioinformatics applications.","AuthorNames-Deduped":"Janko Dietzsch;Julian Heinrich;Kay Nieselt;Dirk Bartz","AuthorNames":"Janko Dietzsch;Julian Heinrich;Kay Nieselt;Dirk Bartz","AuthorAffiliation":"ZBIT, University of Tübingen, Germany;VISUS, University of Stuttgart, Germany;ZBIT, University of Tübingen, Germany;ICCAS/VCM, Universität Leipzig, Germany","InternalReferences":"10.1109/VISUAL.2005.1532828;10.1109/INFVIS.2004.68;10.1109/VISUAL.2004.82;10.1109/TVCG.2006.138;10.1109/TVCG.2006.170;10.1109/INFVIS.2005.1532138","AuthorKeywords":"Visual analytics, bioinformatics, gene expression experiments, microarray data, large-scale microarray","AminerCitationCount_02-2020":18,"AminerCitationCount_06-2020":19,"XploreCitationCount - 2020-01":9,"PubsCited":30,"Award":null,"image":"5333911-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Using projection and 2D plots to visually reveal genetic mechanisms of complex human disorders","DOI":"10.1109/VAST.2009.5333917","Link":"http://dx.doi.org/10.1109/VAST.2009.5333917","FirstPage":171,"LastPage":178,"PaperType":"C","Abstract":"Gene mapping is a statistical method used to localize human disease genes to particular regions of the human genome. When performing such analysis, a genetic likelihood space is generated and sampled, which results in a multidimensional scalar field. Researchers are interested in exploring this likelihood space through the use of visualization. Previous efforts at visualizing this space, though, were slow and cumbersome, only showing a small portion of the space at a time, thus requiring the user to keep a mental picture of several views. We have developed a new technique that displays much more data at once by projecting the multidimensional data into several 2D plots. One plot is created for each parameter that shows the change along that parameter. A radial projection is used to create another plot that provides an overview of the high dimensional surface from the perspective of a single point. Linking and brushing between all the plots are used to determine relationships between parameters. We demonstrate our techniques on real world autism data, showing how to visually examine features of the high dimensional space.","AuthorNames-Deduped":"Boonthanome Nouanesengsy;Sang-Cheol Seok;Han-Wei Shen;Veronica J. Vieland","AuthorNames":"Boonthanome Nouanesengsy;Sang-Cheol Seok;Han-Wei Shen;Veronica J Vieland","AuthorAffiliation":"Battelle Center for Mathematical Medicine, Nationwide Childrens Hospital & The Ohio State University, USA;Battelle Center for Mathematical Medicine, Nationwide Childrens Hospital, USA;The Ohio State University, USA;Battelle Center for Mathematical Medicine, Nationwide Childrens Hospital & The Ohio State University, USA","InternalReferences":"10.1109/VISUAL.1993.398859","AuthorKeywords":"Visualization, Multidimensional data, Linkage Analysis, Posterior Probability of Linkage, PPL, PPLD, LD analysis, Linkage disequilibrium, Autism","AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":2,"PubsCited":14,"Award":null,"image":"5333917-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Visual opinion analysis of customer feedback data","DOI":"10.1109/VAST.2009.5333919","Link":"http://dx.doi.org/10.1109/VAST.2009.5333919","FirstPage":187,"LastPage":194,"PaperType":"C","Abstract":"Today, online stores collect a lot of customer feedback in the form of surveys, reviews, and comments. This feedback is categorized and in some cases responded to, but in general it is underutilized - even though customer satisfaction is essential to the success of their business. In this paper, we introduce several new techniques to interactively analyze customer comments and ratings to determine the positive and negative opinions expressed by the customers. First, we introduce a new discrimination-based technique to automatically extract the terms that are the subject of the positive or negative opinion (such as price or customer service) and that are frequently commented on. Second, we derive a Reverse-Distance-Weighting method to map the attributes to the related positive and negative opinions in the text. Third, the resulting high-dimensional feature vectors are visualized in a new summary representation that provides a quick overview. We also cluster the reviews according to the similarity of the comments. Special thumbnails are used to provide insight into the composition of the clusters and their relationship. In addition, an interactive circular correlation map is provided to allow analysts to detect the relationships of the comments to other important attributes and the scores. We have applied these techniques to customer comments from real-world online stores and product reviews from web sites to identify the strength and problems of different products and services, and show the potential of our technique.","AuthorNames-Deduped":"Daniela Oelke;Ming C. Hao;Christian Rohrdantz;Daniel A. Keim;Umeshwar Dayal;Lars-Erik Haug;Halldór Janetzko","AuthorNames":"Daniela Oelke;Ming Hao;Christian Rohrdantz;Daniel A. Keim;Umeshwar Dayal;Lars-Erik Haug;Halldor Janetzko","AuthorAffiliation":"University of Konstanz, Germany;Hewlett Packard Laboratories, Palo Alto, CA, USA;University of Konstanz, Germany;University of Konstanz, Germany;Hewlett Packard Laboratories, Palo Alto, CA, USA;Hewlett Packard Laboratories, Palo Alto, CA, USA;University of Konstanz, Germany","InternalReferences":null,"AuthorKeywords":"Visual Opinion Analysis, Visual Sentiment Analysis, Visual Document Analysis, Attribute Extraction","AminerCitationCount_02-2020":83,"AminerCitationCount_06-2020":90,"XploreCitationCount - 2020-01":33,"PubsCited":19,"Award":null,"image":"5333919-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"finVis: Applied visual analytics for personal financial planning","DOI":"10.1109/VAST.2009.5333920","Link":"http://dx.doi.org/10.1109/VAST.2009.5333920","FirstPage":195,"LastPage":202,"PaperType":"C","Abstract":"FinVis is a visual analytics tool that allows the non-expert casual user to interpret the return, risk and correlation aspects of financial data and make personal finance decisions. This interactive exploratory tool helps the casual decision-maker quickly choose between various financial portfolio options and view possible outcomes. FinVis allows for exploration of inter-temporal data to analyze outcomes of short-term or long-term investment decisions. FinVis helps the user overcome cognitive limitations and understand the impact of correlation between financial instruments in order to reap the benefits of portfolio diversification. Because this software is accessible by non-expert users, decision-makers from the general population can benefit greatly from using FinVis in practical applications. We quantify the value of FinVis using experimental economics methods and find that subjects using the FinVis software make better financial portfolio decisions as compared to subjects using a tabular version with the same information. We also find that FinVis engages the user, which results in greater exploration of the dataset and increased learning as compared to a tabular display. Further, participants using FinVis reported increased confidence in financial decision-making and noted that they were likely to use this tool in practical application.","AuthorNames-Deduped":"Stephen Rudolph;Anya Samak;David S. Ebert","AuthorNames":"Stephen Rudolph;Anya Savikhin;David S. Ebert","AuthorAffiliation":"Purdue University Regional Visualization and Analytics Center (PURVAC), USA;Purdue University Department of Economics, USA;Purdue University Regional Visualization and Analytics Center (PURVAC), USA","InternalReferences":"10.1109/INFVIS.2000.885098;10.1109/INFVIS.1997.636789;10.1109/TVCG.2007.70541;10.1109/INFVIS.2001.963273;10.1109/TVCG.2007.70589;10.1109/TVCG.2007.70577;10.1109/VAST.2008.4677363;10.1109/INFVIS.2003.1249027","AuthorKeywords":"Casual Information Visualization, visual analytics, personal finance, visualization of risk, economic decision-making","AminerCitationCount_02-2020":26,"AminerCitationCount_06-2020":31,"XploreCitationCount - 2020-01":25,"PubsCited":39,"Award":null,"image":"5333920-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"VIScover: Visualizing, exploring, and analysing structured data","DOI":"10.1109/VAST.2009.5333946","Link":"http://dx.doi.org/10.1109/VAST.2009.5333946","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"Today's challenging task in intelligent data processing is not to store large volumes of interlinked data but to visualize, explore, and understand its explicit or implicit relationships. Our solution to this is the VIScover system. VIScover combines semantic technologies with interactive exploration and visualization techniques able to analyze large volumes of structured data. We briefly describe our VIScover system and show its potential using the example of the VAST 2009 social network and geospatial data set.","AuthorNames-Deduped":"Thorsten Liebig;Olaf Noppens;Friedrich W. von Henke","AuthorNames":"Thorsten Liebig;Olaf Noppens;Friedrich von Henke","AuthorAffiliation":"Ulm University, derivo GmbH, Germany;Ulm University, derivo GmbH, Germany;Ulm University, Germany","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":4,"PubsCited":3,"Award":null,"image":"5333946-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"VIDI surveillance - embassy monitoring and oversight system","DOI":"10.1109/VAST.2009.5333950","Link":"http://dx.doi.org/10.1109/VAST.2009.5333950","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"We hypothesized that potential spies would try to use other employees' terminals in order to not draw attention to themselves. We define one type of suspicious activity as IP use on a terminal when the owner is inside the classified area. We created a timeline visualization of IP usage, overlaid with classified area entrances and exits. The vertical axis divides the timelines into 31 rows, one for each day of the month. The horizontal axis represents the time of day from early morning to late evening. A single employee's entire month is viewed all at once using this visualization. The employee being viewed can be changed using the arrow keys. Every IP event is represented by a vertical bar positioned at the exact time of its appearance. We color the IP events by port number, which is either intranet, HTTP, tomcat, or email, and size the bar based on the outgoing data size. Whenever an employee enters the classified area, a semi-transparent yellow region is drawn until that user exits the classified area. In rare cases when the user double enters, the region is twice as opaque, and in the other rare case where a user leaves the exits without entering, a red region is drawn until the next time the employee enters. The legend key and office diagram showing the current selected employee, highlighted in red, can be seen in the top left-hand corner.","AuthorNames-Deduped":"Chad Jones;Michael Ogawa;James Shearer;Anna Tikhonova;Kwan-Liu Ma","AuthorNames":"Chad Jones;Michael Ogawa;James Shearer;Anna Tikhonova;Kwan-Liu Ma","AuthorAffiliation":"VIDI Group, University of California, Davis, USA;VIDI Group, University of California, Davis, USA;VIDI Group, University of California, Davis, USA;VIDI Group, University of California, Davis, USA;VIDI Group, University of California, Davis, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":0,"PubsCited":3,"Award":null,"image":"5333950-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Multiple step social structure analysis with Cytoscape","DOI":"10.1109/VAST.2009.5333961","Link":"http://dx.doi.org/10.1109/VAST.2009.5333961","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"Cytoscape is a popular open source tool for biologists to visualize interaction networks. We find that it offers most of the desired functionality for visual analytics on graph data to guide us in the identification of the underlying social structure. We demonstrate its utility in the identification of the social structure in the VAST 2009 Flitter Mini Challenge.","AuthorNames-Deduped":"Hao Zhou;Anna A. Shaverdian;H. V. Jagadish;George Michailidis","AuthorNames":"Hao Zhou;Anna A. Shaverdian;H.V. Jagadish;George Michailidis","AuthorAffiliation":"Dept. of Statistics, University of Michigan, USA;Dept. of EECS, University of Michigan, USA;Dept. of EECS, University of Michigan, USA;Dept. of Statistics, University of Michigan, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":1,"PubsCited":3,"Award":null,"image":"5333961-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2009,"Title":"Visualization of uncertainty and analysis of geographical data","DOI":"10.1109/VAST.2009.5333965","Link":"http://dx.doi.org/10.1109/VAST.2009.5333965","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"A team of five worked on this challenge to identify a possible criminal structure within the Flitter social network. Initially we worked on the problem individually, deliberately not sharing any data, results or conclusions. This maximised the chances of spotting any blunders, unjustified assumptions or inferences and allowed us to triangulate any common conclusions. After an agreed period we shared our results demonstrating the visualization applications we had built and the reasoning behind our conclusions. This sharing of assumptions encouraged us to incorporate uncertainty in our visualization approaches as it became clear that there was a number of possible interpretations of the rules and assumptions governing the challenge. This summary of the work emphasises one of those applications detailing the geographic analysis and uncertainty handling of the network data.","AuthorNames-Deduped":"Jo Wood;Aidan Slingsby;Naz Khalili-Shavarini;Jason Dykes;David M. Mountain","AuthorNames":"Jo Wood;Aidan Slingsby;Naz Khalili-Shavarini;Jason Dykes;David Mountain","AuthorAffiliation":"School of Informatics, City University London, UK;School of Informatics, City University London, UK;School of Informatics, City University London, UK;School of Informatics, City University London, UK;School of Informatics, City University London, UK","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":0,"PubsCited":2,"Award":null,"image":"5333965-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"EAKOS: VAST 2009","DOI":"10.1109/VAST.2009.5333967","Link":"http://dx.doi.org/10.1109/VAST.2009.5333967","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"In this article, I describe the tools and techniques used to generate competing hypotheses for the VAST 2009 Flitter mini challenge. I will describe how I approached solving the social networks and the importance of the geospatial relationships to determine that ldquoSocial Structure Form Ardquo was the best matching social network.","AuthorNames-Deduped":"Lorne Leonard","AuthorNames":"Lorne Leonard","AuthorAffiliation":"Penn State University Research Computing & Cyberinfrastructure, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":0,"Award":null,"image":"5333967-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Visualized subgraph search","DOI":"10.1109/VAST.2009.5333968","Link":"http://dx.doi.org/10.1109/VAST.2009.5333968","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"We present a visually supported search and browsing system for network-type data, especially a novel module for subgraph search with a GUI to define subgraphs for queries. We describe how this prototype was applied for the Vast Challenge 2009, Flitter Mini Challenge.","AuthorNames-Deduped":"Dóra Erdös;Zsolt Fekete;András Lukács","AuthorNames":"Dora Erdos;Zsolt Fekete;Andras Lukacs","AuthorAffiliation":"Computer and Automation Research Institute (MTA SZTAKI), Hungarian Academy of Sciences, Data Mining and Web Search Group, Hungary;Computer and Automation Research Institute (MTA SZTAKI), Hungarian Academy of Sciences, Data Mining and Web Search Group, Hungary;Computer and Automation Research Institute (MTA SZTAKI), Hungarian Academy of Sciences, Data Mining and Web Search Group, Hungary","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":0,"PubsCited":1,"Award":null,"image":"5333968-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Innovative filtering techniques and customized analytics tools","DOI":"10.1109/VAST.2009.5334300","Link":"http://dx.doi.org/10.1109/VAST.2009.5334300","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"The VAST 2009 Challenge consisted of three heterogeneous synthetic data sets organized into separate mini-challenges with minimal correspondence information. The challenge task was the identification of a suspected data theft from cyber and real-world traces. The grand challenge required integrating the findings from the mini challenges into a plausible, consistent scenario. A mixture of linked, customized tools based on queryable models and rapid prototyping as well as generic analysis tools (developed in-house) helped us correctly solve all of the mini challenges. A collaborative analytic process was employed to reconstruct the scenario and to propose the correct steps for the reliable identification of the criminal organization based on activity traces of its members.","AuthorNames-Deduped":"Harald Bosch;Julian Heinrich;Christoph Müller 0001;Benjamin Höferlin;Guido Reina;Markus Höferlin;Michael Wörner 0001;Steffen Koch","AuthorNames":"Harald Bosch;Julian Heinrich;Christoph Muller;Benjamin Hoferlin;Guido Reina;Markus Hoferlin;Michael Worner;Steffen Koch","AuthorAffiliation":"Visualization and Interactive Systems Institute and Visualization Research Center, Universität Stuttgart, Germany;Visualization and Interactive Systems Institute and Visualization Research Center, Universität Stuttgart, Germany;Visualization and Interactive Systems Institute and Visualization Research Center, Universität Stuttgart, Germany;Visualization and Interactive Systems Institute and Visualization Research Center, Universität Stuttgart, Germany;Visualization and Interactive Systems Institute and Visualization Research Center, Universität Stuttgart, Germany;Visualization and Interactive Systems Institute and Visualization Research Center, Universität Stuttgart, Germany;Visualization and Interactive Systems Institute and Visualization Research Center, Universität Stuttgart, Germany;Visualization and Interactive Systems Institute and Visualization Research Center, Universität Stuttgart, Germany","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":7,"AminerCitationCount_06-2020":8,"XploreCitationCount - 2020-01":3,"PubsCited":3,"Award":null,"image":""},{"Conference":"VAST","Year":2009,"Title":"VAST 2009 Traffic Mini Challenge: Intuitive analytic information presentation","DOI":"10.1109/VAST.2009.5334301","Link":"http://dx.doi.org/10.1109/VAST.2009.5334301","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"As a solution to the VAST 2009 Traffic Mini Visualization Challenge, we built the Badge and Network Traffic (BNT) tool to create animations of the events taking place in the embassy. Using the embassy layout, the prox-card and web-access entries and their time-stamps, we animated color-based flagging of events. The BNT tool highlights logical anomalies occuring in the badge and network traffic data with color-coded alerts. Prior to the animated visualization, the tool analyzes data with respect to various aspects using (i) the amount of data transfers, (ii) destination IPs access patterns, (iii) employee's browsing patterns and (iv) employee's entry log into the restricted area. Any abnormality noticed is immediately reported to the user in the form of plots. In this presentation, we list out the various analyses performed and how they were utilized in the visualization. A few screenshots of the tool are provided to illustrate our analytic information presentation.","AuthorNames-Deduped":"Shraddha Agrawal;Kollukuduru Sravanthi;Soujanya Vadapalli;Kamalakar Karlapalem","AuthorNames":"Shraddha Agrawal;Kollukuduru Sravanthi;Soujanya Vadapalli;Kamalakar Karlapalem","AuthorAffiliation":"Centre for Data Engineering, International Institute of Information Technology, Hyderabad, India;Centre for Data Engineering, International Institute of Information Technology, Hyderabad, India;Centre for Data Engineering, International Institute of Information Technology, Hyderabad, India;Centre for Data Engineering, International Institute of Information Technology, Hyderabad, India","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":0,"PubsCited":0,"Award":null,"image":"5334301-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Detecting and analyzing relationships among anomalies","DOI":"10.1109/VAST.2009.5334426","Link":"http://dx.doi.org/10.1109/VAST.2009.5334426","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"The HRL anomaly analysis tool was developed as part of the IEEE VAST Challenge 2009. One of the tasks involved processing badge and network traffic in order to detect and identify a fictitious embassy employee suspected of leaking information. The tool is designed to assist an analyst in detecting, analyzing, and visualizing anomalies and their relationships. Two key visualizations in our submission present how we identified the suspicious traffic using network visualization and how subsequently we connected that activity to an employee using an alibi table.","AuthorNames-Deduped":"David Allen;Tsai-Ching Lu;David Huber","AuthorNames":"David Allen;Tsai-Ching Lu;Dave Huber","AuthorAffiliation":"HRL Laboratories, LLC, USA;HRL Laboratories, LLC, USA;HRL Laboratories, LLC, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":0,"PubsCited":1,"Award":null,"image":"5334426-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Integrative visual analytics for suspicious behavior detection","DOI":"10.1109/VAST.2009.5334430","Link":"http://dx.doi.org/10.1109/VAST.2009.5334430","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"In the VAST Challenge 2009 suspicious behavior had to be detected applying visual analytics to heterogeneous data, such as network traffic, social network enriched with geo-spatial attributes, and finally video surveillance data. This paper describes some of the awarded parts from our solution entry.","AuthorNames-Deduped":"Peter Bak;Christian Rohrdantz;Svenja Leifert;Christoph Granacher;Stefan Koch;Simon Butscher;Patrick Jungk;Daniel A. Keim","AuthorNames":"Peter Bak;Christian Rohrdantz;Svenja Leifert;Christoph Granacher;Stefan Koch;Simon Butscher;Patrick Jungk;Daniel A. Keim","AuthorAffiliation":"University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":0,"PubsCited":3,"Award":null,"image":"5334430-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"VAST 2009 challenge: An insider threat","DOI":"10.1109/VAST.2009.5334454","Link":"http://dx.doi.org/10.1109/VAST.2009.5334454","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"The 4&lt;sup&gt;th&lt;/sup&gt; VAST Challenge centered on a cyber analytics scenario and offered three mini-challenges with datasets of badge and network traffic data, a social network including geospatial information, and security video. Teams could also enter the Grand challenge which combined all three datasets. In this paper, we summarize the dataset, the overall scenario and the questions asked in the challenges. We describe the judging process and new infrastructure developed to manage the submissions and compute accuracy measures in the social network mini challenge. We received 49 entries from 30 teams, and gave 23 different awards to a total of 16 teams.","AuthorNames-Deduped":"Georges G. Grinstein;Jean Scholtz;Mark A. Whiting;Catherine Plaisant","AuthorNames":"Georges Grinstein;Jean Scholtz;Mark Whiting;Catherine Plaisant","AuthorAffiliation":"University of Massachusetts Lowell, USA;Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA;University of Maryland, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":8,"XploreCitationCount - 2020-01":0,"PubsCited":7,"Award":null,"image":""},{"Conference":"VAST","Year":2009,"Title":"Solving the traffic and flitter challenges with tulip","DOI":"10.1109/VAST.2009.5334456","Link":"http://dx.doi.org/10.1109/VAST.2009.5334456","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"We present our visualization systems and findings for the badge and network traffic as well as the social network and geospatial challenges of the 2009 VAST contest. The summary starts by presenting an overview of our time series encoding of badge information and network traffic. Our findings suggest that employee 30 may be of interest. In the second part of the paper, we describe our system for finding subgraphs in the social network subject to degree constraints. Subsequently, we present our most likely candidate network which is similar to scenario B.","AuthorNames-Deduped":"Paolo Simonetto;Pierre-Yves Koenig;Faraz Zaidi;Daniel Archambault;Frédéric Gilbert 0001;Trung-Tien Phan-Quang;Morgan Mathiaut;Antoine Lambert;Jonathan Dubois;Ronan Sicre;Mathieu Brulin;Rémy Vieux;Guy Melançon","AuthorNames":"Paolo Simonetto;Mathieu Brulin;Remy Vieux;Guy Melancon;Pierre-Yves Koenig;Faraz Zaidi;Daniel Archambault;Frederic Gilbert;Trung-Tien Phan-Quang;Morgan Mathiaut;Antoine Lambert;Jonathan Dubois;Ronan Sicre","AuthorAffiliation":"INRIA Bordeaux Sud-Ouest and LaBRI, Université de Bordeaux I, France;INRIA Bordeaux Sud-Ouest and LaBRI, Université de Bordeaux I, France;INRIA Bordeaux Sud-Ouest and LaBRI, Université de Bordeaux I, France;INRIA Bordeaux Sud-Ouest and LaBRI, Université de Bordeaux I, France;INRIA Bordeaux Sud-Ouest and LaBRI, Université de Bordeaux I, France;INRIA Bordeaux Sud-Ouest and LaBRI, Université de Bordeaux I, France;INRIA Bordeaux Sud-Ouest and LaBRI, Université de Bordeaux I, France;INRIA Bordeaux Sud-Ouest and LaBRI, Université de Bordeaux I, France;INRIA Bordeaux Sud-Ouest and LaBRI, Université de Bordeaux I, France;INRIA Bordeaux Sud-Ouest and LaBRI, Université de Bordeaux I, France;INRIA Bordeaux Sud-Ouest and LaBRI, Université de Bordeaux I, France;INRIA Bordeaux Sud-Ouest and LaBRI, Université de Bordeaux I, France;INRIA Bordeaux Sud-Ouest and LaBRI, Université de Bordeaux I, France","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":0,"PubsCited":1,"Award":null,"image":"5334456-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Timeline analysis of undercover activities VAST 2009 traffic mini challenge award: Good analytical technique","DOI":"10.1109/VAST.2009.5334460","Link":"http://dx.doi.org/10.1109/VAST.2009.5334460","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"Our visualization tool for the VAST 2009 traffic mini challenge, Timeliner, visualizes badge and network traffic data together in a single timeline. The two views of per-employee and per-day with various filtering interactions enable users to analyze easily employees activities at a particular moment of interest as well as their general daily patterns. Using Timeliner, we present several hypotheses for the task at hand and their validation processes, which reveals various aspects of the data.","AuthorNames-Deduped":"Jaegul Choo;Emily Fujimoto;Hanseung Lee;Pedro R. Walteros","AuthorNames":"Jaegul Choo;Emily Fujimoto;Hanseung Lee;Pedro R. Walteros","AuthorAffiliation":"Georgia Institute of Technology, 266 Ferst Drive, Atlanta, 30332, USA;Georgia Institute of Technology, 266 Ferst Drive, Atlanta, 30332, USA;Georgia Institute of Technology, 266 Ferst Drive, Atlanta, 30332, USA;Georgia Institute of Technology, 266 Ferst Drive, Atlanta, 30332, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":0,"Award":null,"image":"5334460-fig-1-source-large.gif"},{"Conference":"VAST","Year":2009,"Title":"Palantir: A visualization platform for real-world analysis","DOI":"10.1109/VAST.2009.5334462","Link":"http://dx.doi.org/10.1109/VAST.2009.5334462","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"Palantir is an analytic platform currently used worldwide by both governmental and financial analysts. This paper provides a brief overview of the platform, examines our 2009 IEEE VAST Challenge submission, and highlights several key analytic and visualization features we used in our analysis.","AuthorNames-Deduped":"Brandon Wright;Jason Payne;Matthew Steckman;Scott Stevson","AuthorNames":"Brandon Wright;Jason Payne;Matthew Steckman;Scott Stevson","AuthorAffiliation":"Palantir Technologies, USA;Palantir Technologies, USA;Palantir Technologies, USA;Palantir Technologies, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":1,"PubsCited":0,"Award":null,"image":""},{"Conference":"VAST","Year":2009,"Title":"Combining iterative analytical reasoning and software development using the visualization language Processing","DOI":"10.1109/VAST.2009.5334463","Link":"http://dx.doi.org/10.1109/VAST.2009.5334463","FirstPage":null,"LastPage":null,"PaperType":"M","Abstract":"Processing is a very powerful visualization language which combines software concepts with principles of visual form and interaction. Artists, designers and architects use it but it is also a very effective programming language in the area of visual analytics. In the following contribution Processing is utilized in order to visually analyze data provided by IEEE VAST 2009 Mini Challenge Badge and Network Traffic. The applied process is iterative and each stage of the analytical reasoning process is accompanied by customized software development. The visual model, the process and the technical solution will be briefly introduced.","AuthorNames-Deduped":"Claudia Müller-Birn;Lukas Birn","AuthorNames":"Claudia Muller-Birn;Lukas Birn","AuthorAffiliation":"Carnegie Mellon University, USA;Capgemini sd&m AG, Germany","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":4,"Award":null,"image":"5334463-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Adapting Daniel and Wood's modeling approach to interactive visual analytics","DOI":"10.1109/VAST.2010.5649831","Link":"http://dx.doi.org/10.1109/VAST.2010.5649831","FirstPage":253,"LastPage":254,"PaperType":"M","Abstract":"This poster describes our progress in developing an interactive linear modeling system that supports the modeling approach described by Daniel and Wood. Our visual interface permits analysts to build sets of possible models and then creates appropriate visualizations to permit human-in-the-loop model comparison and selection.","AuthorNames-Deduped":"Justin Talbot;Pat Hanrahan","AuthorNames":"Justin Talbot;Pat Hanrahan","AuthorAffiliation":"Computer Science Department Stanford University;Computer Science Department Stanford University","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":9,"Award":null,"image":"5649831-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Enron case study: Analysis of email behavior using EmailTime","DOI":"10.1109/VAST.2010.5649905","Link":"http://dx.doi.org/10.1109/VAST.2010.5649905","FirstPage":235,"LastPage":236,"PaperType":"M","Abstract":"This paper presents a case study with Enron email dataset to explore the behaviors of email users within different organizational positions. We defined email behavior as the email activity level of people regarding a series of measured metrics e.g. sent and received emails, numbers of email addresses, etc. These metrics were calculated through EmailTime, a visual analysis tool of email correspondence over the course of time. Results showed specific patterns in the email datasets of different organizational positions.","AuthorNames-Deduped":"Minoo Erfani Joorabchi;Ji-Dong Yim;Mona Erfani Joorabchi;Chris Shaw 0002","AuthorNames":"Minoo Erfani Joorabchi;Ji-Dong Yim;Mona Erfani Joorabchi;Christopher D. Shaw","AuthorAffiliation":"Simon Fraser University;Simon Fraser University;Simon Fraser University;Simon Fraser University","InternalReferences":null,"AuthorKeywords":"Email, Enron, Case Study, EmailTime, Visual Analysis","AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":4,"Award":null,"image":"5649905-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"A radial visualization tool for depicting hierarchically structured video content","DOI":"10.1109/VAST.2010.5650177","Link":"http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5650177","FirstPage":251,"LastPage":252,"PaperType":"M","Abstract":"The visual analysis of video content is an important research topic due to the huge amount of video data that is generated every day. Annotating this data will become a major problem since the amount of videos further increases. With this work we introduce a system that combines a visualization tool with automatic video segmentation techniques and a characteristic key-frame extraction. A summary of the content of a whole video in one view is realized. Furthermore, the user can interactively browse through the video via our visualization interface to get more detailed information. The system is adapted to two application scenarios and a third application is discussed for future work.","AuthorNames-Deduped":"Tobias Ruppert;Jörn Kohlhammer","AuthorNames":"Tobias Ruppert;Jörn Kohlhammer","AuthorAffiliation":"Fraunhofer Institute for Computer Graphics Research (IGD), Darmstadt, Germany;Fraunhofer Institute for Computer Graphics Research (IGD), Darmstadt, Germany","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":2,"PubsCited":3,"Award":null,"image":"5650177-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"A Visual Analytics approach to identifying protein structural constraints","DOI":"10.1109/VAST.2010.5650199","Link":"http://dx.doi.org/10.1109/VAST.2010.5650199","FirstPage":249,"LastPage":250,"PaperType":"M","Abstract":"Predicting protein structures has long been a grand-challenge problem. Fine-grained computational simulation of folding events from a protein's synthesis to its final stable structure remains computationally intractable. Therefore, methods which derive constraints from other sources are attractive. To date, constraints derived from known structures have proven to be highly successful. However, these cannot be applied to molecules with no identifiable neighbors having already-determined structures. For such molecules, structural constraints must be derived in other ways. One popular approach has been the statistical analysis of large families of proteins, with the hope that residues that “change together” (co-evolve) imply that those residues are in contact. Unfortunately, despite repeated attempts to use this data to deduce structural constraints, this approach has met with minimal success. The consensus of current literature concludes that there is simply too little information contained within the correlated mutations of many protein families to reliably and generally predict structural constraints. Recent work in my laboratory challenges this conclusion. For some time we have been developing methods (MAVL/StickWRLD) to visualize the pattern of co-evolved mutations within sequence families. While our analysis of individual correlations agrees with the literature consensus, we have recently discovered that the visualized pattern of correlations is highly suggestive of structural relationships. In our preliminary test cases, human researchers can unambiguously determine many positive structural constraints by visual analysis of statistical sequence information alone, often with no training on interpretation of the visualization results. Herein we report the visualization design that supports this Visual Analytics approach to identifying high-confidence hypotheses about protein folding from protein sequence, and illustrate preliminary results from this research. Our approach entails a higher-dimensional extension of parallel coordinates which illuminates distant shared sub-tuples of the vectors representing each protein sequence when these sub-tuples occur with an over abundance compared to expectations. It simultaneously eliminates all representations of tuples which occur with frequency near the expected norm. The result is a minimally-occluded representation of outlier, and only outlier co-occurrences within the sequence families.","AuthorNames-Deduped":"William C. Ray","AuthorNames":"William C. Ray","AuthorAffiliation":"The Research Institute at Nationwide Children's Hospital, The Ohio State University Biophysics Program","InternalReferences":null,"AuthorKeywords":"High-order finite elements, spectral/hp elements, cut-plane extraction, GPU-based root-finding, GPU ray-tracing, cut-surface extraction","AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":0,"PubsCited":3,"Award":null,"image":"5650199-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"ProDV - A case study in delivering visual analytics","DOI":"10.1109/VAST.2010.5650219","Link":"http://dx.doi.org/10.1109/VAST.2010.5650219","FirstPage":247,"LastPage":248,"PaperType":"M","Abstract":"We present a custom visual analytics system developed in conjunction with the test and evaluation community of the US Army. We designed and implemented a visual programming environment for configuring a variety of interactive visual analysis capabilities. Our abstraction of the visualization process is based on insights gained from interviews conducted with expert users. We show that this model allowed analysts to implement multiple visual analysis capabilities for network performance, anomalous sensor activity, and engagement results. Long-term interaction with expert users led to development of several custom visual analysis techniques. We have conducted training sessions with expert users, and are working to evaluate the success of our work based on performance metrics captured in a semi-automated fashion during these training sessions. We have also integrated collaborative analysis features such as annotations and shared content.","AuthorNames-Deduped":"Derek Overby;John Keyser;Jim Wall","AuthorNames":"Derek Overby;John Keyser;Jim Wall","AuthorAffiliation":"Department of Computer Science and Engineering, Texas A&M University;Department of Computer Science and Engineering, Texas A&M University;Department of Industrial and Systems Engineering, Texas A&M University","InternalReferences":null,"AuthorKeywords":"Visualization system and toolkit design","AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":0,"PubsCited":4,"Award":null,"image":"5650219-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Poster: Translating cross-filtered queries into questions","DOI":"10.1109/VAST.2010.5650251","Link":"http://dx.doi.org/10.1109/VAST.2010.5650251","FirstPage":245,"LastPage":246,"PaperType":"M","Abstract":"Complex combinations of coordinated multiple views are increasingly used to design tools for highly interactive visual exploration and analysis of multidimensional data. While complex coordination patterns provide substantial utility through expressive querying, they also exhibit usability problems for users when learning required interaction sequences, recalling past queries, and interpreting visual states. As visual analysis tools grow more sophisticated, there is a growing need to make them more understandable as well. Our long-term goal is to exploit natural language familiarity and literacy to directly facilitate individual and collaborative use of visual analysis tools. In this poster, we present work in progress on an automatically generated query-to-question user interface to translate interactive states during visual analysis into an accompanying visual log of formatted text. Our effort currently focuses on a symmetric and thus relatively simple coordination pattern: cross-filtered views. We describe our current thinking about query-to-question translation in a typical cross-filtered visualization of movies, people, and genres in the Internet Movie Database.","AuthorNames-Deduped":"Maryam Nafari;Chris Weaver","AuthorNames":"Maryam Nafari;Chris Weaver","AuthorAffiliation":"School of Computer Science and Center for Spatial Analysis The University of Oklahoma;School of Computer Science and Center for Spatial Analysis The University of Oklahoma","InternalReferences":null,"AuthorKeywords":"Coordinated multiple views, cross-filtered queries, interaction states, natural language generation, visual provenance","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":8,"Award":null,"image":"5650251-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Visual analysis of frequent patterns in large time series","DOI":"10.1109/VAST.2010.5650766","Link":"http://dx.doi.org/10.1109/VAST.2010.5650766","FirstPage":227,"LastPage":228,"PaperType":"M","Abstract":"The detection of previously unknown, frequently occurring patterns in time series, often called motifs, has been recognized as an important task. To find these motifs, we use an advanced temporal data mining algorithm. Since our algorithm usually finds hundreds of motifs, we need to analyze and access the discovered motifs. For this purpose, we introduce three novel visual analytics methods: (1) motif layout, using colored rectangles for visualizing the occurrences and hierarchical relationships of motifs in a multivariate time series, (2) motif distortion, for enlarging or shrinking motifs as appropriate for easy analysis and (3) motif merging, to combine a number of identical adjacent motif instances without cluttering the display. We have applied and evaluated our methods using two real-world data sets: data center cooling and oil well production.","AuthorNames-Deduped":"Ming C. Hao;Manish Marwah;Halldór Janetzko;Daniel A. Keim;Umeshwar Dayal;Ratnesh K. Sharma;Debprakash Patnaik;Naren Ramakrishnan","AuthorNames":"M. C. Hao;M. Marwah;H. Janetzko;D. A. Keim;U. Dayal;R. Sharma;D. Patnaik;N. Ramakrishnan","AuthorAffiliation":"Hewlett Packard Laboratories, USA;Hewlett Packard Laboratories, USA;University of Konstanz, Germany;University of Konstanz, Germany;Hewlett Packard Laboratories, USA;Hewlett Packard Laboratories, USA;Virginia Tech, USA;Virginia Tech, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":2,"PubsCited":2,"Award":null,"image":"5650766-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Enhancing text-based chat with visuals for hazardous weather decision making","DOI":"10.1109/VAST.2010.5650815","Link":"http://dx.doi.org/10.1109/VAST.2010.5650815","FirstPage":225,"LastPage":226,"PaperType":"M","Abstract":"We created a visual chat application for use during hazardous weather events. The application, NWSChat2, allows National Weather Service forecasters, media members, and storm trackers to communicate with each other, basing their conversation on a common shared radar map of the storm. Users can additionally annotate the map with `pins' or draw notes with a stylus. These annotations are automatically shared with all other users. The collaborative nature of NWSChat2 makes it well-suited for disseminating information to all users during weather emergencies.","AuthorNames-Deduped":"Moshe Gutman;Gina Eosco;Monica Zappa;Chris Weaver","AuthorNames":"Moshe Gutman;Gina Eosco;Monica Zappa;Chris Weaver","AuthorAffiliation":"School of Computer Science and School of Meteorology, University of Oklahoma;School of Computer Science and School of Meteorology, University of Oklahoma;School of Computer Science and School of Meteorology, University of Oklahoma;School of Computer Science and School of Meteorology, University of Oklahoma","InternalReferences":null,"AuthorKeywords":"Collaboration, coordinated multiple views, instant messaging, emergency response, hazardous weather","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":4,"Award":null,"image":"5650815-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"ALIDA: Using machine learning for intent discernment in visual analytics interfaces","DOI":"10.1109/VAST.2010.5650854","Link":"http://dx.doi.org/10.1109/VAST.2010.5650854","FirstPage":223,"LastPage":224,"PaperType":"M","Abstract":"In this paper, we introduce ALIDA, an Active Learning Intent Discerning Agent for visual analytics interfaces. As users interact with and explore data in a visual analytics environment they are each developing their own unique analytic process. The goal of ALIDA is to observe and record the human-computer interactions and utilize these observations as a means of supporting user exploration; ALIDA does this by using interaction to make decision about user interest. As such, ALIDA is designed to track the decision history (interactions) of a user. This history is then utilized to enhance the user's decision-making process by allowing the user to return to previously visited search states, as well as providing suggestions of other search states that may be of interest based on past exploration modalities. The agent passes these suggestions (or decisions) back to an interactive visualization prototype, and these suggestions are used to guide the user, either by suggesting searches or changes to the visualization view. Current work has tested ALIDA under the exploration of homonyms for users wishing to explore word linkages within a dictionary. Ongoing work includes using ALIDA to guide users in transfer function design for volume rendering within scientific gateways.","AuthorNames-Deduped":"Tera Marie Green;Ross Maciejewski;Steve DiPaola","AuthorNames":"Tera Marie Green;Ross Maciejewski;Steve DiPaola","AuthorAffiliation":"School of Interactive Arts &#x002B; Technology - Simon Fraser University;Purdue Visual Analytics Center - Purdue University;School of Interactive Arts &#x002B; Technology - Simon Fraser University","InternalReferences":null,"AuthorKeywords":"artificial intelligence, cognition, intent discernment, volume rendering","AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":2,"PubsCited":6,"Award":null,"image":"5650854-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Conveying network features in geospatial battlespace displays","DOI":"10.1109/VAST.2010.5651192","Link":"http://dx.doi.org/10.1109/VAST.2010.5651192","FirstPage":221,"LastPage":222,"PaperType":"M","Abstract":"Advanced battlespace network visualization techniques are required within the modern Air Operations Center (AOC) to improve cross-domain situation awareness and to support planning and decision-making. We present a visualization toolkit to address this need that supports the integration of network health and status information and meta-information with other traditional AOC information resources and activities across air, space, and cyber domains. Applications include the development of battlespace visualization technologies that will improve warfighters' decision-making response time and provide enhanced flexibility for mission planning by efficiently revealing affordances for leveraging, disrupting, or enhancing network connectivity.","AuthorNames-Deduped":"J. Alex Godwin;Ryan M. Kilgore","AuthorNames":"J. Alex Godwin;Ryan M. Kilgore","AuthorAffiliation":"Charles River Analytics;Charles River Analytics","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":2,"Award":null,"image":"5651192-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Visualization of temporal relationships within coordinated views","DOI":"10.1109/VAST.2010.5651617","Link":"http://dx.doi.org/10.1109/VAST.2010.5651617","FirstPage":219,"LastPage":220,"PaperType":"M","Abstract":"In command and control (C2) environments, decision makers must rapidly understand and address key temporal relationships that exist between critical tasks as conditions fluctuate. However, traditional temporal displays, such as mission timelines, fail to support user understanding of and reasoning about critical relationships. We have developed visualization methods to compactly and effectively convey key temporal constraints. In this paper, we present examples of our visualization approach and describe how we are exploring interaction methods within an integrated visualization workspace to support user awareness of temporal constraints.","AuthorNames-Deduped":"Stephanie Dudzic;J. Alex Godwin;Ryan M. Kilgore","AuthorNames":"Stephanie Dudzic;J. Alex Godwin;Ryan M. Kilgore","AuthorAffiliation":"Charles River Analytics;Charles River Analytics;Charles River Analytics","InternalReferences":null,"AuthorKeywords":"temporal relationships, temporal visualization","AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":1,"PubsCited":3,"Award":null,"image":"5651617-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Cluster correspondence views for enhanced analysis of SOM displays","DOI":"10.1109/VAST.2010.5651676","Link":"http://dx.doi.org/10.1109/VAST.2010.5651676","FirstPage":217,"LastPage":218,"PaperType":"M","Abstract":"The Self-Organizing Map (SOM) algorithm is a popular and widely used cluster algorithm. Its constraint to organize clusters on a grid structure makes it very amenable to visualization. On the other hand, the grid constraint may lead to reduced cluster accuracy and reliability, compared to other clustering methods not implementing this restriction. We propose a visual cluster analysis system that allows to validate the output of the SOM algorithm by comparison with alternative clustering methods. Specifically, visual mappings overlaying alternative clustering results onto the SOM are proposed. We apply our system on an example data set, and outline main analytical use cases.","AuthorNames-Deduped":"Jürgen Bernard;Tatiana von Landesberger;Sebastian Bremm;Tobias Schreck","AuthorNames":"Jürgen Bernard;Tatiana von Landesberger;Sebastian Bremm;Tobias Schreck","AuthorAffiliation":"Interactive Graphics Systems Group, TU Darmstadt, Germany;Interactive Graphics Systems Group, TU Darmstadt, Germany, and, Fraunhofer IGD, Darmstadt;Interactive Graphics Systems Group, TU Darmstadt, Germany;Interactive Graphics Systems Group, TU Darmstadt, Germany","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":5,"Award":null,"image":"5651676-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Interactive visual analysis of multiobjective optimizations","DOI":"10.1109/VAST.2010.5651694","Link":"http://dx.doi.org/10.1109/VAST.2010.5651694","FirstPage":215,"LastPage":216,"PaperType":"M","Abstract":"Optimization problems are typically addressed by purely automatic approaches. For multi-objective problems, however, a single best solution often does not exist. In this case, it is necessary to analyze trade-offs between many conflicting goals within a given application context. This poster describes an approach that tightly integrates automatic algorithms for multi-objective optimization and interactive multivariate visualizations. Ad-hoc selections support a flexible definition of input data for subsequent algorithms. These algorithms in turn represent their result as derived data attributes that can be assigned to visualizations or be used as a basis for further selections (e.g., to constrain the result set). This enables a guided search that still involves the knowledge of domain experts. We describe our approach in the context of multi-run simulation data from the application domain of car engine design.","AuthorNames-Deduped":"Wolfgang Berger;Harald Piringer","AuthorNames":"Wolfgang Berger;Harald Piringer","AuthorAffiliation":"VRVis Research Center, Vienna, Austria;VRVis Research Center, Vienna, Austria","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":8,"AminerCitationCount_06-2020":9,"XploreCitationCount - 2020-01":5,"PubsCited":7,"Award":null,"image":"5651694-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2010,"Title":"DimStiller: Workflows for dimensional analysis and reduction","DOI":"10.1109/VAST.2010.5652392","Link":"http://dx.doi.org/10.1109/VAST.2010.5652392","FirstPage":3,"LastPage":10,"PaperType":"C","Abstract":"DimStiller is a system for dimensionality reduction and analysis. It frames the task of understanding and transforming input dimensions as a series of analysis steps where users transform data tables by chaining together different techniques, called operators, into pipelines of expressions. The individual operators have controls and views that are linked together based on the structure of the expression. Users interact with the operator controls to tune parameter choices, with immediate visual feedback guiding the exploration of local neighborhoods of the space of possible data tables. DimStiller also provides global guidance for navigating data-table space through expression templates called workflows, which permit re-use of common patterns of analysis.","AuthorNames-Deduped":"Stephen Ingram;Tamara Munzner;Veronika Irvine;Melanie Tory;Steven Bergner;Torsten Möller","AuthorNames":"Stephen Ingram;Tamara Munzner;Veronika Irvine;Melanie Tory;Steven Bergner;Torsten Möller","AuthorAffiliation":"University of British Columbia;University of British Columbia;University of Victoria;University of Victoria;Simon Fraser University;Simon Fraser University","InternalReferences":"10.1109/INFVIS.2003.1249013;10.1109/VISUAL.1994.346302;10.1109/TVCG.2006.178;10.1109/INFVIS.2003.1249015;10.1109/TVCG.2009.153;10.1109/INFVIS.2004.71","AuthorKeywords":null,"AminerCitationCount_02-2020":49,"AminerCitationCount_06-2020":56,"XploreCitationCount - 2020-01":40,"PubsCited":20,"Award":null,"image":"5652392-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2010,"Title":"Visual exploration of classification models for risk assessment","DOI":"10.1109/VAST.2010.5652398","Link":"http://dx.doi.org/10.1109/VAST.2010.5652398","FirstPage":11,"LastPage":18,"PaperType":"C","Abstract":"In risk assessment applications well informed decisions are made based on huge amounts of multi-dimensional data. In many domains not only the risk of a wrong decision, but in particular the trade-off between the costs of possible decisions are of utmost importance. In this paper we describe a framework tightly integrating interactive visual exploration with machine learning to support the decision making process. The proposed approach uses a series of interactive 2D visualizations of numeric and ordinal data combined with visualization of classification models. These series of visual elements are further linked to the classifier's performance visualized using an interactive performance curve. An interactive decision point on the performance curve allows the decision maker to steer the classification model and instantly identify the critical, cost changing data elements, in the various linked visualizations. The critical data elements are represented as images in order to trigger associations related to the knowledge of the expert. In this context the data visualization and classification results are not only linked together, but are also linked back to the classification model. Such a visual analytics framework allows the user to interactively explore the costs of his decisions for different settings of the model and accordingly use the most suitable classification model and make more informed and reliable decisions. A case study on data from the Forensic Psychiatry domain reveals the usefulness of the suggested approach.","AuthorNames-Deduped":"Gosia Migut;Marcel Worring","AuthorNames":"Malgorzata Migut;Marcel Worring","AuthorAffiliation":"Intelligent Systems Lab Amsterdam, University of Amsterdam;Intelligent Systems Lab Amsterdam, University of Amsterdam","InternalReferences":"10.1109/TVCG.2007.70515;10.1109/INFVIS.2005.1532139;10.1109/VAST.2009.5332628;10.1109/INFVIS.1998.729559;10.1109/TVCG.2009.199;10.1109/INFVIS.2000.885086;10.1109/TVCG.2007.70521;10.1109/VAST.2008.4677369;10.1109/VISUAL.1994.346302;10.1109/TVCG.2008.153","AuthorKeywords":"Visual Analytics, Interactive Visual Exploration, Decision Boundary Visualization, Multi-dimensional Space, Classification","AminerCitationCount_02-2020":20,"AminerCitationCount_06-2020":24,"XploreCitationCount - 2020-01":16,"PubsCited":29,"Award":null,"image":"5652398-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Improving the visual analysis of high-dimensional datasets using quality measures","DOI":"10.1109/VAST.2010.5652433","Link":"http://dx.doi.org/10.1109/VAST.2010.5652433","FirstPage":19,"LastPage":26,"PaperType":"C","Abstract":"Modern visualization methods are needed to cope with very high-dimensional data. Efficient visual analytical techniques are required to extract the information content in these data. The large number of possible projections for each method, which usually grow quadrat-ically or even exponentially with the number of dimensions, urges the necessity to employ automatic reduction techniques, automatic sorting or selecting the projections, based on their information-bearing content. Different quality measures have been successfully applied for several specified user tasks and established visualization techniques, like Scatterplots, Scatterplot Matrices or Parallel Coordinates. Many other popular visualization techniques exist, but due to the structural differences, the measures are not directly applicable to them and new approaches are needed. In this paper we propose new quality measures for three popular visualization methods: Radviz, Pixel-Oriented Displays and Table Lenses. Our experiments show that these measures efficiently guide the visual analysis task.","AuthorNames-Deduped":"Georgia Albuquerque;Martin Eisemann;Dirk J. Lehmann;Holger Theisel;Marcus A. Magnor","AuthorNames":"Georgia Albuquerque;Martin Eisemann;Dirk J. Lehmann;Holger Theisel;Marcus Magnor","AuthorAffiliation":"TU Braunschweig, Germany;TU Braunschweig, Germany;University of Magdeburg, Germany;University of Magdeburg, Germany;TU Braunschweig, Germany","InternalReferences":"10.1109/INFVIS.2005.1532145;10.1109/INFVIS.2005.1532142;10.1109/VISUAL.1997.663916;10.1109/VAST.2006.261423;10.1109/VAST.2009.5332628;10.1109/TVCG.2008.173","AuthorKeywords":null,"AminerCitationCount_02-2020":37,"AminerCitationCount_06-2020":43,"XploreCitationCount - 2020-01":30,"PubsCited":29,"Award":null,"image":"5652433-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"iVisClassifier: An interactive visual analytics system for classification based on supervised dimension reduction","DOI":"10.1109/VAST.2010.5652443","Link":"http://dx.doi.org/10.1109/VAST.2010.5652443","FirstPage":27,"LastPage":34,"PaperType":"C","Abstract":"We present an interactive visual analytics system for classification, iVisClassifier, based on a supervised dimension reduction method, linear discriminant analysis (LDA). Given high-dimensional data and associated cluster labels, LDA gives their reduced dimensional representation, which provides a good overview about the cluster structure. Instead of a single two- or three-dimensional scatter plot, iVisClassifier fully interacts with all the reduced dimensions obtained by LDA through parallel coordinates and a scatter plot. Furthermore, it significantly improves the interactivity and interpretability of LDA. LDA enables users to understand each of the reduced dimensions and how they influence the data by reconstructing the basis vector into the original data domain. By using heat maps, iVisClassifier gives an overview about the cluster relationship in terms of pairwise distances between cluster centroids both in the original space and in the reduced dimensional space. Equipped with these functionalities, iVisClassifier supports users' classification tasks in an efficient way. Using several facial image data, we show how the above analysis is performed.","AuthorNames-Deduped":"Jaegul Choo;Hanseung Lee;Jaeyeon Kihm;Haesun Park","AuthorNames":"Jaegul Choo;Hanseung Lee;Jaeyeon Kihm;Haesun Park","AuthorAffiliation":"School of Computational Science and Engineering, Georgia Institute of Technology;School of Electrical and Computer Engineering, Georgia Institute of Technology;School of Interactive, Computing, Georgia Institute of Technology;School of Computational Science and Engineering, Georgia Institute of Technology","InternalReferences":"10.1109/VAST.2009.5332629;10.1109/INFVIS.2003.1249015;10.1109/INFVIS.2004.60;10.1109/TVCG.2009.153","AuthorKeywords":null,"AminerCitationCount_02-2020":55,"AminerCitationCount_06-2020":75,"XploreCitationCount - 2020-01":59,"PubsCited":29,"Award":null,"image":"5652443-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"finding and visualizing relevant subspaces for clustering high-dimensional astronomical data using connected morphological operators","DOI":"10.1109/VAST.2010.5652450","Link":"http://dx.doi.org/10.1109/VAST.2010.5652450","FirstPage":35,"LastPage":42,"PaperType":"C","Abstract":"Data sets in astronomy are growing to enormous sizes. Modern astronomical surveys provide not only image data but also catalogues of millions of objects (stars, galaxies), each object with hundreds of associated parameters. Exploration of this very high-dimensional data space poses a huge challenge. Subspace clustering is one among several approaches which have been proposed for this purpose in recent years. However, many clustering algorithms require the user to set a large number of parameters without any guidelines. Some methods also do not provide a concise summary of the datasets, or, if they do, they lack additional important information such as the number of clusters present or the significance of the clusters. In this paper, we propose a method for ranking subspaces for clustering which overcomes many of the above limitations. First we carry out a transformation from parametric space to discrete image space where the data are represented by a grid-based density field. Then we apply so-called connected morphological operators on this density field of astronomical objects that provides visual support for the analysis of the important subspaces. Clusters in subspaces correspond to high-intensity regions in the density image. The importance of a cluster is measured by a new quality criterion based on the dynamics of local maxima of the density. Connected operators are able to extract such regions with an indication of the number of clusters present. The subspaces are visualized during computation of the quality measure, so that the user can interact with the system to improve the results. In the result stage, we use three visualization toolkits linked within a graphical user interface so that the user can perform an in-depth exploration of the ranked subspaces. Evaluation based on synthetic as well as real astronomical datasets demonstrates the power of the new method. We recover various known astronomical relations directly from the data with little or no a priori assumptions. Hence, our method holds good prospects for discovering new relations as well.","AuthorNames-Deduped":"Bilkis J. Ferdosi;Hugo Buddelmeijer;Scott C. Trager;Michael H. F. Wilkinson;Jos B. T. M. Roerdink","AuthorNames":"Bilkis J. Ferdosi;Hugo Buddelmeijer;Scott Trager;Michael H. F. Wilkinson;Jos B. T. M. Roerdink","AuthorAffiliation":"Johann Bernoulli Institute for Mathematics and Computer Science, University of Groningen;Kapteyn Astronomical Institute, University of Groningen;Kapteyn Astronomical Institute, University of Groningen;Johann Bernoulli Institute for Mathematics and Computer Science, University of Groningen;Johann Bernoulli Institute for Mathematics and Computer Science, University of Groningen","InternalReferences":null,"AuthorKeywords":"Subspace finding, clustering high-dimensional data, connected morphological operators, visual exploration, astronomical data","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":15,"PubsCited":31,"Award":null,"image":"5652450-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Flow-based scatterplots for sensitivity analysis","DOI":"10.1109/VAST.2010.5652460","Link":"http://dx.doi.org/10.1109/VAST.2010.5652460","FirstPage":43,"LastPage":50,"PaperType":"C","Abstract":"Visualization of multi-dimensional data is challenging due to the number of complex correlations that may be present in the data but that are difficult to be visually identified. One of the main causes for this problem is the inherent loss of information that occurs when high-dimensional data is projected into 2D or 3D. Although 2D scatterplots are ubiquitous due to their simplicity and familiarity, there are not a lot of variations on their basic metaphor. In this paper, we present a new way of visualizing multidimensional data using scatterplots. We extend 2D scatterplots using sensitivity coefficients to highlight local variation of one variable with respect to another. When applied to a scatterplot, these sensitivities can be understood as velocities, and the resulting visualization resembles a flow field. We also present a number of operations, based on flow-field analysis, that help users navigate, select and cluster points in an efficient manner. We show the flexibility and generality of this approach using a number of multidimensional data sets across different domains.","AuthorNames-Deduped":"Yu-Hsuan Chan;Carlos D. Correa;Kwan-Liu Ma","AuthorNames":"Yu-Hsuan Chan;Carlos D. Correa;Kwan-Liu Ma","AuthorAffiliation":"University of California at Davis;University of California at Davis;University of California at Davis","InternalReferences":"10.1109/TVCG.2008.119;10.1109/VAST.2008.4677368;10.1109/VAST.2009.5332611;10.1109/VAST.2007.4389000;10.1109/TVCG.2006.166;10.1109/TVCG.2008.153","AuthorKeywords":"Uncertainty, Data Transformations, Principal Component Analysis, Model fitting","AminerCitationCount_02-2020":32,"AminerCitationCount_06-2020":41,"XploreCitationCount - 2020-01":31,"PubsCited":35,"Award":null,"image":"5652460-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Anomaly detection in GPS data based on visual analytics","DOI":"10.1109/VAST.2010.5652467","Link":"http://dx.doi.org/10.1109/VAST.2010.5652467","FirstPage":51,"LastPage":58,"PaperType":"C","Abstract":"Modern machine learning techniques provide robust approaches for data-driven modeling and critical information extraction, while human experts hold the advantage of possessing high-level intelligence and domain-specific expertise. We combine the power of the two for anomaly detection in GPS data by integrating them through a visualization and human-computer interaction interface. In this paper we introduce GPSvas (GPS Visual Analytics System), a system that detects anomalies in GPS data using the approach of visual analytics: a conditional random field (CRF) model is used as the machine learning component for anomaly detection in streaming GPS traces. A visualization component and an interactive user interface are built to visualize the data stream, display significant analysis results (i.e., anomalies or uncertain predications) and hidden information extracted by the anomaly detection model, which enable human experts to observe the real-time data behavior and gain insights into the data flow. Human experts further provide guidance to the machine learning model through the interaction tools; the learning model is then incrementally improved through an active learning procedure.","AuthorNames-Deduped":"Zicheng Liao;Yizhou Yu;Baoquan Chen","AuthorNames":"Zicheng Liao;Yizhou Yu;Baoquan Chen","AuthorAffiliation":"University of Illinois at Urbana-Champaign;University of Illinois at Urbana-Champaign;Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences","InternalReferences":"10.1109/TVCG.2009.145","AuthorKeywords":null,"AminerCitationCount_02-2020":48,"AminerCitationCount_06-2020":59,"XploreCitationCount - 2020-01":43,"PubsCited":28,"Award":null,"image":"5652467-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Discovering bits of place histories from people's activity traces","DOI":"10.1109/VAST.2010.5652478","Link":"http://dx.doi.org/10.1109/VAST.2010.5652478","FirstPage":59,"LastPage":66,"PaperType":"C","Abstract":"Events that happened in the past are important for understanding the ongoing processes, predicting future developments, and making informed decisions. Significant and/or interesting events tend to attract many people. Some people leave traces of their attendance in the form of computer-processable data, such as records in the databases of mobile phone operators or photos on photo sharing web sites. We developed a suite of visual analytics methods for reconstructing past events from these activity traces. Our tools combine geocomputations, interactive geovisualizations and statistical methods to enable integrated analysis of the spatial, temporal, and thematic components of the data, including numeric attributes and texts. We demonstrate the utility of our approach on two large real data sets, mobile phone calls in Milano during 9 days and flickr photos made on British Isles during 5 years.","AuthorNames-Deduped":"Gennady L. Andrienko;Natalia V. Andrienko;Martin Mladenov;Michael Mock;Christian Pölitz","AuthorNames":"Gennady Andrienko;Natalia Andrienko;Martin Mladenov;Michael Mock;Christian Pölitz","AuthorAffiliation":"Fraunhofer Institute IAIS (Intelligent Analysis and Information Systems), Sankt Augustin, Germany;Fraunhofer Institute IAIS (Intelligent Analysis and Information Systems), Sankt Augustin, Germany;Fraunhofer Institute IAIS (Intelligent Analysis and Information Systems), Sankt Augustin, Germany;Fraunhofer Institute IAIS (Intelligent Analysis and Information Systems), Sankt Augustin, Germany;Fraunhofer Institute IAIS (Intelligent Analysis and Information Systems), Sankt Augustin, Germany","InternalReferences":"10.1109/INFVIS.1999.801851;10.1109/TVCG.2007.70621","AuthorKeywords":"event detection, spatio-temporal data, time series analysis, scalable visualization, geovisualization","AminerCitationCount_02-2020":24,"AminerCitationCount_06-2020":26,"XploreCitationCount - 2020-01":17,"PubsCited":23,"Award":"HM","image":"5652478-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"A visual analytics approach to model learning","DOI":"10.1109/VAST.2010.5652484","Link":"http://dx.doi.org/10.1109/VAST.2010.5652484","FirstPage":67,"LastPage":74,"PaperType":"C","Abstract":"The process of learning models from raw data typically requires a substantial amount of user input during the model initialization phase. We present an assistive visualization system which greatly reduces the load on the users and makes the process of model initialization and refinement more efficient, problem-driven, and engaging. Utilizing a sequence segmentation task with a Hidden Markov Model as an example, we assign each token in the sequence a feature vector based on its various properties within the sequence. These vectors are then clustered according to similarity, generating a layout of the individual tokens in form of a node link diagram where the length of the links is determined by the feature vector similarity. Users may then tune the weights of the feature vector components to improve the segmentation, which is visualized as a better separation of the clusters. Also, as individual clusters represent different classes, the user can now work at the cluster level to define token classes, instead of labelling one entry at time. Inconsistent entries visually identify themselves by locating at the periphery of clusters, and the user then helps refine the model by resolving these inconsistencies. Our system therefore makes efficient use of the knowledge of its users, only requesting user assistance for non-trivial data items. It so allows users to visually analyse data at a higher, more abstract level, improving scalability.","AuthorNames-Deduped":"Supriya Garg;I. V. Ramakrishnan;Klaus Mueller","AuthorNames":"Supriya Garg;I. V. Ramakrishnan;Klaus Mueller","AuthorAffiliation":"Computer Science Department, Stony Brook University;Computer Science Department, Stony Brook University;Computer Science Department, Stony Brook University","InternalReferences":"10.1109/VAST.2008.4677352;10.1109/VAST.2008.4677350;10.1109/VAST.2009.5332584;10.1109/VAST.2007.4388990;10.1109/VAST.2009.5333428;10.1109/TVCG.2007.70592","AuthorKeywords":"Visual Knowledge Discovery, Visual Knowledge Representation, Data Clustering, Human-Computer Interaction","AminerCitationCount_02-2020":11,"AminerCitationCount_06-2020":13,"XploreCitationCount - 2020-01":9,"PubsCited":20,"Award":null,"image":"5652484-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Multidimensional data dissection using attribute relationship graphs","DOI":"10.1109/VAST.2010.5652520","Link":"http://dx.doi.org/10.1109/VAST.2010.5652520","FirstPage":75,"LastPage":82,"PaperType":"C","Abstract":"Visual exploration and analysis is a process of discovering and dissecting the abundant and complex attribute relationships that pervade multidimensional data. Recent research has identified and characterized patterns of multiple coordinated views, such as cross-filtered views, in which rapid sequences of simple interactions can be used to express queries on subsets of attribute values. In visualizations designed around these patterns, for the most part, distinct views serve to visually isolate each attribute from the others. Although the brush-and-click simplicity of visual isolation facilitates discovery of many-to-many relationships between attributes, dissecting these relationships into more fine-grained one-to-many relationships is interactively tedious and, worse, visually fragmented over prolonged sequences of queries. This paper describes: (1) a method for interactively dissecting multidimensional data by iteratively slicing and manipulating a multigraph representation of data values and value co-occurrences; and (2) design strategies for extending the construction of coordinated multiple view interfaces for dissection as well as discovery of attribute relationships in multidimensional data sets. Using examples from different domains, we describe how attribute relationship graphs can be combined with cross-filtered views, modularized for reuse across designs, and integrated into broader visual analysis tools. The exploratory and analytic utility of these examples suggests that an attribute relationship graph would be a useful addition to a wide variety of visual analysis tools.","AuthorNames-Deduped":"Chris Weaver","AuthorNames":"Chris Weaver","AuthorAffiliation":"School of Computer Science and Center for Spatial Analysis The University of Oklahoma","InternalReferences":"10.1109/TVCG.2008.137;10.1109/TVCG.2006.122;10.1109/VAST.2006.261432;10.1109/INFVIS.2004.12;10.1109/TVCG.2006.178;10.1109/INFVIS.2002.1173158;10.1109/VISUAL.1994.346302;10.1109/INFVIS.1998.729560;10.1109/TVCG.2007.70582;10.1109/TVCG.2007.70594;10.1109/VAST.2007.4389006;10.1109/INFVIS.2005.1532129;10.1109/INFVIS.2004.64","AuthorKeywords":null,"AminerCitationCount_02-2020":10,"AminerCitationCount_06-2020":13,"XploreCitationCount - 2020-01":7,"PubsCited":37,"Award":null,"image":"5652520-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2010,"Title":"Visual market sector analysis for financial time series data","DOI":"10.1109/VAST.2010.5652530","Link":"http://dx.doi.org/10.1109/VAST.2010.5652530","FirstPage":83,"LastPage":90,"PaperType":"C","Abstract":"The massive amount of financial time series data that originates from the stock market generates large amounts of complex data of high interest. However, adequate solutions that can effectively handle the information in order to gain insight and to understand the market mechanisms are rare. In this paper, we present two techniques and applications that enable the user to interactively analyze large amounts of time series data in real-time in order to get insight into the development of assets, market sectors, countries, and the financial market as a whole. The first technique allows users to quickly analyze combinations of single assets, market sectors as well as countries, compare them to each other, and to visually discover the periods of time where market sectors and countries get into turbulence. The second application clusters a selection of large amounts of financial time series data according to their similarity, and analyzes the distribution of the assets among market sectors. This allows users to identify the characteristic graphs which are representative for the development of a particular market sector, and also to identify the assets which behave considerably differently compared to other assets in the same sector. Both applications allow the user to perform investigative exploration techniques and interactive visual analysis in real-time.","AuthorNames-Deduped":"Hartmut Ziegler;Marco Jenny;Tino Gruse;Daniel A. Keim","AuthorNames":"Hartmut Ziegler;Marco Jenny;Tino Gruse;Daniel A. Keim","AuthorAffiliation":"University of Konstanz;University of Konstanz;University of Konstanz;University of Konstanz","InternalReferences":"10.1109/INFVIS.2001.963273;10.1109/INFVIS.1997.636789;10.1109/INFVIS.2001.963288;10.1109/INFVIS.1999.801851;10.1109/INFVIS.2003.1249027","AuthorKeywords":"Visual Analytics, financial Information Visualization, Time Series Data, Time Series Clustering, Explorative Analysis","AminerCitationCount_02-2020":31,"AminerCitationCount_06-2020":43,"XploreCitationCount - 2020-01":33,"PubsCited":49,"Award":null,"image":"5652530-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Large-scale neuroanatomical visualization using a manifold embedding approach","DOI":"10.1109/VAST.2010.5652532","Link":"http://dx.doi.org/10.1109/VAST.2010.5652532","FirstPage":237,"LastPage":238,"PaperType":"M","Abstract":"We present a unified framework for data processing, mining and interactive visualization of large-scale neuroanatomical databases. The input data is assumed to lie in a specific atlas space, or simply exist as a separate collection. Users can specify their own atlas for comparative analyses. The original data exist as MRI images in standard formats. It is uploaded to a remote server and processed offline by a parallelized pipeline workflow. This workflow transforms the data to represent it as both volumetric and triangular mesh cortical surfaces. We use multiresolution representations to scale complexity to data storage availability as well as graphical processing performance. Our workflow implements predefined metrics for clustering and classification, and data projection schemes to aid in visualization. Additionally the system provides a visual query interface for performing selection requests based on user-defined search criteria.","AuthorNames-Deduped":"Shantanu H. Joshi;Ian Bowman;John D. Van Horn","AuthorNames":"Shantanu H. Joshi;Ian Bowman;John Darrell Van Horn","AuthorAffiliation":"Laboratory of Neuro Imaging, Department of Neurology, UCLA School of Medicine, University of California Los Angeles, CA 90034;Laboratory of Neuro Imaging, Department of Neurology, UCLA School of Medicine, University of California Los Angeles, CA 90034;Laboratory of Neuro Imaging, Department of Neurology, UCLA School of Medicine, University of California Los Angeles, CA 90034","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":6,"Award":null,"image":"5652532-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"A closer look at note taking in the co-located collaborative visual analytics process","DOI":"10.1109/VAST.2010.5652879","Link":"http://dx.doi.org/10.1109/VAST.2010.5652879","FirstPage":171,"LastPage":178,"PaperType":"C","Abstract":"This paper highlights the important role that record-keeping (i.e. taking notes and saving charts) plays in collaborative data analysis within the business domain. The discussion of record-keeping is based on observations from a user study in which co-located teams worked on collaborative visual analytics tasks using large interactive wall and tabletop displays. Part of our findings is a collaborative data analysis framework that encompasses note taking as one of the main activities. We observed that record-keeping was a critical activity within the analysis process. Based on our observations, we characterize notes according to their content, scope, and usage, and describe how they fit into a process of collaborative data analysis. We then discuss suggestions for the design of collaborative visual analytics tools.","AuthorNames-Deduped":"Narges Mahyar;Ali Sarvghad;Melanie Tory","AuthorNames":"Narges Mahyar;Ali Sarvghad;Melanie Tory","AuthorAffiliation":"University of Victoria;University of Victoria;University of Victoria","InternalReferences":"10.1109/TVCG.2008.137;10.1109/VAST.2008.4677358;10.1109/TVCG.2007.70568;10.1109/VAST.2009.5333020;10.1109/VAST.2008.4677365;10.1109/VAST.2009.5333023;10.1109/TVCG.2007.70577","AuthorKeywords":"note taking, recording, collaboration, tabletop, wall display, history, provenance","AminerCitationCount_02-2020":14,"AminerCitationCount_06-2020":16,"XploreCitationCount - 2020-01":8,"PubsCited":33,"Award":null,"image":"5652879-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"An exploratory study of co-located collaborative visual analytics around a tabletop display","DOI":"10.1109/VAST.2010.5652880","Link":"http://dx.doi.org/10.1109/VAST.2010.5652880","FirstPage":179,"LastPage":186,"PaperType":"C","Abstract":"Co-located collaboration can be extremely valuable during complex visual analytics tasks. This paper presents an exploratory study of a system designed to support collaborative visual analysis tasks on a digital tabletop display. Fifteen participant pairs employed Cam-biera, a visual analytics system, to solve a problem involving 240 digital documents. Our analysis, supported by observations, system logs, questionnaires, and interview data, explores how pairs approached the problem around the table. We contribute a unique, rich understanding of how users worked together around the table and identify eight types of collaboration styles that can be used to identify how closely people work together while problem solving. We show how the closeness of teams' collaboration influenced how well they performed on the task overall. We further discuss the role of the tabletop for visual analytics tasks and derive novel design implications for future co-located collaborative tabletop problem solving systems.","AuthorNames-Deduped":"Petra Isenberg;Danyel Fisher;Meredith Ringel Morris;Kori Inkpen;Mary Czerwinski","AuthorNames":"Petra Isenberg;Danyel Fisher;Meredith Ringel Morris;Kori Inkpen;Mary Czerwinski","AuthorAffiliation":"INRIA;Microsoft Research;Microsoft Research;Microsoft Research;Microsoft Research","InternalReferences":"10.1109/VAST.2006.261439;10.1109/VAST.2007.4389006;10.1109/VAST.2006.261415;10.1109/TVCG.2007.70577;10.1109/VAST.2008.4677358;10.1109/TVCG.2007.70568","AuthorKeywords":null,"AminerCitationCount_02-2020":66,"AminerCitationCount_06-2020":75,"XploreCitationCount - 2020-01":40,"PubsCited":22,"Award":"HM","image":"5652880-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Click2Annotate: Automated Insight Externalization with rich semantics","DOI":"10.1109/VAST.2010.5652885","Link":"http://dx.doi.org/10.1109/VAST.2010.5652885","FirstPage":155,"LastPage":162,"PaperType":"C","Abstract":"Insight Externalization (IE) refers to the process of capturing and recording the semantics of insights in decision making and problem solving. To reduce human effort, Automated Insight Externalization (AIE) is desired. Most existing IE approaches achieve automation by capturing events (e.g., clicks and key presses) or actions (e.g., panning and zooming). In this paper, we propose a novel AIE approach named Click2Annotate. It allows semi-automatic insight annotation that captures low-level analytics task results (e.g., clusters and outliers), which have higher semantic richness and abstraction levels than actions and events. Click2Annotate has two significant benefits. First, it reduces human effort required in IE and generates annotations easy to understand. Second, the rich semantic information encoded in the annotations enables various insight management activities, such as insight browsing and insight retrieval. We present a formal user study that proved this first benefit. We also illustrate the second benefit by presenting the novel insight management activities we developed based on Click2Annotate, namely scented insight browsing and faceted insight search.","AuthorNames-Deduped":"Yang Chen;Scott Barlowe;Jing Yang 0001","AuthorNames":"Yang Chen;Scott Barlowe;Jing Yang","AuthorAffiliation":"Department of Computer Science UNC Charlotte;Department of Computer Science UNC Charlotte;Department of Computer Science UNC Charlotte","InternalReferences":"10.1109/VISUAL.1990.146375;10.1109/INFVIS.2005.1532136;10.1109/TVCG.2007.70541;10.1109/VAST.2008.4677365;10.1109/TVCG.2007.70577;10.1109/TVCG.2009.139","AuthorKeywords":"Visual Analytics, Decision Making, Annotation, Insight Management, Multidimensional Visualization","AminerCitationCount_02-2020":18,"AminerCitationCount_06-2020":24,"XploreCitationCount - 2020-01":13,"PubsCited":18,"Award":null,"image":"5652885-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2010,"Title":"Interactive querying of temporal data using a comic strip metaphor","DOI":"10.1109/VAST.2010.5652890","Link":"http://dx.doi.org/10.1109/VAST.2010.5652890","FirstPage":163,"LastPage":170,"PaperType":"C","Abstract":"Finding patterns in temporal data is an important data analysis task in many domains. Static visualizations can help users easily see certain instances of patterns, but are not specially designed to support systematic analysis tasks, such as finding all instances of a pattern automatically. VizPattern is an interactive visual query environment that uses a comic strip metaphor to enable users to easily and quickly define and locate complex temporal patterns. Evaluations provide evidence that VizPattern is applicable in many domains, and that it enables a wide variety of users to answer questions about temporal data faster and with fewer errors than existing state-of-the-art visual analysis systems.","AuthorNames-Deduped":"Jing Jin;Pedro A. Szekely","AuthorNames":"Jing Jin;Pedro Szekely","AuthorAffiliation":"Information Sciences Institute University of Southern California;Information Sciences Institute University of Southern California","InternalReferences":"10.1109/VAST.2006.261421;10.1109/VAST.2006.261436","AuthorKeywords":null,"AminerCitationCount_02-2020":19,"AminerCitationCount_06-2020":23,"XploreCitationCount - 2020-01":13,"PubsCited":23,"Award":null,"image":"5652890-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2010,"Title":"Geo-historical context support for information foraging and sensemaking: Conceptual model, implementation, and assessment","DOI":"10.1109/VAST.2010.5652895","Link":"http://dx.doi.org/10.1109/VAST.2010.5652895","FirstPage":139,"LastPage":146,"PaperType":"C","Abstract":"Information foraging and sensemaking with heterogeneous information are context-dependent activities. Thus visual analytics tools to support these activities must incorporate context. But, context is a difficult concept to define, model, and represent. Creating and representing context in support of visually-enabled reasoning about complex problems with complex information is a complementary but different challenge than that addressed in context-aware computing. In the latter, the goal is automated adaptation of the system to meet user needs for applications such as mobile location-based services where information about the location, the user, and the user goals filters what gets presented on a small mobile device. In contrast, for visual analytics-enabled information foraging and sensemaking, the user is likely to take an active role in foraging for the contextual information needed to support sensemaking in relation to some multifaceted problem. In this paper, we address the challenges of constructing and representing context within visual interfaces that support analytical reasoning in crisis management and humanitarian relief. The challenges stem from the diverse forms of information that can provide context and difficulty in defining and operationalizing context itself. Here, we pay particular attention to document foraging to support construction of the geographic and historical context within which monitoring and sensemaking can be carried out. Specifically, we present the concept of geo-historical context (GHC) and outline an empirical assessment of both the concept and its implementation in the Context Discovery Application, a web-based tool that supports document foraging and sensemaking.","AuthorNames-Deduped":"Brian M. Tomaszewski;Alan M. MacEachren","AuthorNames":"Brian Tomaszewski;Alan M. MacEachren","AuthorAffiliation":"Department of Information Sciences and Technologies, Rochester Institute of Technology;GeoVISTA Center, Dept. of Geography, The Pennsylvania State University","InternalReferences":null,"AuthorKeywords":"context, foraging, sensemaking, mapping, text analysis, geographic information retrieval","AminerCitationCount_02-2020":13,"AminerCitationCount_06-2020":13,"XploreCitationCount - 2020-01":7,"PubsCited":32,"Award":null,"image":"5652895-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Real-time aggregation of Wikipedia data for visual analytics","DOI":"10.1109/VAST.2010.5652896","Link":"http://dx.doi.org/10.1109/VAST.2010.5652896","FirstPage":147,"LastPage":154,"PaperType":"C","Abstract":"Wikipedia has been built to gather encyclopedic knowledge using a collaborative social process that has proved its effectiveness. However, the workload required for raising the quality and increasing the coverage of Wikipedia is exhausting the community. Based on several participatory design sessions with active Wikipedia contributors (a.k.a. Wikipedians), we have collected a set of measures related to Wikipedia activity that, if available and visualized effectively, could spare a lot of monitoring time to these Wikipedians, allowing them to focus on quality and coverage of Wikipedia instead of spending their time navigating heavily to track vandals and copyright infringements. However, most of these measures cannot be computed on the fly using the available Wikipedia API. Therefore, we have designed an open architecture called WikiReactive to compute incrementally and maintain several aggregated measures on the French Wikipedia. This aggregated data is available as a Web Service and can be used to overlay information on Wikipedia articles through Wikipedia Skins or for new services for Wikipedians or people studying Wikipedia. This article describes the architecture, its performance and some of its uses.","AuthorNames-Deduped":"Nadia Boukhelifa;Fanny Chevalier;Jean-Daniel Fekete","AuthorNames":"Nadia Boukhelifa;Fanny Chevalier;Jean-Daniel Fekete","AuthorAffiliation":"Microsoft Research - INRIA Joint Centre;Microsoft Research - INRIA Joint Centre;INRIA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":7,"AminerCitationCount_06-2020":8,"XploreCitationCount - 2020-01":4,"PubsCited":23,"Award":null,"image":"5652896-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"NetClinic: Interactive visualization to enhance automated fault diagnosis in enterprise networks","DOI":"10.1109/VAST.2010.5652910","Link":"http://dx.doi.org/10.1109/VAST.2010.5652910","FirstPage":131,"LastPage":138,"PaperType":"C","Abstract":"Diagnosing faults in an operational computer network is a frustrating, time-consuming exercise. Despite advances, automatic diagnostic tools are far from perfect: they occasionally miss the true culprit and are mostly only good at narrowing down the search to a few potential culprits. This uncertainty and the inability to extract useful sense from tool output renders most tools not usable to administrators. To bridge this gap, we present NetClinic, a visual analytics system that couples interactive visualization with an automated diagnostic tool for enterprise networks. It enables administrators to verify the output of the automatic analysis at different levels of detail and to move seamlessly across levels while retaining appropriate context. A qualitative user study shows that NetClinic users can accurately identify the culprit, even when it is not present in the suggestions made by the automated component. We also find that supporting a variety of sensemaking strategies is a key to the success of systems that enhance automated diagnosis.","AuthorNames-Deduped":"Zhicheng Liu;Bongshin Lee;Srikanth Kandula;Ratul Mahajan","AuthorNames":"Zhicheng Liu;Bongshin Lee;Srikanth Kandula;Ratul Mahajan","AuthorAffiliation":"Microsoft Research, Georgia Institute of Technology;Microsoft Research;Microsoft Research;Microsoft Research","InternalReferences":"10.1109/TVCG.2006.122;10.1109/TVCG.2007.70522;10.1109/VAST.2009.5333878;10.1109/VAST.2007.4389006;10.1109/VAST.2006.261429","AuthorKeywords":"Sensemaking, Semantic Graph Layout, Visual Analytics, Network Diagnosis, Information Visualization","AminerCitationCount_02-2020":18,"AminerCitationCount_06-2020":18,"XploreCitationCount - 2020-01":8,"PubsCited":28,"Award":null,"image":"5652910-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Diamonds in the rough: Social media visual analytics for journalistic inquiry","DOI":"10.1109/VAST.2010.5652922","Link":"http://dx.doi.org/10.1109/VAST.2010.5652922","FirstPage":115,"LastPage":122,"PaperType":"C","Abstract":"Journalists increasingly turn to social media sources such as Facebook or Twitter to support their coverage of various news events. For large-scale events such as televised debates and speeches, the amount of content on social media can easily become overwhelming, yet still contain information that may aid and augment reporting via individual content items as well as via aggregate information from the crowd's response. In this work we present a visual analytic tool, Vox Civitas, designed to help journalists and media professionals extract news value from large-scale aggregations of social media content around broadcast events. We discuss the design of the tool, present the text analysis techniques used to enable the presentation, and provide details on the visual and interaction design. We provide an exploratory evaluation based on a user study in which journalists interacted with the system to explore and report on a dataset of over one hundred thousand twitter messages collected during the U.S. State of the Union presidential address in 2010.","AuthorNames-Deduped":"Nicholas Diakopoulos;Mor Naaman;Funda Kivran-Swaine","AuthorNames":"Nicholas Diakopoulos;Mor Naaman;Funda Kivran-Swaine","AuthorAffiliation":"Rutgers University, School of Communication and Information;Rutgers University, School of Communication and Information;Rutgers University, School of Communication and Information","InternalReferences":"10.1109/VAST.2009.5333437;10.1109/VAST.2009.5333443;10.1109/VAST.2009.5333878;10.1109/VAST.2008.4677364","AuthorKeywords":"Computational Journalism, Computer Assisted Reporting, Social Media, Sensemaking","AminerCitationCount_02-2020":134,"AminerCitationCount_06-2020":157,"XploreCitationCount - 2020-01":85,"PubsCited":27,"Award":null,"image":"5652922-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Visual readability analysis: How to make your writings easier to read","DOI":"10.1109/VAST.2010.5652926","Link":"http://dx.doi.org/10.1109/VAST.2010.5652926","FirstPage":123,"LastPage":130,"PaperType":"C","Abstract":"We present a tool that is specifically designed to support a writer in revising a draft-version of a document. In addition to showing which paragraphs and sentences are difficult to read and understand, we assist the reader in understanding why this is the case. This requires features that are expressive predictors of readability, and are also semantically understandable. In the first part of the paper, we therefore discuss a semi-automatic feature selection approach that is used to choose appropriate measures from a collection of 141 candidate readability features. In the second part, we present the visual analysis tool VisRA, which allows the user to analyze the feature values across the text and within single sentences. The user can choose different visual representations accounting for differences in the size of the documents and the availability of information about the physical and logical layout of the documents. We put special emphasis on providing as much transparency as possible to ensure that the user can purposefully improve the readability of a sentence. Several case-studies are presented that show the wide range of applicability of our tool.","AuthorNames-Deduped":"Daniela Oelke;David Spretke;Andreas Stoffel;Daniel A. Keim","AuthorNames":"Daniela Oelke;David Spretke;Andreas Stoffel;Daniel A. Keim","AuthorAffiliation":"University of Konstanz;University of Konstanz;University of Konstanz;University of Konstanz","InternalReferences":"10.1109/VAST.2007.4389004","AuthorKeywords":null,"AminerCitationCount_02-2020":24,"AminerCitationCount_06-2020":22,"XploreCitationCount - 2020-01":3,"PubsCited":29,"Award":"BP","image":"5652926-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Understanding text corpora with multiple facets","DOI":"10.1109/VAST.2010.5652931","Link":"http://dx.doi.org/10.1109/VAST.2010.5652931","FirstPage":99,"LastPage":106,"PaperType":"C","Abstract":"Text visualization becomes an increasingly more important research topic as the need to understand massive-scale textual information is proven to be imperative for many people and businesses. However, it is still very challenging to design effective visual metaphors to represent large corpora of text due to the unstructured and high-dimensional nature of text. In this paper, we propose a data model that can be used to represent most of the text corpora. Such a data model contains four basic types of facets: time, category, content (unstructured), and structured facet. To understand the corpus with such a data model, we develop a hybrid visualization by combining the trend graph with tag-clouds. We encode the four types of data facets with four separate visual dimensions. To help people discover evolutionary and correlation patterns, we also develop several visual interaction methods that allow people to interactively analyze text by one or more facets. Finally, we present two case studies to demonstrate the effectiveness of our solution in support of multi-faceted visual analysis of text corpora.","AuthorNames-Deduped":"Lei Shi;Furu Wei;Shixia Liu;Li Tan;Xiaoxiao Lian;Michelle X. Zhou","AuthorNames":"Lei Shi;Furu Wei;Shixia Liu;Li Tan;Xiaoxiao Lian;Michelle X. Zhou","AuthorAffiliation":"IBM Research - China, 19 Zhongguancun Software Park, Beijing 100193, China;IBM Research - China, 19 Zhongguancun Software Park, Beijing 100193, China;IBM Research - China, 19 Zhongguancun Software Park, Beijing 100193, China;IBM Research - China, 19 Zhongguancun Software Park, Beijing 100193, China;IBM Research - China, 19 Zhongguancun Software Park, Beijing 100193, China;IBM Research - Almaden, 650 Harry Road San Jose, CA 95120, USA","InternalReferences":"10.1109/VAST.2009.5333443;10.1109/VAST.2007.4389005;10.1109/TVCG.2008.172;10.1109/TVCG.2009.171;10.1109/TVCG.2008.166;10.1109/TVCG.2009.165;10.1109/INFVIS.2002.1173155;10.1109/INFVIS.1999.801866;10.1109/VAST.2007.4389006;10.1109/INFVIS.2005.1532122;10.1109/INFVIS.2000.885097","AuthorKeywords":"text visualization, multi-facet data visualization","AminerCitationCount_02-2020":56,"AminerCitationCount_06-2020":59,"XploreCitationCount - 2020-01":30,"PubsCited":29,"Award":null,"image":"5652931-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2010,"Title":"VizCept: Supporting synchronous collaboration for constructing visualizations in intelligence analysis","DOI":"10.1109/VAST.2010.5652932","Link":"http://dx.doi.org/10.1109/VAST.2010.5652932","FirstPage":107,"LastPage":114,"PaperType":"C","Abstract":"In this paper, we present a new web-based visual analytics system, VizCept, which is designed to support fluid, collaborative analysis of large textual intelligence datasets. The main approach of the design is to combine individual workspace and shared visualization in an integrated environment. Collaborating analysts will be able to identify concepts and relationships from the dataset based on keyword searches in their own workspace and collaborate visually with other analysts using visualization tools such as a concept map view and a timeline view. The system allows analysts to parallelize the work by dividing initial sets of concepts, investigating them on their own workspace, and then integrating individual findings automatically on shared visualizations with support for interaction and personal graph layout in real time, in order to develop a unified plot. We highlight several design considerations that promote communication and analytic performance in small team synchronous collaboration. We report the result of a pair of case study applications including collaboration and communication methods, analysis strategies, and user behaviors under a competition setting in the same location at the same time. The results of these demonstrate the tool's effectiveness for synchronous collaborative construction and use of visualizations in intelligence data analysis.","AuthorNames-Deduped":"Haeyong Chung;Seungwon Yang;Naveed Massjouni;Christopher Andrews;Rahul Kanna;Chris North","AuthorNames":"Haeyong Chung;Seungwon Yang;Naveed Massjouni;Christopher Andrews;Rahul Kanna;Chris North","AuthorAffiliation":"Department of Computer Science, Virginia Tech;Department of Computer Science, Virginia Tech;Department of Computer Science, Virginia Tech;Department of Computer Science, Virginia Tech;Department of Computer Science, Virginia Tech;Department of Computer Science, Virginia Tech","InternalReferences":"10.1109/TVCG.2009.148;10.1109/VAST.2007.4389006;10.1109/VAST.2009.5333245;10.1109/VAST.2008.4677362;10.1109/TVCG.2007.70577;10.1109/VAST.2008.4677366","AuthorKeywords":"Collaborative visualization, text and document data, intelligence analysis","AminerCitationCount_02-2020":22,"AminerCitationCount_06-2020":27,"XploreCitationCount - 2020-01":14,"PubsCited":24,"Award":null,"image":"5652932-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2010,"Title":"Two-stage framework for a topology-based projection and visualization of classified document collections","DOI":"10.1109/VAST.2010.5652940","Link":"http://dx.doi.org/10.1109/VAST.2010.5652940","FirstPage":91,"LastPage":98,"PaperType":"C","Abstract":"During the last decades, electronic textual information has become the world's largest and most important information source. Daily newspapers, books, scientific and governmental publications, blogs and private messages have grown into a wellspring of endless information and knowledge. Since neither existing nor new information can be read in its entirety, we rely increasingly on computers to extract and visualize meaningful or interesting topics and documents from this huge information reservoir. In this paper, we extend, improve and combine existing individual approaches into an overall framework that supports topologi-cal analysis of high dimensional document point clouds given by the well-known tf-idf document-term weighting method. We show that traditional distance-based approaches fail in very high dimensional spaces, and we describe an improved two-stage method for topology-based projections from the original high dimensional information space to both two dimensional (2-D) and three dimensional (3-D) visualizations. To demonstrate the accuracy and usability of this framework, we compare it to methods introduced recently and apply it to complex document and patent collections.","AuthorNames-Deduped":"Patrick Oesterling;Gerik Scheuermann;Sven Teresniak;Gerhard Heyer;Steffen Koch;Thomas Ertl;Gunther H. Weber","AuthorNames":"Patrick Oesterling;Gerik Scheuermann;Sven Teresniak;Gerhard Heyer;Steffen Koch;Thomas Ertl;Gunther H. Weber","AuthorAffiliation":"University of Leipzig;University of Leipzig;University of Leipzig;University of Leipzig;University of Stuttgart;University of Stuttgart;Lawrence Berkeley National Laboratory","InternalReferences":"10.1109/VAST.2009.5333564;10.1109/TVCG.2007.70601;10.1109/VISUAL.1990.146402;10.1109/INFVIS.1995.528686;10.1109/VAST.2009.5332629;10.1109/TVCG.2009.119","AuthorKeywords":null,"AminerCitationCount_02-2020":19,"AminerCitationCount_06-2020":19,"XploreCitationCount - 2020-01":15,"PubsCited":25,"Award":null,"image":"5652940-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Visually representing geo-temporal differences","DOI":"10.1109/VAST.2010.5652951","Link":"http://dx.doi.org/10.1109/VAST.2010.5652951","FirstPage":229,"LastPage":230,"PaperType":"M","Abstract":"Data sets that contain geospatial and temporal elements can be challenging to analyze. In particular, it can be difficult to determine how the data have changed over spatial and temporal ranges. In this poster, we present a visual approach for representing the pair-wise differences between geographically and temporally binned data. In addition to providing a novel method for visualizing such geo-temporal differences, GTdiff provides a high degree of interactivity that supports the exploration and analysis of the data.","AuthorNames-Deduped":"Orland Hoeber;Garnett Carl Wilson;Simon Harding;René Enguehard;Rodolphe Devillers","AuthorNames":"Orland Hoeber;Garnett Wilson;Simon Harding;René Enguehard;Rodolphe Devillers","AuthorAffiliation":"Department of Computer Science, Memorial University;Department of Computer Science, Memorial University;Department of Computer Science, Memorial University;Department of Geography, Memorial University;Department of Geography, Memorial University","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":7,"AminerCitationCount_06-2020":7,"XploreCitationCount - 2020-01":4,"PubsCited":5,"Award":null,"image":"5652951-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"A continuous analysis process between desktop and collaborative visual analytics environments","DOI":"10.1109/VAST.2010.5652958","Link":"http://dx.doi.org/10.1109/VAST.2010.5652958","FirstPage":231,"LastPage":232,"PaperType":"M","Abstract":"Since its inception, the field of visual analytics has undergone tremendous growth in understanding how to create interactive visual tools to solve analytical problems. However, with few exceptions, most of these tools have been designed for single users in desktop environments. While often effective on their own, most single-user systems do not reflect the collaborative nature of solving real-world analytical tasks. Many intelligence analysts, for example, have been observed to switch repeatedly between working alone and collaborating with members of a small team. In this paper, we propose that a complete visual analytical system designed for solving real-world tasks ought to have two integrated components: a single-user desktop system and a mirroring system suitable for a collaborative environment.","AuthorNames-Deduped":"Dong Hyun Jeong;Evan A. Suma;Thomas Butkiewicz;William Ribarsky;Remco Chang","AuthorNames":"Dong Hyun Jeong;Evan Suma;Thomas Butkiewicz;William Ribarsky;Remco Chang","AuthorAffiliation":"Univ. of the District of Columbia;Univ. of Southern California;Univ. of North Carolina at Charlotte;Univ. of North Carolina at Charlotte;Tufts University","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":2,"Award":null,"image":"5652958-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"EmailTime: Visual analytics of emails","DOI":"10.1109/VAST.2010.5652968","Link":"http://dx.doi.org/10.1109/VAST.2010.5652968","FirstPage":233,"LastPage":234,"PaperType":"M","Abstract":"Although the discovery and analysis of communication patterns in large and complex email datasets are difficult tasks, they can be a valuable source of information. This paper presents EmailTime's capabilities through several examples. EmailTime is a visual analysis of email correspondence patterns over the course of time that interactively portrays personal and interpersonal networks using the correspondence in the email dataset. We suggest that integrating both statistics and visualizations in order to display information about the email datasets may simplify its evaluation.","AuthorNames-Deduped":"Minoo Erfani Joorabchi;Ji-Dong Yim;Chris Shaw 0002","AuthorNames":"Minoo Erfani Joorabchi;Ji-Dong Yim;Christopher D. Shaw","AuthorAffiliation":"Simon Fraser University;Simon Fraser University;Simon Fraser University","InternalReferences":null,"AuthorKeywords":"Email, Enron, EmailTime, Email Correspondents, Visual Analysis","AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":9,"XploreCitationCount - 2020-01":4,"PubsCited":2,"Award":null,"image":"5652968-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Poster: Dynamic time transformation for interpreting clusters of trajectories with space-time cube","DOI":"10.1109/VAST.2010.5653580","Link":"http://dx.doi.org/10.1109/VAST.2010.5653580","FirstPage":213,"LastPage":214,"PaperType":"M","Abstract":"We propose a set of techniques that support visual interpretation of trajectory clusters by transforming absolute time references into relative positions within temporal cycles or with respect to the starting and/or ending times of the trajectories. We demonstrate the work of the approach on a real data set about individual movement over one year.","AuthorNames-Deduped":"Gennady L. Andrienko;Natalia V. Andrienko","AuthorNames":"Gennady Andrienko;Natalia Andrienko","AuthorAffiliation":"University of Bonn and Fraunhofer Institute IAIS;University of Bonn and Fraunhofer Institute IAIS","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":17,"AminerCitationCount_06-2020":21,"XploreCitationCount - 2020-01":17,"PubsCited":9,"Award":null,"image":"5653580-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Towards the Personal Equation of Interaction: The impact of personality factors on visual analytics interface interaction","DOI":"10.1109/VAST.2010.5653587","Link":"http://dx.doi.org/10.1109/VAST.2010.5653587","FirstPage":203,"LastPage":210,"PaperType":"C","Abstract":"These current studies explored the impact of individual differences in personality factors on interface interaction and learning performance behaviors in both an interactive visualization and a menu-driven web table in two studies. Participants were administered 3 psychometric measures designed to assess Locus of Control, Extraversion, and Neuroticism. Participants were then asked to complete multiple procedural learning tasks in each interface. Results demonstrated that all three measures predicted completion times. Additionally, results analyses demonstrated personality factors also predicted the number of insights participants reported while completing the tasks in each interface. We discuss how these findings advance our ongoing research in the Personal Equation of Interaction.","AuthorNames-Deduped":"Tera Marie Green;Brian D. Fisher","AuthorNames":"Tear Marie Green;Brian Fisher","AuthorAffiliation":"School of Interactive Arts &#x002B; Technology Simon Fraser University;School of Interactive Arts &#x002B; Technology Simon Fraser University","InternalReferences":null,"AuthorKeywords":"visual analytics, cognition and perception theory, embodied cognition, visualization taxonomies and models","AminerCitationCount_02-2020":38,"AminerCitationCount_06-2020":42,"XploreCitationCount - 2020-01":23,"PubsCited":25,"Award":null,"image":"5653587-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2010,"Title":"Helping users recall their reasoning process","DOI":"10.1109/VAST.2010.5653598","Link":"http://dx.doi.org/10.1109/VAST.2010.5653598","FirstPage":187,"LastPage":194,"PaperType":"C","Abstract":"The final product of an analyst's investigation using a visualization is often a report of the discovered knowledge, as well as the methods employed and reasoning behind the discovery. We believe that analysts may have difficulty keeping track of their knowledge discovery process and will require tools to assist in accurately recovering their reasoning. We first report on a study examining analysts' recall of their strategies and methods, demonstrating their lack of memory of the path of knowledge discovery. We then explore whether a tool visualizing the steps of the visual analysis can aid users in recalling their reasoning process. The results of our second study indicate that visualizations of interaction logs can serve as an effective memory aid, allowing analysts to recall additional details of their strategies and decisions.","AuthorNames-Deduped":"Heather Lipford;Felesia Stukes;Wenwen Dou;Matthew E. Hawkins;Remco Chang","AuthorNames":"Heather Richter Lipford;Felesia Stukes;Wenwen Dou;Matthew E. Hawkins;Remco Chang","AuthorAffiliation":"University of North Carolina at Charlotte;University of North Carolina at Charlotte;University of North Carolina at Charlotte;University of North Carolina at Charlotte;University of North Carolina at Charlotte","InternalReferences":"10.1109/TVCG.2008.137;10.1109/VAST.2007.4388992;10.1109/VAST.2008.4677365;10.1109/VAST.2008.4677360;10.1109/VAST.2007.4389009","AuthorKeywords":"Visual analytics, visualization, reasoning process ","AminerCitationCount_02-2020":23,"AminerCitationCount_06-2020":28,"XploreCitationCount - 2020-01":19,"PubsCited":20,"Award":null,"image":"5653598-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2010,"Title":"Comparing different levels of interaction constraints for deriving visual problem isomorphs","DOI":"10.1109/VAST.2010.5653599","Link":"http://dx.doi.org/10.1109/VAST.2010.5653599","FirstPage":195,"LastPage":202,"PaperType":"C","Abstract":"Interaction and manual manipulation have been shown in the cognitive science literature to play a critical role in problem solving. Given different types of interactions or constraints on interactions, a problem can appear to have different degrees of difficulty. While this relationship between interaction and problem solving has been well studied in the cognitive science literatures, the visual analytics community has yet to exploit this understanding for analytical problem solving. In this paper, we hypothesize that constraints on interactions and constraints encoded in visual representations can lead to strategies of varying effectiveness during problem solving. To test our hypothesis, we conducted a user study in which participants were given different levels of interaction constraints when solving a simple math game called Number Scrabble. Number Scrabble is known to have an optimal visual problem isomorph, and the goal of this study is to learn if and how the participants could derive the isomorph and to analyze the strategies that the participants utilize in solving the problem. Our results indicate that constraints on interactions do affect problem solving, and that while the optimal visual isomorph is difficult to derive, certain interaction constraints can lead to a higher chance of deriving the isomorph.","AuthorNames-Deduped":"Wenwen Dou;Caroline Ziemkiewicz;Lane Harrison;Dong Hyun Jeong;Roxanne Ryan;William Ribarsky;Xiaoyu Wang;Remco Chang","AuthorNames":"Wenwen Dou;Caroline Ziemkiewicz;Lane Harrison;Dong Hyun Jeong;Roxanne Ryan;William Ribarsky;Xiaoyu Wang;Remco Chang","AuthorAffiliation":"University of North Carolina at Charlotte;University of North Carolina at Charlotte;University of North Carolina at Charlotte;University of North Carolina at Charlotte;University of North Carolina at Charlotte;University of North Carolina at Charlotte;University of North Carolina at Charlotte;University of North Carolina at Charlotte","InternalReferences":"10.1109/TVCG.2007.70515;10.1109/TVCG.2008.121","AuthorKeywords":"Interaction, Visual Isomorph, Problem Solving","AminerCitationCount_02-2020":10,"AminerCitationCount_06-2020":9,"XploreCitationCount - 2020-01":4,"PubsCited":21,"Award":null,"image":"5653599-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Data representation and exploration with Geometric Wavelets","DOI":"10.1109/VAST.2010.5653822","Link":"http://dx.doi.org/10.1109/VAST.2010.5653822","FirstPage":243,"LastPage":244,"PaperType":"M","Abstract":"Geometric Wavelets is a new multi-scale data representation technique which is useful for a variety of applications such as data compression, interpretation and anomaly detection. We have developed an interactive visualization with multiple linked views to help users quickly explore data sets and understand this novel construction. Currently the interface is being used by applied mathematicians to view results and gain new insights, speeding methods development.","AuthorNames-Deduped":"Eric E. Monson;Guangliang Chen;Rachel Brady;Mauro Maggioni","AuthorNames":"Eric E. Monson;Guangliang Chen;Rachael Brady;Mauro Maggioni","AuthorAffiliation":"Duke Visualization Technology Group;Duke Mathematics;Duke Visualization Technology Group;Duke Mathematics and, Computer Science","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":1,"PubsCited":7,"Award":null,"image":"5653822-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Combining statistical independence testing, visual attribute selection and automated analysis to find relevant attributes for classification","DOI":"10.1109/VAST.2010.5654445","Link":"http://dx.doi.org/10.1109/VAST.2010.5654445","FirstPage":239,"LastPage":240,"PaperType":"M","Abstract":"We present an iterative strategy for finding a relevant subset of attributes for the purpose of classification in high-dimensional, heterogeneous data sets. The attribute subset is used for the construction of a classifier function. In order to cope with the challenge of scalability, the analysis is split into an overview of all attributes and a detailed analysis of small groups of attributes. The overview provides generic information on statistical dependencies between attributes. With this information the user can select groups of attributes and an analytical method for their detailed analysis. The detailed analysis involves the identification of redundant attributes (via classification or regression) and the creation of summarizing attributes (via clustering or dimension reduction). Our strategy does not prescribe specific analytical methods. Instead, we recursively combine the results of different methods to find or generate a subset of attributes to use for classification.","AuthorNames-Deduped":"Thorsten May;James Davey;Jörn Kohlhammer","AuthorNames":"Thorsten May;James Davey;Jörn Kohlhammer","AuthorAffiliation":"Fraunhofer Institute for Computer Graphics Research;Fraunhofer Institute for Computer Graphics Research;Fraunhofer Institute for, Computer Graphics, Research","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":5,"Award":null,"image":"5654445-fig-1-source-large.gif"},{"Conference":"VAST","Year":2010,"Title":"Visual tools for dynamic analysis of complex situations","DOI":"10.1109/VAST.2010.5654451","Link":"http://dx.doi.org/10.1109/VAST.2010.5654451","FirstPage":241,"LastPage":242,"PaperType":"M","Abstract":"This paper presents an interactive interface synchronized with a simulation framework for exploring complex scenarios. This interface exploits visual analysis for facilitating the understanding of complex situation by human users.","AuthorNames-Deduped":"Marielle Mokhtari;Eric Boivin;Denis Laurendeau;Maxime Girardin","AuthorNames":"Marielle Mokhtari;Eric Boivin;Denis Laurendeau;Maxime Girardin","AuthorAffiliation":"System of Systems Section, Defence R&amp;D Canada Quebec (QC), Canada;System of Systems Section, Defence R&amp;D Canada Quebec (QC), Canada;Computer Vision and Systems Laboratory Dept. of ECE, Laval University, Quebec (QC), Canada;Computer Vision and Systems Laboratory Dept. of ECE, Laval University, Quebec (QC), Canada","InternalReferences":null,"AuthorKeywords":"Information visualization, 2D1/2 animation, line & surface graph animation, interaction, synchronization","AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":2,"PubsCited":5,"Award":null,"image":""},{"Conference":"VAST","Year":2011,"Title":"Visual analytic roadblocks for novice investigators","DOI":"10.1109/VAST.2011.6102435","Link":"http://dx.doi.org/10.1109/VAST.2011.6102435","FirstPage":3,"LastPage":11,"PaperType":"C","Abstract":"We have observed increasing interest in visual analytics tools and their applications in investigative analysis. Despite the growing interest and substantial studies regarding the topic, understanding the major roadblocks of using such tools from novice users' perspectives is still limited. Therefore, we attempted to identify such “visual analytic roadblocks” for novice users in an investigative analysis scenario. To achieve this goal, we reviewed the existing models, theories, and frameworks that could explain the cognitive processes of human-visualization interaction in investigative analysis. Then, we conducted a qualitative experiment with six novice participants, using a slightly modified version of pair analytics, and analyzed the results through the open-coding method. As a result, we came up with four visual analytic roadblocks and explained these roadblocks using existing cognitive models and theories. We also provided design suggestions to overcome these roadblocks.","AuthorNames-Deduped":"Bum Chul Kwon;Brian D. Fisher;Ji Soo Yi","AuthorNames":"Bum chul Kwon;Brian Fisher;Ji Soo Yi","AuthorAffiliation":"Purdue University, USA;Simon Fraser University, Canada;Purdue University, USA","InternalReferences":"10.1109/INFVIS.2004.10;10.1109/VAST.2007.4389006;10.1109/TVCG.2010.164;10.1109/VAST.2009.5333878;10.1109/TVCG.2010.179;10.1109/TVCG.2008.121;10.1109/INFVIS.2004.5;10.1109/TVCG.2007.70515;10.1109/TVCG.2010.177;10.1109/VAST.2006.261416;10.1109/TVCG.2008.171;10.1109/TVCG.2008.109;10.1109/TVCG.2007.70535;10.1109/VAST.2008.4677361;10.1109/TVCG.2007.70589;10.1109/TVCG.2007.70594","AuthorKeywords":"Visual analytics, investigative analysis, cognitive model, framework, roadblock, qualitative experiment","AminerCitationCount_02-2020":17,"AminerCitationCount_06-2020":24,"XploreCitationCount - 2020-01":13,"PubsCited":28,"Award":null,"image":"6102435-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2011,"Title":"Perception-based visual quality measures","DOI":"10.1109/VAST.2011.6102437","Link":"http://dx.doi.org/10.1109/VAST.2011.6102437","FirstPage":13,"LastPage":20,"PaperType":"C","Abstract":"In recent years diverse quality measures to support the exploration of high-dimensional data sets have been proposed. Such measures can be very useful to rank and select information-bearing projections of very high dimensional data, when the visual exploration of all possible projections becomes unfeasible. But even though a ranking of the low dimensional projections may support the user in the visual exploration task, different measures deliver different distances between the views that do not necessarily match the expectations of human perception. As an alternative solution, we propose a perception-based approach that, similar to the existing measures, can be used to select information bearing projections of the data. Specifically, we construct a perceptual embedding for the different projections based on the data from a psychophysics study and multi-dimensional scaling. This embedding together with a ranking function is then used to estimate the value of the projections for a specific user task in a perceptual sense.","AuthorNames-Deduped":"Georgia Albuquerque;Martin Eisemann;Marcus A. Magnor","AuthorNames":"Georgia Albuquerque;Martin Eisemann;Marcus Magnor","AuthorAffiliation":"TU Braunschweig, Germany;TU Braunschweig, Germany;TU Braunschweig, Germany","InternalReferences":"10.1109/INFVIS.2005.1532142;10.1109/VAST.2010.5652433;10.1109/VAST.2006.261423;10.1109/VAST.2009.5332628;10.1109/TVCG.2010.184;10.1109/TVCG.2009.153","AuthorKeywords":null,"AminerCitationCount_02-2020":29,"AminerCitationCount_06-2020":30,"XploreCitationCount - 2020-01":22,"PubsCited":19,"Award":null,"image":"6102437-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Characterizing the intelligence analysis process: Informing visual analytics design through a longitudinal field study","DOI":"10.1109/VAST.2011.6102438","Link":"http://dx.doi.org/10.1109/VAST.2011.6102438","FirstPage":21,"LastPage":30,"PaperType":"C","Abstract":"While intelligence analysis has been a primary target domain for visual analytics system development, relatively little user and task analysis has been conducted within this area. Our research community's understanding of the work processes and practices of intelligence analysts is not deep enough to adequately address their needs. Without a better understanding of the analysts and their problems, we cannot build visual analytics systems that integrate well with their work processes and truly provide benefit to them. In order to close this knowledge gap, we conducted a longitudinal, observational field study of intelligence analysts in training within the intelligence program at Mercyhurst College. We observed three teams of analysts, each working on an intelligence problem for a ten-week period. Based upon study findings, we describe and characterize processes and methods of intelligence analysis that we observed, make clarifications regarding the processes and practices, and suggest design implications for visual analytics systems for intelligence analysis.","AuthorNames-Deduped":"Youn ah Kang;John T. Stasko","AuthorNames":"Youn-ah Kang;John Stasko","AuthorAffiliation":"Georgia Institute of Technology, USA;Georgia Institute of Technology, USA","InternalReferences":"10.1109/VAST.2008.4677362;10.1109/VISUAL.1992.235203;10.1109/VAST.2008.4677358;10.1109/TVCG.2009.111;10.1109/VAST.2007.4389006","AuthorKeywords":"Intelligence analysis, qualitatvie user study","AminerCitationCount_02-2020":47,"AminerCitationCount_06-2020":59,"XploreCitationCount - 2020-01":37,"PubsCited":31,"Award":null,"image":"6102438-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Interactive visual comparison of multiple trees","DOI":"10.1109/VAST.2011.6102439","Link":"http://dx.doi.org/10.1109/VAST.2011.6102439","FirstPage":31,"LastPage":40,"PaperType":"C","Abstract":"Traditionally, the visual analysis of hierarchies, respectively, trees, is conducted by focusing on one given hierarchy. However, in many research areas multiple, differing hierarchies need to be analyzed simultaneously in a comparative way - in particular to highlight differences between them, which sometimes can be subtle. A prominent example is the analysis of so-called phylogenetic trees in biology, reflecting hierarchical evolutionary relationships among a set of organisms. Typically, the analysis considers multiple phylogenetic trees, either to account for statistical significance or for differences in derivation of such evolutionary hierarchies; for example, based on different input data, such as the 16S ribosomal RNA and protein sequences of highly conserved enzymes. The simultaneous analysis of a collection of such trees leads to more insight into the evolutionary process. We introduce a novel visual analytics approach for the comparison of multiple hierarchies focusing on both global and local structures. A new tree comparison score has been elaborated for the identification of interesting patterns. We developed a set of linked hierarchy views showing the results of automatic tree comparison on various levels of details. This combined approach offers detailed assessment of local and global tree similarities. The approach was developed in close cooperation with experts from the evolutionary biology domain. We apply it to a phylogenetic data set on bacterial ancestry, demonstrating its application benefit.","AuthorNames-Deduped":"Sebastian Bremm;Tatiana von Landesberger;Martin Hess;Tobias Schreck;Philipp Weil;Kay Hamacher","AuthorNames":"Sebastian Bremm;Tatiana von Landesberger;Martin Heß;Tobias Schreck;Philipp Weil;Kay Hamacherk","AuthorAffiliation":"Interactive-Graphics Systems, TU Darmstadt, Germany;Interactive-Graphics Systems, TU Darmstadt, Germany;Interactive-Graphics Systems, TU Darmstadt, Germany;Data Analysis and Visualization Group, University of Konstanz, Germany;Computational Biology, TU Darmstadt, Germany;Computational Biology, TU Darmstadt, Germany","InternalReferences":"10.1109/TVCG.2008.114;10.1109/TVCG.2009.130;10.1109/VAST.2009.5333893;10.1109/TVCG.2007.70529","AuthorKeywords":null,"AminerCitationCount_02-2020":39,"AminerCitationCount_06-2020":44,"XploreCitationCount - 2020-01":32,"PubsCited":39,"Award":null,"image":"6102439-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Network-based visual analysis of tabular data","DOI":"10.1109/VAST.2011.6102440","Link":"http://dx.doi.org/10.1109/VAST.2011.6102440","FirstPage":41,"LastPage":50,"PaperType":"C","Abstract":"Tabular data are pervasive. Although tables often describe multivariate data without explicit network semantics, it may be advantageous to explore the data modeled as a graph or network for analysis. Even when a given table design conveys some static network semantics, analysts may want to look at multiple networks from different perspectives, at different levels of abstraction, and with different edge semantics. We present a system called Ploceus that offers a general approach for performing multi-dimensional and multi-level network-based visual analysis on multivariate tabular data. Powered by an underlying relational algebraic framework, Ploceus supports flexible construction and transformation of networks through a direct manipulation interface, and integrates dynamic network manipulation with visual exploration for a seamless analytic experience.","AuthorNames-Deduped":"Zhicheng Liu;Shamkant B. Navathe;John T. Stasko","AuthorNames":"Zhicheng Liu;Shamkant B. Navathe;John T. Stasko","AuthorAffiliation":"Georgia Institute of Technology, USA;Georgia Institute of Technology, USA;Georgia Institute of Technology, USA","InternalReferences":"10.1109/TVCG.2006.122;10.1109/TVCG.2010.177;10.1109/TVCG.2007.70582;10.1109/TVCG.2006.166;10.1109/VAST.2010.5652520;10.1109/INFVIS.2000.885086;10.1109/VAST.2007.4389006","AuthorKeywords":null,"AminerCitationCount_02-2020":35,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":20,"PubsCited":37,"Award":"HM","image":"6102440-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Orion: A system for modeling, transformation and visualization of multidimensional heterogeneous networks","DOI":"10.1109/VAST.2011.6102441","Link":"http://dx.doi.org/10.1109/VAST.2011.6102441","FirstPage":51,"LastPage":60,"PaperType":"C","Abstract":"The study of complex activities such as scientific production and software development often require modeling connections among heterogeneous entities including people, institutions and artifacts. Despite numerous advances in algorithms and visualization techniques for understanding such social networks, the process of constructing network models and performing exploratory analysis remains difficult and time-consuming. In this paper we present Orion, a system for interactive modeling, transformation and visualization of network data. Orion's interface enables the rapid manipulation of large graphs-including the specification of complex linking relationships-using simple drag-and-drop operations with desired node types. Orion maps these user interactions to statements in a declarative workflow language that incorporates both relational operators (e.g., selection, aggregation and joins) and network analytics (e.g., centrality measures). We demonstrate how these features enable analysts to flexibly construct and compare networks in domains such as online health communities, academic collaboration and distributed software development.","AuthorNames-Deduped":"Jeffrey Heer;Adam Perer","AuthorNames":"Jeffrey Heer;Adam Perer","AuthorAffiliation":"Stanford University, USA;IBM Research, USA","InternalReferences":"10.1109/TVCG.2010.144;10.1109/TVCG.2009.174;10.1109/TVCG.2006.178;10.1109/TVCG.2007.70582;10.1109/TVCG.2006.166;10.1109/VAST.2006.261426;10.1109/INFVIS.2000.885086","AuthorKeywords":"Social network analysis, data management, data transformation, graphs, visualization, end-user programming","AminerCitationCount_02-2020":17,"AminerCitationCount_06-2020":25,"XploreCitationCount - 2020-01":9,"PubsCited":34,"Award":null,"image":"6102441-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2011,"Title":"G-PARE: A visual analytic tool for comparative analysis of uncertain graphs","DOI":"10.1109/VAST.2011.6102442","Link":"http://dx.doi.org/10.1109/VAST.2011.6102442","FirstPage":61,"LastPage":70,"PaperType":"C","Abstract":"There are a growing number of machine learning algorithms which operate on graphs. Example applications for these algorithms include predicting which customers will recommend products to their friends in a viral marketing campaign using a customer network, predicting the topics of publications in a citation network, or predicting the political affiliations of people in a social network. It is important for an analyst to have tools to help compare the output of these machine learning algorithms. In this work, we present G-PARE, a visual analytic tool for comparing two uncertain graphs, where each uncertain graph is produced by a machine learning algorithm which outputs probabilities over node labels. G-PARE provides several different views which allow users to obtain a global overview of the algorithms output, as well as focused views that show subsets of nodes of interest. By providing an adaptive exploration environment, G-PARE guides the users to places in the graph where two algorithms predictions agree and places where they disagree. This enables the user to follow cascades of misclassifications by comparing the algorithms outcome with the ground truth. After describing the features of G-PARE, we illustrate its utility through several use cases based on networks from different domains.","AuthorNames-Deduped":"Hossam Sharara;Awalin Sopan;Galileo Namata;Lise Getoor;Lisa Singh","AuthorNames":"Hossam Sharara;Awalin Sopan;Galileo Namata;Lise Getoor;Lisa Singh","AuthorAffiliation":"Computer Science Department, University of Maryland, College Park, USA;Computer Science Department, University of Maryland, College Park, USA;Computer Science Department, University of Maryland, College Park, USA;Computer Science Department, University of Maryland, College Park, USA;Computer Science Department, Georgetown University, Washington DC, USA","InternalReferences":"10.1109/TVCG.2006.122;10.1109/VAST.2010.5652398;10.1109/VAST.2010.5652910;10.1109/VAST.2006.261429;10.1109/VAST.2010.5652443;10.1109/TVCG.2007.70582","AuthorKeywords":"Uncertain Graphs, Comparative Analysis, Model Comparison, Visualizing Uncertainty","AminerCitationCount_02-2020":10,"AminerCitationCount_06-2020":11,"XploreCitationCount - 2020-01":8,"PubsCited":38,"Award":null,"image":"6102442-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2011,"Title":"Visual social network analytics for relationship discovery in the enterprise","DOI":"10.1109/VAST.2011.6102443","Link":"http://dx.doi.org/10.1109/VAST.2011.6102443","FirstPage":71,"LastPage":79,"PaperType":"C","Abstract":"As people continue to author and share increasing amounts of information in social media, the opportunity to leverage such information for relationship discovery tasks increases. In this paper, we describe a set of systems that mine, aggregate, and infer a social graph from social media inside an enterprise, resulting in over 73 million relationships between 450,000 people. We then describe SaNDVis, a novel visual analytics tool that supports people-centric tasks like expertise location, team building, and team coordination in the enterprise. We also provide details of a 12-month-long, large-scale deployment to almost 1,800 users from which we extract dominant use cases from log and interview data. By integrating social position, evidence, and facets into SaNDVis, we demonstrate how users can use a visual analytics tool to reflect on existing relationships as well as build new relationships in an enterprise setting.","AuthorNames-Deduped":"Adam Perer;Ido Guy;Erel Uziel;Inbal Ronen;Michal Jacovi","AuthorNames":"Adam Perer;Ido Guy;Erel Uziel;Inbal Ronen;Michal Jacovi","AuthorAffiliation":"IBM Research, USA;IBM Research, USA;IBM Research, USA;IBM Research, USA;IBM Research, USA","InternalReferences":"10.1109/TVCG.2006.122;10.1109/TVCG.2007.70582;10.1109/VAST.2006.261426;10.1109/INFVIS.2005.1532126;10.1109/TVCG.2006.166","AuthorKeywords":"information discovery, social networks, social data mining, social visualization","AminerCitationCount_02-2020":24,"AminerCitationCount_06-2020":34,"XploreCitationCount - 2020-01":17,"PubsCited":45,"Award":"HM","image":"6102443-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"How locus of control influences compatibility with visualization style","DOI":"10.1109/VAST.2011.6102445","Link":"http://dx.doi.org/10.1109/VAST.2011.6102445","FirstPage":81,"LastPage":90,"PaperType":"C","Abstract":"Existing research suggests that individual personality differences are correlated with a user's speed and accuracy in solving problems with different types of complex visualization systems. In this paper, we extend this research by isolating factors in personality traits as well as in the visualizations that could have contributed to the observed correlation. We focus on a personality trait known as “locus of control,” which represents a person's tendency to see themselves as controlled by or in control of external events. To isolate variables of the visualization design, we control extraneous factors such as color, interaction, and labeling, and specifically focus on the overall layout style of the visualizations. We conduct a user study with four visualizations that gradually shift from an indentation metaphor to a containment metaphor and compare the participants' speed, accuracy, and preference with their locus of control. Our findings demonstrate that there is indeed a correlation between the two: participants with an internal locus of control perform more poorly with visualizations that employ a containment metaphor, while those with an external locus of control perform well with such visualizations. We discuss a possible explanation for this relationship based in cognitive psychology and propose that these results can be used to better understand how people use visualizations and how to adapt visual analytics design to an individual user's needs.","AuthorNames-Deduped":"Caroline Ziemkiewicz;R. Jordan Crouser;Ashley Rye Yauilla;Sara L. Su;William Ribarsky;Remco Chang","AuthorNames":"Caroline Ziemkiewicz;R. Jordan Crouser;Ashley Rye Yauilla;Sara L. Su;William Ribarsky;Remco Chang","AuthorAffiliation":"Brown University, USA;Tufts University, USA;Winthrop University, USA;Tufts University, USA;UNC Charlotte, USA;Tufts University, USA","InternalReferences":"10.1109/VAST.2010.5653587;10.1109/TVCG.2008.171;10.1109/TVCG.2008.121","AuthorKeywords":null,"AminerCitationCount_02-2020":30,"AminerCitationCount_06-2020":36,"XploreCitationCount - 2020-01":17,"PubsCited":31,"Award":"HM","image":"6102445-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2011,"Title":"Obvious: A meta-toolkit to encapsulate information visualization toolkits - One toolkit to bind them all","DOI":"10.1109/VAST.2011.6102446","Link":"http://dx.doi.org/10.1109/VAST.2011.6102446","FirstPage":91,"LastPage":100,"PaperType":"C","Abstract":"This article describes “Obvious”: a meta-toolkit that abstracts and encapsulates information visualization toolkits implemented in the Java language. It intends to unify their use and postpone the choice of which concrete toolkit(s) to use later-on in the development of visual analytics applications. We also report on the lessons we have learned when wrapping popular toolkits with Obvious, namely Prefuse, the InfoVis Toolkit, partly Improvise, JUNG and other data management libraries. We show several examples on the uses of Obvious, how the different toolkits can be combined, for instance sharing their data models. We also show how Weka and Rapid-Miner, two popular machine-learning toolkits, have been wrapped with Obvious and can be used directly with all the other wrapped toolkits. We expect Obvious to start a co-evolution process: Obvious is meant to evolve when more components of Information Visualization systems will become consensual. It is also designed to help information visualization systems adhere to the best practices to provide a higher level of interoperability and leverage the domain of visual analytics.","AuthorNames-Deduped":"Jean-Daniel Fekete;Pierre-Luc Hemery;Thomas Baudel;Jo Wood","AuthorNames":"Jean-Daniel Fekete;Pierre-Luc Hémery;Thomas Baudel;Jo Wood","AuthorAffiliation":"INRIA, France;INRIA, France;IBM, USA;City University, London, USA","InternalReferences":"10.1109/INFVIS.2004.12;10.1109/TVCG.2009.152;10.1109/TVCG.2009.174;10.1109/TVCG.2006.178;10.1109/TVCG.2010.159;10.1109/INFVIS.2004.64;10.1109/INFVIS.1998.729560;10.1109/INFVIS.2002.1173148","AuthorKeywords":null,"AminerCitationCount_02-2020":12,"AminerCitationCount_06-2020":13,"XploreCitationCount - 2020-01":6,"PubsCited":42,"Award":null,"image":"6102446-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Supporting effective common ground construction in Asynchronous Collaborative Visual Analytics","DOI":"10.1109/VAST.2011.6102447","Link":"http://dx.doi.org/10.1109/VAST.2011.6102447","FirstPage":101,"LastPage":110,"PaperType":"C","Abstract":"Asynchronous Collaborative Visual Analytics (ACVA) leverages group sensemaking by releasing the constraints on when, where, and who works collaboratively. A significant task to be addressed before ACVA can reach its full potential is effective common ground construction, namely the process in which users evaluate insights from individual work to develop a shared understanding of insights and collectively pool them. This is challenging due to the lack of instant communication and scale of collaboration in ACVA. We propose a novel visual analytics approach that automatically gathers, organizes, and summarizes insights to form common ground with reduced human effort. The rich set of visualization and interaction techniques provided in our approach allows users to effectively and flexibly control the common ground construction and review, explore, and compare insights in detail. A working prototype of the approach has been implemented. We have conducted a case study and a user study to demonstrate its effectiveness.","AuthorNames-Deduped":"Yang Chen;Jamal Alsakran;Scott Barlowe;Jing Yang 0001;Ye Zhao","AuthorNames":"Yang Chen;Jamal Alsakran;Scott Barlowe;Jing Yang;Ye Zhao","AuthorAffiliation":"University of North Carolina at Charlotte, USA;Kent State University, USA;University of North Carolina at Charlotte, USA;University of North Carolina at Charlotte, USA;Kent State University, USA","InternalReferences":"10.1109/TVCG.2007.70541;10.1109/VAST.2009.5333023;10.1109/TVCG.2007.70577;10.1109/VAST.2010.5652879;10.1109/VAST.2008.4677358;10.1109/VAST.2010.5652932;10.1109/VAST.2007.4389011;10.1109/VAST.2008.4677365;10.1109/TVCG.2006.166;10.1109/VAST.2010.5652885","AuthorKeywords":"Visual analytics, asynchronous collaboration, insight, multidimensional visualization","AminerCitationCount_02-2020":10,"AminerCitationCount_06-2020":12,"XploreCitationCount - 2020-01":10,"PubsCited":30,"Award":null,"image":"6102447-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2011,"Title":"Guiding feature subset selection with an interactive visualization","DOI":"10.1109/VAST.2011.6102448","Link":"http://dx.doi.org/10.1109/VAST.2011.6102448","FirstPage":111,"LastPage":120,"PaperType":"C","Abstract":"We propose a method for the semi-automated refinement of the results of feature subset selection algorithms. Feature subset selection is a preliminary step in data analysis which identifies the most useful subset of features (columns) in a data table. So-called filter techniques use statistical ranking measures for the correlation of features. Usually a measure is applied to all entities (rows) of a data table. However, the differing contributions of subsets of data entities are masked by statistical aggregation. Feature and entity subset selection are, thus, highly interdependent. Due to the difficulty in visualizing a high-dimensional data table, most feature subset selection algorithms are applied as a black box at the outset of an analysis. Our visualization technique, SmartStripes, allows users to step into the feature subset selection process. It enables the investigation of dependencies and interdependencies between different feature and entity subsets. A user may even choose to control the iterations manually, taking into account the ranking measures, the contributions of different entity subsets, as well as the semantics of the features.","AuthorNames-Deduped":"Thorsten May;Andreas Bannach;James Davey;Tobias Ruppert;Jörn Kohlhammer","AuthorNames":"Thorsten May;Andreas Bannach;James Davey;Tobias Ruppert;Jörn Kohlhammer","AuthorAffiliation":"Fraunhofer Institute for Computer Graphics Research, Darmstadt, Germany;Fraunhofer Institute for Computer Graphics Research, Darmstadt, Germany;Fraunhofer Institute for Computer Graphics Research, Darmstadt, Germany;Fraunhofer Institute for Computer Graphics Research, Darmstadt, Germany;Fraunhofer Institute for Computer Graphics Research, Darmstadt, Germany","InternalReferences":"10.1109/VAST.2010.5652392;10.1109/INFVIS.2003.1249006;10.1109/TVCG.2009.153;10.1109/TVCG.2008.153","AuthorKeywords":null,"AminerCitationCount_02-2020":36,"AminerCitationCount_06-2020":48,"XploreCitationCount - 2020-01":29,"PubsCited":23,"Award":null,"image":"6102448-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Observation-level interaction with statistical models for visual analytics","DOI":"10.1109/VAST.2011.6102449","Link":"http://dx.doi.org/10.1109/VAST.2011.6102449","FirstPage":121,"LastPage":130,"PaperType":"C","Abstract":"In visual analytics, sensemaking is facilitated through interactive visual exploration of data. Throughout this dynamic process, users combine their domain knowledge with the dataset to create insight. Therefore, visual analytic tools exist that aid sensemaking by providing various interaction techniques that focus on allowing users to change the visual representation through adjusting parameters of the underlying statistical model. However, we postulate that the process of sensemaking is not focused on a series of parameter adjustments, but instead, a series of perceived connections and patterns within the data. Thus, how can models for visual analytic tools be designed, so that users can express their reasoning on observations (the data), instead of directly on the model or tunable parameters? Observation level (and thus “observation”) in this paper refers to the data points within a visualization. In this paper, we explore two possible observation-level interactions, namely exploratory and expressive, within the context of three statistical methods, Probabilistic Principal Component Analysis (PPCA), Multidimensional Scaling (MDS), and Generative Topographic Mapping (GTM). We discuss the importance of these two types of observation level interactions, in terms of how they occur within the sensemaking process. Further, we present use cases for GTM, MDS, and PPCA, illustrating how observation level interaction can be incorporated into visual analytic tools.","AuthorNames-Deduped":"Alex Endert;Chao Han;Dipayan Maiti;Leanna House;Scotland Leman;Chris North","AuthorNames":"Alex Endert;Chao Han;Dipayan Maiti;Leanna House;Scotland Leman;Chris North","AuthorAffiliation":"Department of Computer Science, Virginia Tech, USA;Department of Statistics, Virginia Tech, USA;Department of Statistics, Virginia Tech, USA;Department of Statistics, Virginia Tech, USA;Department of Statistics, Virginia Tech, USA;Department of Computer Science, Virginia Tech, USA","InternalReferences":null,"AuthorKeywords":"observation-level interaction, visual analytics, statistical models","AminerCitationCount_02-2020":74,"AminerCitationCount_06-2020":88,"XploreCitationCount - 2020-01":61,"PubsCited":34,"Award":null,"image":"6102449-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Pointwise local pattern exploration for sensitivity analysis","DOI":"10.1109/VAST.2011.6102450","Link":"http://dx.doi.org/10.1109/VAST.2011.6102450","FirstPage":131,"LastPage":140,"PaperType":"C","Abstract":"Sensitivity analysis is a powerful method for discovering the significant factors that contribute to targets and understanding the interaction between variables in multivariate datasets. A number of sensitivity analysis methods fall into the class of local analysis, in which the sensitivity is defined as the partial derivatives of a target variable with respect to a group of independent variables. Incorporating sensitivity analysis in visual analytic tools is essential for multivariate phenomena analysis. However, most current multivariate visualization techniques do not allow users to explore local patterns individually for understanding the sensitivity from a pointwise view. In this paper, we present a novel pointwise local pattern exploration system for visual sensitivity analysis. Using this system, analysts are able to explore local patterns and the sensitivity at individual data points, which reveals the relationships between a focal point and its neighbors. During exploration, users are able to interactively change the derivative coefficients to perform sensitivity analysis based on different requirements as well as their domain knowledge. Each local pattern is assigned an outlier factor, so that users can quickly identify anomalous local patterns that do not conform with the global pattern. Users can also compare the local pattern with the global pattern both visually and statistically. Finally, the local pattern is integrated into the original attribute space using color mapping and jittering, which reveals the distribution of the partial derivatives. Case studies with real datasets are used to investigate the effectiveness of the visualizations and interactions.","AuthorNames-Deduped":"Zhenyu Guo;Matthew O. Ward;Elke A. Rundensteiner;Carolina Ruiz","AuthorNames":"Zhenyu Guo;Matthew O. Ward;Elke A. Rundensteiner;Carolina Ruiz","AuthorAffiliation":"Computer Science Department, Worcester Polytechnic Institute, USA;Computer Science Department, Worcester Polytechnic Institute, USA;Computer Science Department, Worcester Polytechnic Institute, USA;Computer Science Department, Worcester Polytechnic Institute, USA","InternalReferences":"10.1109/VISUAL.2005.1532821;10.1109/VAST.2008.4677368;10.1109/VAST.2010.5652460;10.1109/VAST.2009.5332611;10.1109/INFVIS.2004.71;10.1109/VAST.2009.5333431","AuthorKeywords":"Knowledge discovery, sensitivity analysis, local pattern visualizations","AminerCitationCount_02-2020":11,"AminerCitationCount_06-2020":15,"XploreCitationCount - 2020-01":9,"PubsCited":26,"Award":null,"image":"6102450-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Interactive decision making using dissimilarity to visually represented prototypes","DOI":"10.1109/VAST.2011.6102451","Link":"http://dx.doi.org/10.1109/VAST.2011.6102451","FirstPage":141,"LastPage":149,"PaperType":"C","Abstract":"To make informed decisions, an expert has to reason with multi-dimensional, heterogeneous data and analysis results of these. Items in such datasets are typically represented by features. However, as argued in cognitive science, features do not yield an optimal space for human reasoning. In fact, humans tend to organize complex information in terms of prototypes or known cases rather than in absolute terms. When confronted with unknown data items, humans assess them in terms of similarity to these prototypical elements. Interestingly, an analogues similarity-to-prototype approach, where prototypes are taken from the data, has been successfully applied in machine learning. Combining such a machine learning approach with human prototypical reasoning in a Visual Analytics context requires to integrate similarity-based classification with interactive visualizations. To that end, the data prototypes should be visually represented to trigger direct associations to cases familiar to the domain experts. In this paper, we propose a set of highly interactive visualizations to explore data and classification results in terms of dissimilarities to visually represented prototypes. We argue that this approach not only supports human reasoning processes, but is also suitable to enhance understanding of heterogeneous data. The proposed framework is applied to a risk assessment case study in Forensic Psychiatry.","AuthorNames-Deduped":"Gosia Migut;Jan C. van Gemert;Marcel Worring","AuthorNames":"M.A. Migut;J.C. van Gemert;M. Worring","AuthorAffiliation":"Intelligent Systems Lab Amsterdam, University of Amsterdam, The Netherlands;Expertise Center Forensic Psychiatry, Utrecht, The Netherlands;Intelligent Systems Lab Amsterdam, University of Amsterdam, The Netherlands","InternalReferences":"10.1109/TVCG.2007.70515;10.1109/TVCG.2009.174;10.1109/TVCG.2009.199;10.1109/VAST.2010.5652398;10.1109/VISUAL.1994.346302;10.1109/INFVIS.1998.729559;10.1109/INFVIS.2000.885086","AuthorKeywords":"dissimilarity based classication, dissimilarity based visualization, prototypes, interactive visualization, visual analytics","AminerCitationCount_02-2020":7,"AminerCitationCount_06-2020":11,"XploreCitationCount - 2020-01":5,"PubsCited":35,"Award":null,"image":"6102451-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"BaobabView: Interactive construction and analysis of decision trees","DOI":"10.1109/VAST.2011.6102453","Link":"http://dx.doi.org/10.1109/VAST.2011.6102453","FirstPage":151,"LastPage":160,"PaperType":"C","Abstract":"We present a system for the interactive construction and analysis of decision trees that enables domain experts to bring in domain specific knowledge. We identify different user tasks and corresponding requirements, and develop a system incorporating a tight integration of visualization, interaction and algorithmic support. Domain experts are supported in growing, pruning, optimizing and analysing decision trees. Furthermore, we present a scalable decision tree visualization optimized for exploration. We show the effectiveness of our approach by applying the methods to two use cases. The first case illustrates the advantages of interactive construction, the second case demonstrates the effectiveness of analysis of decision trees and exploration of the structure of the data.","AuthorNames-Deduped":"Stef van den Elzen;Jarke J. van Wijk","AuthorNames":"Stef van den Elzen;Jarke J. van Wijk","AuthorAffiliation":"Eindhoven University of Technology, the Netherlands;Eindhoven University of Technology, the Netherlands","InternalReferences":"10.1109/TVCG.2008.166;10.1109/INFVIS.2001.963292;10.1109/INFVIS.2001.963290","AuthorKeywords":null,"AminerCitationCount_02-2020":42,"AminerCitationCount_06-2020":82,"XploreCitationCount - 2020-01":42,"PubsCited":44,"Award":null,"image":"6102453-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"From movement tracks through events to places: Extracting and characterizing significant places from mobility data","DOI":"10.1109/VAST.2011.6102454","Link":"http://dx.doi.org/10.1109/VAST.2011.6102454","FirstPage":161,"LastPage":170,"PaperType":"C","Abstract":"We propose a visual analytics procedure for analyzing movement data, i.e., recorded tracks of moving objects. It is oriented to a class of problems where it is required to determine significant places on the basis of certain types of events occurring repeatedly in movement data. The procedure consists of four major steps: (1) event extraction from trajectories; (2) event clustering and extraction of relevant places; (3) spatio-temporal aggregation of events or trajectories; (4) analysis of the aggregated data. All steps are scalable with respect to the amount of the data under analysis. We demonstrate the use of the procedure by example of two real-world problems requiring analysis at different spatial scales.","AuthorNames-Deduped":"Gennady L. Andrienko;Natalia V. Andrienko;Christophe Hurter;Salvatore Rinzivillo;Stefan Wrobel","AuthorNames":"Gennady Andrienko;Natalia Andrienko;Christophe Hurter;Salvatore Rinzivillo;Stefan Wrobel","AuthorAffiliation":"Fraunhofer Institute IAIS (Intelligent Analysis and Information Systems) and University of Bonn, Germany;Fraunhofer Institute IAIS (Intelligent Analysis and Information Systems) and University of Bonn, Germany;DGAC/DTI R&amp;D, ENAC and the University of Toulouse, France;KDDLab, ISTI - CNR, Pisa, Italy;Fraunhofer Institute IAIS (Intelligent Analysis and Information Systems) and University of Bonn, Germany","InternalReferences":"10.1109/VAST.2009.5332593;10.1109/TVCG.2009.145","AuthorKeywords":"movement, trajectories, spatio-temporal data, spatial events, spatial clustering, spatio-temporal clustering","AminerCitationCount_02-2020":70,"AminerCitationCount_06-2020":80,"XploreCitationCount - 2020-01":50,"PubsCited":31,"Award":"BP","image":"6102454-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Visual analysis of route diversity","DOI":"10.1109/VAST.2011.6102455","Link":"http://dx.doi.org/10.1109/VAST.2011.6102455","FirstPage":171,"LastPage":180,"PaperType":"C","Abstract":"Route suggestion is an important feature of GPS navigation systems. Recently, Microsoft T-drive has been enabled to suggest routes chosen by experienced taxi drivers for given source/destination pairs in given time periods, which often take less time than the routes calculated according to distance. However, in real environments, taxi drivers may use different routes to reach the same destination, which we call route diversity. In this paper we first propose a trajectory visualization method that examines the regions where the diversity exists and then develop several novel visualization techniques to display the high dimensional attributes and statistics associated with different routes to help users analyze diversity patterns. Our techniques have been applied to the real trajectory data of thousands of taxis and some interesting findings about route diversity have been obtained. We further demonstrate that our system can be used not only to suggest better routes for drivers but also to analyze traffic bottlenecks for transportation management.","AuthorNames-Deduped":"He Liu;Yuan Gao;Lu Lu;Siyuan Liu;Huamin Qu;Lionel M. Ni","AuthorNames":"He Liu;Yuan Gao;Lu Lu;Siyuan Liu;Huamin Qu;Lionel M. Ni","AuthorAffiliation":"The Hong Kong University of Science and Technology, China;The Hong Kong University of Science and Technology, China;The Hong Kong University of Science and Technology, China;The Hong Kong University of Science and Technology, China;The Hong Kong University of Science and Technology, China;The Hong Kong University of Science and Technology, China","InternalReferences":"10.1109/INFVIS.2004.27;10.1109/VAST.2008.4677356;10.1109/TVCG.2008.149;10.1109/TVCG.2007.70570;10.1109/TVCG.2007.70574;10.1109/TVCG.2006.202;10.1109/VAST.2009.5332593;10.1109/TVCG.2007.70561;10.1109/TVCG.2009.145;10.1109/TVCG.2010.180","AuthorKeywords":null,"AminerCitationCount_02-2020":55,"AminerCitationCount_06-2020":61,"XploreCitationCount - 2020-01":52,"PubsCited":30,"Award":null,"image":"6102455-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"SensePlace2: GeoTwitter analytics support for situational awareness","DOI":"10.1109/VAST.2011.6102456","Link":"http://dx.doi.org/10.1109/VAST.2011.6102456","FirstPage":181,"LastPage":190,"PaperType":"C","Abstract":"Geographically-grounded situational awareness (SA) is critical to crisis management and is essential in many other decision making domains that range from infectious disease monitoring, through regional planning, to political campaigning. Social media are becoming an important information input to support situational assessment (to produce awareness) in all domains. Here, we present a geovisual analytics approach to supporting SA for crisis events using one source of social media, Twitter. Specifically, we focus on leveraging explicit and implicit geographic information for tweets, on developing place-time-theme indexing schemes that support overview+detail methods and that scale analytical capabilities to relatively large tweet volumes, and on providing visual interface methods to enable understanding of place, time, and theme components of evolving situations. Our approach is user-centered, using scenario-based design methods that include formal scenarios to guide design and validate implementation as well as a systematic claims analysis to justify design choices and provide a framework for future testing. The work is informed by a structured survey of practitioners and the end product of Phase-I development is demonstrated / validated through implementation in SensePlace2, a map-based, web application initially focused on tweets but extensible to other media.","AuthorNames-Deduped":"Alan M. MacEachren;Anuj R. Jaiswal;Anthony C. Robinson;Scott Pezanowski;Alexander Savelyev;Prasenjit Mitra;Xiao Zhang;Justine I. Blanford","AuthorNames":"Alan M. MacEachren;Anuj Jaiswal;Anthony C. Robinson;Scott Pezanowski;Alexander Savelyev;Prasenjit Mitra;Xiao Zhang;Justine Blanford","AuthorAffiliation":"GeoVISTA Center, The Pennsylvania State University, USA;GeoVISTA Center, The Pennsylvania State University, USA;GeoVISTA Center, The Pennsylvania State University, USA;GeoVISTA Center, The Pennsylvania State University, USA;GeoVISTA Center, The Pennsylvania State University, USA;GeoVISTA Center, The Pennsylvania State University, USA;GeoVISTA Center, The Pennsylvania State University, USA;GeoVISTA Center, The Pennsylvania State University, USA","InternalReferences":"10.1109/VAST.2010.5652478;10.1109/VAST.2007.4388994;10.1109/TVCG.2010.129;10.1109/INFVIS.2005.1532134;10.1109/VAST.2010.5652922","AuthorKeywords":"social media analytics, scenario-based design, geovisualization, situational awareness, text analytics, crisis management, spatio-temporal analysis ","AminerCitationCount_02-2020":171,"AminerCitationCount_06-2020":224,"XploreCitationCount - 2020-01":141,"PubsCited":37,"Award":null,"image":"6102456-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2011,"Title":"Visual analytics decision support environment for epidemic modeling and response evaluation","DOI":"10.1109/VAST.2011.6102457","Link":"http://dx.doi.org/10.1109/VAST.2011.6102457","FirstPage":191,"LastPage":200,"PaperType":"C","Abstract":"In modeling infectious diseases, scientists are studying the mechanisms by which diseases spread, predicting the future course of the outbreak, and evaluating strategies applied to control an epidemic. While recent work has focused on accurately modeling disease spread, less work has been performed in developing interactive decision support tools for analyzing the future course of the outbreak and evaluating potential disease mitigation strategies. The absence of such tools makes it difficult for researchers, analysts and public health officials to evaluate response measures within outbreak scenarios. As such, our research focuses on the development of an interactive decision support environment in which users can explore epidemic models and their impact. This environment provides a spatiotemporal view where users can interactively utilize mitigative response measures and observe the impact of their decision over time. Our system also provides users with a linked decision history visualization and navigation tool that support the simultaneous comparison of mortality and infection rates corresponding to different response measures at different points in time.","AuthorNames-Deduped":"Shehzad Afzal;Ross Maciejewski;David S. Ebert","AuthorNames":"Shehzad Afzal;Ross Maciejewski;David S. Ebert","AuthorAffiliation":"Purdue University Visualization and Analytics Center, USA;Purdue University Visualization and Analytics Center, USA;Purdue University Visualization and Analytics Center, USA","InternalReferences":"10.1109/TVCG.2008.137;10.1109/TVCG.2007.70594;10.1109/VISUAL.1993.398857;10.1109/VAST.2009.5333020;10.1109/TVCG.2010.223;10.1109/TVCG.2010.190;10.1109/TVCG.2010.206;10.1109/TVCG.2009.187;10.1109/TVCG.2010.171;10.1109/VAST.2006.261450;10.1109/INFVIS.2000.885086","AuthorKeywords":null,"AminerCitationCount_02-2020":40,"AminerCitationCount_06-2020":47,"XploreCitationCount - 2020-01":33,"PubsCited":30,"Award":null,"image":"6102457-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"SAVE: Sensor anomaly visualization engine","DOI":"10.1109/VAST.2011.6102458","Link":"http://dx.doi.org/10.1109/VAST.2011.6102458","FirstPage":201,"LastPage":210,"PaperType":"C","Abstract":"Diagnosing a large-scale sensor network is a crucial but challenging task. Particular challenges include the resource and bandwidth constraints on sensor nodes, the spatiotemporally dynamic network behaviors, and the lack of accurate models to understand such behaviors in a hostile environment. In this paper, we present the Sensor Anomaly Visualization Engine (SAVE), a system that fully leverages the power of both visualization and anomaly detection analytics to guide the user to quickly and accurately diagnose sensor network failures and faults. SAVE combines customized visualizations over separate sensor data facets as multiple coordinated views. Temporal expansion model, correlation graph and dynamic projection views are proposed to effectively interpret the topological, correlational and dimensional sensor data dynamics and their anomalies. Through a case study with real-world sensor network system and administrators, we demonstrate that SAVE is able to help better locate the system problem and further identify the root cause of major sensor network failure scenarios.","AuthorNames-Deduped":"Lei Shi 0002;Qi Liao;Yuan He;Rui Li;Aaron Striegel;Zhong Su","AuthorNames":"Lei Shi;Qi Liao;Yuan He;Rui Li;Aaron Striegel;Zhong Su","AuthorAffiliation":"IBM Research - China;Computer Science and Engineering Department, University of Notre Dame, USA;Computer Science and Engineering Department, Hong Kong University of Science and Technology, HK;Computer Science and Technology Department, Xi'an Jiao Tong University, China;Computer Science and Engineering Department, University of Notre Dame, USA;IBM Research - China","InternalReferences":"10.1109/TVCG.2009.182;10.1109/VAST.2009.5333880;10.1109/INFVIS.2005.1532126;10.1109/INFVIS.2004.1;10.1109/VAST.2010.5652910","AuthorKeywords":null,"AminerCitationCount_02-2020":16,"AminerCitationCount_06-2020":19,"XploreCitationCount - 2020-01":17,"PubsCited":30,"Award":null,"image":"6102458-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2011,"Title":"A visual navigation system for querying neural stem cell imaging data","DOI":"10.1109/VAST.2011.6102459","Link":"http://dx.doi.org/10.1109/VAST.2011.6102459","FirstPage":211,"LastPage":220,"PaperType":"C","Abstract":"Cellular biology deals with studying the behavior of cells. Current time-lapse imaging microscopes help us capture the progress of experiments at intervals that allow for understanding of the dynamic and kinematic behavior of the cells. On the other hand, these devices generate such massive amounts of data (250GB of data per experiment) that manual sieving of data to identify interesting patterns becomes virtually impossible. In this paper we propose an end-to-end system to analyze time-lapse images of the cultures of human neural stem cells (hNSC), that includes an image processing system to analyze the images to extract all the relevant geometric and statistical features within and between images, a database management system to manage and handle queries on the data, a visual analytic system to navigate through the data, and a visual query system to explore different relationships and correlations between the parameters. In each stage of the pipeline we make novel algorithmic and conceptual contributions, and the entire system design is motivated by many different yet unanswered exploratory questions pursued by our neurobiologist collaborators. With a few examples we show how such abstract biological queries can be analyzed and answered by our system.","AuthorNames-Deduped":"Ishwar Kulkarni;Shanaz Y. Mistry;Brian Cummings;Meenakshisundaram Gopi","AuthorNames":"Ishwar Kulkarni;Shanaz Y. Mistry;Brian Cummings;M. Gopi","AuthorAffiliation":"Department of Computer Science, University of California, Irvine, USA;Department of Computer Science, University of California, Irvine, USA;Department of Anatomy and Neurobiology, University of California, Irvine, USA;Department of Computer Science, University of California, Irvine, USA","InternalReferences":"10.1109/TVCG.2009.121;10.1109/VAST.2009.5333895","AuthorKeywords":"Neuroscience, stem cell segmentation, tracking, cell imaging, data management, visual analytics, navigation, exploration, query processing","AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":0,"PubsCited":23,"Award":null,"image":"6102459-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"A visual analytics process for maritime resource allocation and risk assessment","DOI":"10.1109/VAST.2011.6102460","Link":"http://dx.doi.org/10.1109/VAST.2011.6102460","FirstPage":221,"LastPage":230,"PaperType":"C","Abstract":"In this paper, we present our collaborative work with the U.S. Coast Guard's Ninth District and Atlantic Area Commands where we developed a visual analytics system to analyze historic response operations and assess the potential risks in the maritime environment associated with the hypothetical allocation of Coast Guard resources. The system includes linked views and interactive displays that enable the analysis of trends, patterns and anomalies among the U.S. Coast Guard search and rescue (SAR) operations and their associated sorties. Our system allows users to determine the potential change in risks associated with closing certain stations in terms of response time, potential lives and property lost and provides optimal direction as to the nearest available station. We provide maritime risk assessment tools that allow analysts to explore Coast Guard coverage for SAR operations and identify regions of high risk. The system also enables a thorough assessment of all SAR operations conducted by each Coast Guard station in the Great Lakes region. Our system demonstrates the effectiveness of visual analytics in analyzing risk within the maritime domain and is currently being used by analysts at the Coast Guard Atlantic Area.","AuthorNames-Deduped":"Abish Malik;Ross Maciejewski;Ben Maule;David S. Ebert","AuthorNames":"Abish Malik;Ross Maciejewski;Ben Maule;David S. Ebert","AuthorAffiliation":"Purdue University Visualization and Analytics Center (PURVAC), USA;Purdue University Visualization and Analytics Center (PURVAC), USA;United States Coast Guard, USA;Purdue University Visualization and Analytics Center (PURVAC), USA","InternalReferences":"10.1109/VISUAL.1993.398870;10.1109/VAST.2010.5652398;10.1109/VAST.2009.5333920;10.1109/VAST.2008.4677363;10.1109/INFVIS.1999.801851;10.1109/VAST.2007.4389006;10.1109/VAST.2009.5332611","AuthorKeywords":"Visual analytics, risk assessment, Coast Guard","AminerCitationCount_02-2020":19,"AminerCitationCount_06-2020":28,"XploreCitationCount - 2020-01":19,"PubsCited":34,"Award":null,"image":"6102460-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2011,"Title":"ParallelTopics: A probabilistic approach to exploring document collections","DOI":"10.1109/VAST.2011.6102461","Link":"http://dx.doi.org/10.1109/VAST.2011.6102461","FirstPage":231,"LastPage":240,"PaperType":"C","Abstract":"Scalable and effective analysis of large text corpora remains a challenging problem as our ability to collect textual data continues to increase at an exponential rate. To help users make sense of large text corpora, we present a novel visual analytics system, Parallel-Topics, which integrates a state-of-the-art probabilistic topic model Latent Dirichlet Allocation (LDA) with interactive visualization. To describe a corpus of documents, ParallelTopics first extracts a set of semantically meaningful topics using LDA. Unlike most traditional clustering techniques in which a document is assigned to a specific cluster, the LDA model accounts for different topical aspects of each individual document. This permits effective full text analysis of larger documents that may contain multiple topics. To highlight this property of the model, ParallelTopics utilizes the parallel coordinate metaphor to present the probabilistic distribution of a document across topics. Such representation allows the users to discover single-topic vs. multi-topic documents and the relative importance of each topic to a document of interest. In addition, since most text corpora are inherently temporal, ParallelTopics also depicts the topic evolution over time. We have applied ParallelTopics to exploring and analyzing several text corpora, including the scientific proposals awarded by the National Science Foundation and the publications in the VAST community over the years. To demonstrate the efficacy of ParallelTopics, we conducted several expert evaluations, the results of which are reported in this paper.","AuthorNames-Deduped":"Wenwen Dou;Xiaoyu Wang;Remco Chang;William Ribarsky","AuthorNames":"Wenwen Dou;Xiaoyu Wang;Remco Chang;William Ribarsky","AuthorAffiliation":"UNC Charlotte, USA;UNC Charlotte, USA;Tufts University, USA;UNC Charlotte, USA","InternalReferences":"10.1109/VAST.2010.5652931;10.1109/VAST.2009.5333428;10.1109/TVCG.2010.184;10.1109/VAST.2010.5652940;10.1109/TVCG.2009.140;10.1109/INFVIS.2000.885098","AuthorKeywords":null,"AminerCitationCount_02-2020":59,"AminerCitationCount_06-2020":74,"XploreCitationCount - 2020-01":51,"PubsCited":32,"Award":null,"image":"6102461-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2011,"Title":"Analysis of large digital collections with interactive visualization","DOI":"10.1109/VAST.2011.6102462","Link":"http://dx.doi.org/10.1109/VAST.2011.6102462","FirstPage":241,"LastPage":250,"PaperType":"C","Abstract":"To make decisions about the long-term preservation and access of large digital collections, archivists gather information such as the collections' contents, their organizational structure, and their file format composition. To date, the process of analyzing a collection - from data gathering to exploratory analysis and final conclusions - has largely been conducted using pen and paper methods. To help archivists analyze large-scale digital collections for archival purposes, we developed an interactive visual analytics application. The application narrows down different kinds of information about the collection, and presents them as meaningful data views. Multiple views and analysis features can be linked or unlinked on demand to enable researchers to compare and contrast different analyses, and to identify trends. We describe and present two user scenarios to show how the application allowed archivists to learn about a collection with accuracy, facilitated decision-making, and helped them arrive at conclusions.","AuthorNames-Deduped":"Weijia Xu;Maria Esteva;Suyog Dott Jain;Varun Jain","AuthorNames":"Weijia Xu;Maria Esteva;Suyog Dutt Jain;Varun Jain","AuthorAffiliation":"The University of Texas at Austin, USA;The University of Texas at Austin, USA;The University of Texas at Austin, USA;The University of Texas at Austin, USA","InternalReferences":"10.1109/INFVIS.2000.885091;10.1109/TVCG.2008.172;10.1109/TVCG.2009.176;10.1109/VAST.2007.4389006;10.1109/INFVIS.2004.64;10.1109/VAST.2010.5652931;10.1109/INFVIS.1999.801860","AuthorKeywords":"Digital collections, archival analysis, visual anaytics, data curation","AminerCitationCount_02-2020":7,"AminerCitationCount_06-2020":9,"XploreCitationCount - 2020-01":6,"PubsCited":34,"Award":null,"image":"6102462-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"A two-stage framework for designing visual analytics system in organizational environments","DOI":"10.1109/VAST.2011.6102463","Link":"http://dx.doi.org/10.1109/VAST.2011.6102463","FirstPage":251,"LastPage":260,"PaperType":"C","Abstract":"A perennially interesting research topic in the field of visual analytics is how to effectively develop systems that support organizational users' decision-making and reasoning processes. The problem is, however, most domain analytical practices generally vary from organization to organization. This leads to diverse designs of visual analytics systems in incorporating domain analytical processes, making it difficult to generalize the success from one domain to another. Exacerbating this problem is the dearth of general models of analytical workflows available to enable such timely and effective designs. To alleviate these problems, we present a two-stage framework for informing the design of a visual analytics system. This design framework builds upon and extends current practices pertaining to analytical workflow and focuses, in particular, on incorporating both general domain analysis processes as well as individual's analytical activities. We illustrate both stages and their design components through examples, and hope this framework will be useful for designing future visual analytics systems. We validate the soundness of our framework with two visual analytics systems, namely Entity Workspace [8] and PatViz [37].","AuthorNames-Deduped":"Xiaoyu Wang;Wenwen Dou;Thomas Butkiewicz;Eric A. Bier;William Ribarsky","AuthorNames":"Xiaoyu Wang;Wenwen Dou;Thomas Butkiewicz;Eric A. Bier;William Ribarsky","AuthorAffiliation":"UNC Charlotte, USA;UNC Charlotte, USA;University of New Hampshire, USA;Palo Alto Research Center, USA;UNC Charlotte, USA","InternalReferences":"10.1109/VAST.2008.4677362;10.1109/VAST.2009.5333020;10.1109/TVCG.2008.137;10.1109/VAST.2006.261416;10.1109/VAST.2007.4389009;10.1109/TVCG.2009.139;10.1109/VAST.2008.4677361;10.1109/TVCG.2009.111;10.1109/VISUAL.2005.1532781;10.1109/VAST.2008.4677352;10.1109/TVCG.2008.109;10.1109/TVCG.2007.70515;10.1109/VAST.2008.4677360;10.1109/VAST.2008.4677365;10.1109/VAST.2009.5333564","AuthorKeywords":"Design Theory, Visual Analytics, HCI","AminerCitationCount_02-2020":15,"AminerCitationCount_06-2020":18,"XploreCitationCount - 2020-01":7,"PubsCited":66,"Award":null,"image":"6102463-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Using random projections to identify class-separating variables in high-dimensional spaces","DOI":"10.1109/VAST.2011.6102465","Link":"http://dx.doi.org/10.1109/VAST.2011.6102465","FirstPage":263,"LastPage":264,"PaperType":"M","Abstract":"Projection Pursuit has been an effective method for finding interesting low-dimensional (usually 2D) projections in multidimensional spaces. Unfortunately, projection pursuit is not scalable to high-dimensional spaces. We introduce a novel method for approximating the results of projection pursuit to find class-separating views by using random projections. We build an analytic visualization platform based on this algorithm that is scalable to extremely large problems. Then, we discuss its extension to the recognition of other noteworthy configurations in high-dimensional spaces.","AuthorNames-Deduped":"Anushka Anand;Leland Wilkinson;Tommy Dang","AuthorNames":"Anushka Anand;Leland Wilkinson;Tuan Nhon Dang","AuthorAffiliation":"Department of Computer Science, University of Illinois at Chicago, USA;Department of Computer Science, University of Illinois at Chicago, USA;Department of Computer Science, University of Illinois at Chicago, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":1,"PubsCited":12,"Award":null,"image":"6102465-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Evaluation of large display interaction using smart phones","DOI":"10.1109/VAST.2011.6102466","Link":"http://dx.doi.org/10.1109/VAST.2011.6102466","FirstPage":265,"LastPage":266,"PaperType":"M","Abstract":"Visual analytics, “the science of analytical reasoning facilitated by visual interactive interfaces” [5], puts high demands on the applications visualization as well as interaction capabilities. Due to their size large high-resolution screens have become popular display devices, especially when used in collaborative data analysis scenarios. However, traditional interaction methods based on combinations of computer mice and keyboards often do not scale to the number of users or the size of the display. Modern smart phones featuring multi-modal input/output and considerable memory offer a way to address these issues. In the last couple of years they have become common everyday life gadgets. In this paper we conduct an extensive user study comparing the experience of test candidates when using traditional input devices and metaphors with the one when using new smart phone based techniques, like multi-modal drag and tilt. Candidates were asked to complete various interaction tasks relevant for most applications on a large, monitor-based, high-resolution tiled wall system. Our study evaluates both user performance and satisfaction, identifying strengths and weaknesses of the researched interaction methods in specific tasks. Results reveal good performance of users in certain tasks when using the new interaction techniques. Even first-time users were able to complete a task faster with the smart phone than with traditional devices.","AuthorNames-Deduped":"Jens Bauer;Sebastian Thelen;Achim Ebert","AuthorNames":"Jens Bauer;Sebastian Thelen;Achim Ebert","AuthorAffiliation":"Computer Graphics & HCI Lab, University of Kaiserslautern, Germany;Computer Graphics & HCI Lab, University of Kaiserslautern, Germany;Computer Graphics & HCI Lab, University of Kaiserslautern, Germany","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":0,"PubsCited":5,"Award":null,"image":"6102466-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Query-based coordinated multiple views with Feature Similarity Space for visual analysis of MRI repositories","DOI":"10.1109/VAST.2011.6102467","Link":"http://dx.doi.org/10.1109/VAST.2011.6102467","FirstPage":267,"LastPage":268,"PaperType":"M","Abstract":"It is a laborious process to quantify relationship patterns within a feature-rich archive. For example, understanding the degree of neuroanatomical similarity between the scanned subjects of a Magnetic Resonance Imaging (MRI) repository is a nontrivial task. In this work we present a Coordinated Multiple View (CMV) system for visually analyzing collections of feature-rich datasets. A query-based user interface operates on a feature-respective data scheme, and is geared towards domain experts that are non-specialists in informatics and analytics. We employ multi-dimensional scaling (MDS) to project feature surface representations into three-dimensions, where proximity in location is proportional to the feature similarity. Through query feedback and environment navigation, the user groups clusters that exhibit probable trends across feature and attribute. The system provides supervised classification methods for determining attribute classes within the user selected groups. Finally, using visual or analytical feature-wise exploration the user determines intra-group feature commonality.","AuthorNames-Deduped":"Ian Bowman;Shantanu H. Joshi;John D. Van Horn","AuthorNames":"Ian Bowman;Shantanu H. Joshi;John Darrell Van Horn","AuthorAffiliation":"Laboratory of Neuro Imaging, University of California Los Angeles, 90034, USA;Laboratory of Neuro Imaging, University of California Los Angeles, 90034, USA;Laboratory of Neuro Imaging, University of California Los Angeles, 90034, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":7,"AminerCitationCount_06-2020":7,"XploreCitationCount - 2020-01":5,"PubsCited":2,"Award":null,"image":"6102467-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Reasonable abstractions: Semantics for dynamic data visualization","DOI":"10.1109/VAST.2011.6102468","Link":"http://dx.doi.org/10.1109/VAST.2011.6102468","FirstPage":269,"LastPage":270,"PaperType":"M","Abstract":"Chi showed how to treat visualization programing models abstractly. This provided a firm theoretical basis for the data-state model of visualization. However, Chi's models did not look deeper into fine-grained program properties, such as execution semantics. We present conditionally deterministic and resource bounded semantics for the data flow model of visualization based on E-FRP. These semantics are used in the Stencil system to move between data state and data flow execution, build task-based parallelism, and build complex analysis chains for dynamic data. This initial work also shows promise for other complex operators, compilation techniques to enable efficient use of time and space, and mixing task and data parallelism.","AuthorNames-Deduped":"Joseph A. Cottam;Andrew Lumsdaine","AuthorNames":"Joseph A. Cottam;Andrew Lumsdaine","AuthorAffiliation":null,"InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":6,"Award":null,"image":"6102468-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Exploring agent-based simulations using temporal graphs","DOI":"10.1109/VAST.2011.6102469","Link":"http://dx.doi.org/10.1109/VAST.2011.6102469","FirstPage":271,"LastPage":272,"PaperType":"M","Abstract":"Agent-based simulation has become a key technique for modeling and simulating dynamic, complicated behaviors in social and behavioral sciences. Lacking the appropriate tools and support, it is difficult for social scientists to thoroughly analyze the results of these simulations. In this work, we capture the complex relationships between discrete simulation states by visualizing the data as a temporal graph. In collaboration with expert analysts, we identify two graph structures which capture important relationships between pivotal states in the simulation and their inevitable outcomes. Finally, we demonstrate the utility of these structures in the interactive analysis of a large-scale social science simulation of political power in present-day Thailand.","AuthorNames-Deduped":"R. Jordan Crouser;Jeremy G. Freeman;Remco Chang","AuthorNames":"R. Jordan Crouser;Jeremy G. Freeman;Remco Chang","AuthorAffiliation":"Tufts University, USA;Tufts University, USA;Tufts University, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":8,"Award":null,"image":"6102469-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Visual analytical approaches to evaluating uncertainty and bias in crowd sourced crisis information","DOI":"10.1109/VAST.2011.6102470","Link":"http://dx.doi.org/10.1109/VAST.2011.6102470","FirstPage":273,"LastPage":274,"PaperType":"M","Abstract":"Concerns about verification mean the humanitarian community are reluctant to use information collected during crisis events, even though such information could potentially enhance the response effort. Consequently, a program of research is presented that aims to evaluate the degree to which uncertainty and bias are found in public collections of incident reports gathered during crisis events. These datasets exemplify a class whose members have spatial and temporal attributes, are gathered from heterogeneous sources, and do not have readily available attribution information. An interactive software prototype, and existing software, are applied to a dataset related to the current armed conflict in Libya to identify `intrinsic' characteristics against which uncertainty and bias can be evaluated. Requirements on the prototype are identified, which in time will be expanded into full research objectives.","AuthorNames-Deduped":"Iain Dillingham;Jason Dykes;Jo Wood","AuthorNames":"Iain Dillingham;Jason Dykes;Jo Wood","AuthorAffiliation":"giCentre, School of Informatics, City University London, UK;giCentre, School of Informatics, City University London, UK;giCentre, School of Informatics, City University London, UK","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":0,"PubsCited":16,"Award":null,"image":"6102470-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"TreeVersity: Comparing tree structures by topology and node's attributes differences","DOI":"10.1109/VAST.2011.6102471","Link":"http://dx.doi.org/10.1109/VAST.2011.6102471","FirstPage":275,"LastPage":276,"PaperType":"M","Abstract":"It is common to classify data in hierarchies, they provide a comprehensible way of understanding big amounts of data. From budgets to organizational charts or even the stock market, trees are everywhere and people find them easy to use. However when analysts need to compare two versions of the same tree structure, or two related taxonomies, the task is not so easy. Much work has been done on this topic, but almost all of it has been restricted to either compare the trees by topology, or by the node attribute values. With this project we are proposing TreeVersity, a framework for comparing tree structures, both by structural changes and by differences in the node attributes. This paper is based on our previous work on comparing traffic agencies using LifeFlow [1, 2] and on a first prototype of TreeVersity.","AuthorNames-Deduped":"John Alexis Guerra Gómez;Audra Buck-Coleman;Catherine Plaisant;Ben Shneiderman","AuthorNames":"John Alexis Guerra Gómez;Audra Buck-Coleman;Catherine Plaisant;Ben Shneiderman","AuthorAffiliation":"HCIL & Department of Computer Science, University of Maryland, USA;Department of Art, University of Maryland, USA;HCIL & UMIACS, University of Maryland, USA;HCIL & Department of Computer Science, University of Maryland, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":7,"XploreCitationCount - 2020-01":0,"PubsCited":9,"Award":null,"image":"6102471-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2011,"Title":"Visual sentiment analysis on twitter data streams","DOI":"10.1109/VAST.2011.6102472","Link":"http://dx.doi.org/10.1109/VAST.2011.6102472","FirstPage":277,"LastPage":278,"PaperType":"M","Abstract":"Twitter currently receives about 190 million tweets (small text-based Web posts) a day, in which people share their comments regarding a wide range of topics. A large number of tweets include opinions about products and services. However, with Twitter being a relatively new phenomenon, these tweets are underutilized as a source for evaluating customer sentiment. To explore high-volume twitter data, we introduce three novel time-based visual sentiment analysis techniques: (1) topic-based sentiment analysis that extracts, maps, and measures customer opinions; (2) stream analysis that identifies interesting tweets based on their density, negativity, and influence characteristics; and (3) pixel cell-based sentiment calendars and high density geo maps that visualize large volumes of data in a single view. We applied these techniques to a variety of twitter data, (e.g., movies, amusement parks, and hotels) to show their distribution and patterns, and to identify influential opinions.","AuthorNames-Deduped":"Ming C. Hao;Christian Rohrdantz;Halldór Janetzko;Umeshwar Dayal;Daniel A. Keim;Lars-Erik Haug;Meichun Hsu","AuthorNames":"Ming Hao;Christian Rohrdantz;Halldór Janetzko;Umeshwar Dayal;Daniel A. Keim;Lars-Erik Haug;Mei-Chun Hsu","AuthorAffiliation":"Hewlett-Packard Labs, USA;Hewlett-Packard Labs, USA;Hewlett-Packard Labs, USA;Hewlett-Packard Labs, USA;Hewlett-Packard Labs, USA;University of Konstanz, Germany;University of Konstanz, Germany","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":35,"AminerCitationCount_06-2020":39,"XploreCitationCount - 2020-01":16,"PubsCited":3,"Award":null,"image":"6102472-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Analysts aren't machines: Inferring frustration through visualization interaction","DOI":"10.1109/VAST.2011.6102473","Link":"http://dx.doi.org/10.1109/VAST.2011.6102473","FirstPage":279,"LastPage":280,"PaperType":"M","Abstract":"Recent work in visual analytics has explored the extent to which information regarding analyst action and reasoning can be inferred from interaction. However, these methods typically rely on humans instead of automatic extraction techniques. Furthermore, there is little discussion regarding the role of user frustration when interacting with a visual interface. We demonstrate that automatic extraction of user frustration is possible given action-level visualization interaction logs. An experiment is described which collects data that accurately reflects user emotion transitions and corresponding interaction sequences. This data is then used in building HiddenMarkov Models (HMMs) which statistically connect interaction events with frustration. The capabilities of HMMs in predicting user frustration are tested using standard machine learning evaluation methods. The resulting classifier serves as a suitable predictor of user frustration that performs similarly across different users and datasets.","AuthorNames-Deduped":"Lane Harrison;Wenwen Dou;Aidong Lu;William Ribarsky;Xiaoyu Wang","AuthorNames":"Lane Harrison;Wenwen Dou;Aidong Lu;William Ribarsky;Xiaoyu Wang","AuthorAffiliation":"Computer Science, UNC-Charlotte, USA;Computer Science, UNC-Charlotte, USA;Computer Science, UNC-Charlotte, USA;Computer Science, UNC-Charlotte, USA;Computer Science, UNC-Charlotte, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":3,"PubsCited":7,"Award":null,"image":"6102473-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Automated measures for interpretable dimensionality reduction for visual classification: A user study","DOI":"10.1109/VAST.2011.6102474","Link":"http://dx.doi.org/10.1109/VAST.2011.6102474","FirstPage":281,"LastPage":282,"PaperType":"M","Abstract":"This paper studies the interpretability of transformations of labeled higher dimensional data into a 2D representation (scatterplots) for visual classification.&lt;sup&gt;1&lt;/sup&gt;In this context, the term interpretability has two components: the interpretability of the visualization (the image itself) and the interpretability of the visualization axes (the data transformation functions). We define a data transformation function as any linear or non-linear function of the original variables mapping the data into 1D. Even for a small dataset, the space of possible data transformations is beyond the limit of manual exploration, therefore it is important to develop automated techniques that capture both aspects of interpretability so that they can be used to guide the search process without human intervention. The goal of the search process is to find a smaller number of interpretable data transformations for the users to explore. We briefly discuss how we used such automated measures in an evolutionary computing based data dimensionality reduction application for visual analytics. In this paper, we present a two-part user study in which we separately investigated how humans rated the visualizations of labeled data and comprehensibility of mathematical expressions that could be used as data transformation functions. In the first part, we compared human perception with a number of automated measures from the machine learning and visual analytics literature. In the second part, we studied how various structural properties of an expression related to its interpretability.","AuthorNames-Deduped":"Ilknur Icke;Andrew Rosenberg","AuthorNames":"Ilknur Icke;Andrew Rosenberg","AuthorAffiliation":"The Graduate Center, The City University of New York, USA;The Graduate Center, The City University of New York, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":5,"XploreCitationCount - 2020-01":0,"PubsCited":8,"Award":null,"image":""},{"Conference":"VAST","Year":2011,"Title":"3D Visualization of temporal changes in bloggers' activities and interests","DOI":"10.1109/VAST.2011.6102475","Link":"http://dx.doi.org/10.1109/VAST.2011.6102475","FirstPage":283,"LastPage":284,"PaperType":"M","Abstract":"This paper presents a novel system for analyzing temporal changes in bloggers' activities and interests on a topic through a 3D visualization of dependency structures related to the topic. Having a dependency database built from a blog archive, our 3D visualization framework helps users to interactively exploring temporal changes in bloggers' activities and interests related to the topic.","AuthorNames-Deduped":"Masahiko Itoh;Naoki Yoshinaga 0001;Masashi Toyoda;Masaru Kitsuregawa","AuthorNames":"Masahiko Itoh;Naoki Yoshinaga;Masashi Toyoda;Masaru Kitsuregawa","AuthorAffiliation":"Institute of Industrial Science, University of Tokyo, Japan;Institute of Industrial Science, University of Tokyo, Japan;Institute of Industrial Science, University of Tokyo, Japan;Institute of Industrial Science, University of Tokyo, Japan","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":3,"Award":null,"image":"6102475-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"A state transition approach to understanding users' interactions","DOI":"10.1109/VAST.2011.6102476","Link":"http://dx.doi.org/10.1109/VAST.2011.6102476","FirstPage":285,"LastPage":286,"PaperType":"M","Abstract":"Understanding users' interactions is considered as one of the important research topics in visual analytics. Although numerous empirical user studies have been performed to understand a user's interaction, a limited study has been successful in connecting the user's interaction to his/her reasoning. In this paper, we present an approach of understanding experts' interactive analysis by connecting their interactions to conclusions (i.e. findings) through a state transition approach.","AuthorNames-Deduped":"Dong Hyun Jeong;Soo-Yeon Ji;William Ribarsky;Remco Chang","AuthorNames":"Dong Hyun Jeong;Soo-Yeon Ji;William Ribarsky;Remco Chang","AuthorAffiliation":"University of the District of Columbia, USA;Bowie State University, USA;University of North Carolina at Charlotte, USA;Tufts University, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":5,"Award":null,"image":"6102476-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2011,"Title":"Visualizing an information assurance risk taxonomy","DOI":"10.1109/VAST.2011.6102477","Link":"http://dx.doi.org/10.1109/VAST.2011.6102477","FirstPage":287,"LastPage":288,"PaperType":"M","Abstract":"The researchers explore the intersections between Information Assurance and Risk using visual analysis of text mining operations. The methodological approach involves searching for and extracting for analysis those abstracts and keywords groupings that relate to risk within a defined subset of scientific research journals. This analysis is conducted through a triangulated study incorporating visualizations produced using both Starlight and In-Spire visual analysis software. The results are definitional, showing current attitudes within the Information Assurance research community towards risk management strategies, while simultaneously demonstrating the value of visual analysis processes when engaging in sense making of a large body of knowledge.","AuthorNames-Deduped":"Victoria Lemieux;Barbara Endicott-Popovsky;Karl Eckler;Thomas Dang;Adam Jansen","AuthorNames":"Victoria Lemieux;Barbara Endicott-Popovsky;Karl Eckler;Thomas Dang;Adam Jansen","AuthorAffiliation":"University of British Columbia, USA;University of Washington, USA;University of Washington, USA;University of British Columbia, USA;University of British Columbia, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":2,"Award":null,"image":"6102477-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Find distance function, hide model inference","DOI":"10.1109/VAST.2011.6102478","Link":"http://dx.doi.org/10.1109/VAST.2011.6102478","FirstPage":289,"LastPage":290,"PaperType":"M","Abstract":"Faced with a large, high-dimensional dataset, many turn to data analysis approaches that they understand less well than the domain of their data. An expert's knowledge can be leveraged into many types of analysis via a domain-specific distance function, but creating such a function is not intuitive to do by hand. We have created a system that shows an initial visualization, adapts to user feedback, and produces a distance function as a result. Specifically, we present a multidimensional scaling (MDS) visualization and an iterative feedback mechanism for a user to affect the distance function that informs the visualization without having to adjust the parameters of the visualization directly. An encouraging experimental result suggests that using this tool, data attributes with useless data are given low importance in the distance function.","AuthorNames-Deduped":"Jingjing Liu;Eli T. Brown;Remco Chang","AuthorNames":"Jingjing Liu;Eli T. Brown;Remco Chang","AuthorAffiliation":"Tufts University, USA;Tufts University, USA;Tufts University, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":3,"PubsCited":4,"Award":null,"image":"6102478-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"KD-photomap: Exploring photographs in space and time","DOI":"10.1109/VAST.2011.6102479","Link":"http://dx.doi.org/10.1109/VAST.2011.6102479","FirstPage":291,"LastPage":292,"PaperType":"M","Abstract":"KD-photomap is a web-based visual analytics system for browsing collections of geotagged Flickr photographs in search of interesting pictures, places, and events. Spatial filtering of the data is performed through zooming, moving or searching along the map. Temporal filtering is possible through defining time windows using interactive histograms and calendar controls. Information about the number and spatiotemporal distribution of photos captured in an explored area is continuously provided using various visual cues.","AuthorNames-Deduped":"Iulian Peca;Haolin Zhi;Katerina Vrotsou;Natalia V. Andrienko;Gennady L. Andrienko","AuthorNames":"Iulian Peca;Haolin Zhi;Katerina Vrotsou;Natalia Andrienko;Gennady Andrienko","AuthorAffiliation":"University of Bonn and Fraunhofer Institute for Intelligent Analysis and Information Systems (IAIS), USA;University of Bonn and Fraunhofer Institute for Intelligent Analysis and Information Systems (IAIS), USA;University of Bonn and Fraunhofer Institute for Intelligent Analysis and Information Systems (IAIS), USA;University of Bonn and Fraunhofer Institute for Intelligent Analysis and Information Systems (IAIS), USA;University of Bonn and Fraunhofer Institute for Intelligent Analysis and Information Systems (IAIS), USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":3,"PubsCited":6,"Award":null,"image":"6102479-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2011,"Title":"PORGY: Interactive and visual reasoning with graph rewriting systems","DOI":"10.1109/VAST.2011.6102480","Link":"http://dx.doi.org/10.1109/VAST.2011.6102480","FirstPage":293,"LastPage":294,"PaperType":"M","Abstract":"Graph rewriting systems are easily described and explained. They can be seen as a game where one iterates transformation rules on an initial graph, until some condition is met. A rule describes a local pattern (i.e. a subgraph) that must be identified in a graph and specifies how to transform this subgraph. The graph rewriting formalism is at the same time extremely rich and complex, making the study of a model expressed in terms of graph rewriting quite challenging. For instance, predicting whether rules can be applied in any order is often difficult. When modelling complex systems, graphical formalisms have clear advantages: they are more intuitive and make it easier to visualize a system and convey intuitions about it. This work focuses on the design of an interactive visual graph rewriting system which supports graphical manipulations and computation to reason and simulate on a system. PORGY has been designed based on regular exchanges with graph rewriting systems experts and users over the past three years. The design choices relied on a careful methodology inspired from Munzner's nested process model for visualization design and validation [4].","AuthorNames-Deduped":"Bruno Pinaud;Jonathan Dubois;Guy Melançon","AuthorNames":"Bruno Pinaud;Jonathan Dubois;Guy Melançon","AuthorAffiliation":"University of Bordeaux and INRIA Bordeaux Sud-Ouest, France;University of Bordeaux and INRIA Bordeaux Sud-Ouest, France;University of Bordeaux and INRIA Bordeaux Sud-Ouest, France","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":6,"Award":null,"image":"6102480-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Exploring proportions: Comparative visualization of categorical data","DOI":"10.1109/VAST.2011.6102481","Link":"http://dx.doi.org/10.1109/VAST.2011.6102481","FirstPage":295,"LastPage":296,"PaperType":"M","Abstract":"This poster describes an approach to facilitate comparisons in multi-dimensional categorical data. The key idea is to represent over- or under-proportional relationships explicitly. On an overview level, the visualization of various measures conveys pair-wise relationships between categorical dimensions. For more details, interaction supports to relate a single category to all categories of multiple dimensions. We discuss methods for representing relationships and visualization-driven strategies for ordering dimensions and categories, and we illustrate the approach by means of data from a social survey.","AuthorNames-Deduped":"Harald Piringer;Matthias Buchetics","AuthorNames":"Harald Piringer;Matthias Buchetics","AuthorAffiliation":"VRVis Research Center, Vienna, Austria;VRVis Research Center, Vienna, Austria","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":2,"PubsCited":4,"Award":null,"image":"6102481-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Pexel and heatmap visual analysis of multidimensional gun/homicide data","DOI":"10.1109/VAST.2011.6102482","Link":"http://dx.doi.org/10.1109/VAST.2011.6102482","FirstPage":297,"LastPage":298,"PaperType":"M","Abstract":"We present a visual analysis tool for mining correlations in county-level, multidimensional gun/homicide data. The tool uses 2D pexels, heatmaps, linked-views, dynamic queries and details-on-demand to analyze annual county-level data on firearm homicide rates and gun availability, as well as various socio-demographic measures. A statistical significance filter was implemented as a visual means to validate exploratory hypotheses. Results from expert evaluations indicate that our methods outperform typical graphical techniques used by statisticians, such as bar graphs, scatterplots and residual plots, to show spatial and temporal relationships. Our visualization has the potential to convey the impact of gun availability on firearm homicides to the public health arena and the general public.","AuthorNames-Deduped":"Scott D. Rothenberger;John E. Wenskovitch;G. Elisabeta Marai","AuthorNames":"Scott D. Rothenberger;John E. Wenskovitch;G. Elisabeta Marai","AuthorAffiliation":"Department of Statistics, University of Pittsburgh, USA;Department of Computer Science, University of Pittsburgh, USA;Department of Computer Science, University of Pittsburgh, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":1,"PubsCited":4,"Award":null,"image":"6102482-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2011,"Title":"City sentinel - VAST 2011 mini challenge 1 award: \"Outstanding integration of computational and visual methods\"","DOI":"10.1109/VAST.2011.6102485","Link":"http://dx.doi.org/10.1109/VAST.2011.6102485","FirstPage":305,"LastPage":306,"PaperType":"M","Abstract":"We present City Sentinel, an in-house built visual analytic software capable of handling a large collection of textual documents by combining diverse text mining and visualization tools. We applied this tool for the Vast Challenge 2011, Mini Challenge 1 over millions of tweet messages. We demonstrate how City Sentinel aided the analyst in retrieving the hidden information from the tweet messages to analyze and locate a hypothetical epidemic outbreak.","AuthorNames-Deduped":"N. Banfi;L. Dudas;Zsolt Fekete;J. Gobolos-Szabo;András Lukács;A. Nagy;A. Szabo;Z. Szabo;Gabor Szücs","AuthorNames":"N. Bánfi;L. Dudás;Zs. Fekete;J. Göbö lös-Szabó;A. Lukács;Á. Nagy;A. Szabó;Z. Szabó;G. Szűcs","AuthorAffiliation":"Computer and Automation Research Institute (MTA SZTAKI), Hungarian Academy of Sciences, Hungary;Computer and Automation Research Institute (MTA SZTAKI), Hungarian Academy of Sciences, Hungary;Computer and Automation Research Institute (MTA SZTAKI), Hungarian Academy of Sciences, Hungary;Data Mining and Web Search Group, Hungary;Computer and Automation Research Institute (MTA SZTAKI), Hungarian Academy of Sciences, Hungary;Computer and Automation Research Institute (MTA SZTAKI), Hungarian Academy of Sciences, Hungary;Computer and Automation Research Institute (MTA SZTAKI), Hungarian Academy of Sciences, Hungary;Computer and Automation Research Institute (MTA SZTAKI), Hungarian Academy of Sciences, Hungary;Computer and Automation Research Institute (MTA SZTAKI), Hungarian Academy of Sciences, Hungary","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":3,"Award":null,"image":"6102485-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Mapping an epidemic outbreak: Effective analysis and presentation","DOI":"10.1109/VAST.2011.6102486","Link":"http://dx.doi.org/10.1109/VAST.2011.6102486","FirstPage":307,"LastPage":308,"PaperType":"M","Abstract":"The microblog challenge presented an opportunity to use commercial software for visual analysis. An epidemic outbreak occurred in the city of Vastopolis, requiring visualizations of symptoms and their spread over time. Using these tools, analysts could successfully identify the outbreak's origin and pattern of dispersion. The maps used to analyze the data and present the results provided clear, easily understood representations, and presented a logical explanation of a complex progression of events.","AuthorNames-Deduped":"Kevin Boone;Edward Swing","AuthorNames":"Kevin Boone;Edward Swing","AuthorAffiliation":"Vision Systems &amp; Technology, Inc., a SAS Company, Australia;Vision Systems &amp; Technology, Inc., a SAS Company, Australia","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":1,"Award":null,"image":"6102486-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"ScatterBlogs: Geo-spatial document analysis","DOI":"10.1109/VAST.2011.6102488","Link":"http://dx.doi.org/10.1109/VAST.2011.6102488","FirstPage":309,"LastPage":310,"PaperType":"M","Abstract":"We presented Scatterblogs, a system for microblog analysis that seamlessly integrates search backend and visual frontend. It provides powerful, automatic algorithms for detecting spatio-temporal `anomalies' within blog entries as well as corresponding visual representations and interaction facilities for inspecting anomalies or exploiting them in further analytic steps. Apart from that, we consider the system's combinatoric facilities for building complex hypotheses from temporal, spatial, and content-related aspects an important feature. This was the key for creating a cross-checked analysis for MC1.","AuthorNames-Deduped":"Harald Bosch;Dennis Thom;Michael Wörner 0001;Steffen Koch;Edwin Puttmann;Dominik Jäckle;Thomas Ertl","AuthorNames":"Harald Bosch;Dennis Thom;Michael Wörner;Steffen Koch;Edwin Püttmann;Dominik Jäckle;Thomas Ertl","AuthorAffiliation":"Institute for Visualization and Interactive Systems, Universität Stuttgart, Germany;Institute for Visualization and Interactive Systems, Universität Stuttgart, Germany;Institute for Visualization and Interactive Systems, Universität Stuttgart, Germany;Institute for Visualization and Interactive Systems, Universität Stuttgart, Germany;Institute for Visualization and Interactive Systems, Universität Stuttgart, Germany;Institute for Visualization and Interactive Systems, Universität Stuttgart, Germany;Institute for Visualization and Interactive Systems, Universität Stuttgart, Germany","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":19,"AminerCitationCount_06-2020":25,"XploreCitationCount - 2020-01":17,"PubsCited":2,"Award":null,"image":"6102488-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"epSpread - Storyboarding for visual analytics","DOI":"10.1109/VAST.2011.6102489","Link":"http://dx.doi.org/10.1109/VAST.2011.6102489","FirstPage":311,"LastPage":312,"PaperType":"M","Abstract":"We present epSpread, an analysis and storyboarding tool for geolocated microblogging data. Individual time points and ranges are analysed through queries, heatmaps, word clouds and streamgraphs. The underlying narrative is shown on a storyboard-style timeline for discussion, refinement and presentation. The tool was used to analyse data from the VAST Challenge 2011 Mini-Challenge 1, tracking the spread of an epidemic using microblogging data. In this article we describe how the tool was used to identify the origin and track the spread of the epidemic.","AuthorNames-Deduped":"Llyr ap Cenydd;Rick Walker;Serban R. Pop;Helen C. Miles;Chris J. Hughes;William John Teahan;Jonathan C. Roberts","AuthorNames":"Llyr ap Cenydd;Rick Walker;Serban Pop;Helen Miles;Chris Hughes;William Teahan;Jonathan C. Roberts","AuthorAffiliation":"School of Computer Science, Bangor University, UK;School of Computer Science, Bangor University, UK;School of Computer Science, Bangor University, UK;School of Computer Science, Bangor University, UK;School of Computer Science, Bangor University, UK;School of Computer Science, Bangor University, UK;School of Computer Science, Bangor University, UK","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":5,"XploreCitationCount - 2020-01":2,"PubsCited":3,"Award":null,"image":"6102489-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2011,"Title":"MobileAnalymator: Animating data changes on mobile devices","DOI":"10.1109/VAST.2011.6102490","Link":"http://dx.doi.org/10.1109/VAST.2011.6102490","FirstPage":313,"LastPage":314,"PaperType":"M","Abstract":"MobileAnalymator (Mobile Analysis Animator) is a visual analytic system designed to analyze geospatial-temporal data on mobile devices. The system is an Internet based application that allows analysts to work in flexile enviornments at anytime. Its client side is developed by Adobe Flash to animate and interact with data. The server side uses Java and MySQL to query, compute, and serve data. The analyst can run the analytical task from a tablet (or computer) with Internet connection. MobileAnalymator adopted spatial and temporal autocorrelations in the interface design and integrated tangible interaction in the navigation to support analysis process.","AuthorNames-Deduped":"Victor Y. Chen;Cheryl Z. Qian;Li Zhang","AuthorNames":"Yingjie Victor Chen;Zhenyu Cheryl Qian;Li Zhang","AuthorAffiliation":"Interaction Design, Purdue University, USA;Interaction Design, Purdue University, USA;Interaction Design, Purdue University, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":3,"Award":null,"image":"6102490-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Geovisual analytics for cyber security: Adopting the GeoViz Toolkit","DOI":"10.1109/VAST.2011.6102491","Link":"http://dx.doi.org/10.1109/VAST.2011.6102491","FirstPage":315,"LastPage":316,"PaperType":"M","Abstract":"For the VAST 2011 Network Security Mini-Challenge, we adopted geovisual analytic methods and applied them in the field of network security. We used the GeoViz Toolkit [1] to represent cyber security events, by fabricating a simple “geography” of several sets of blocks (one for the workstations, one for the servers, and one for the Internet) using ArcGIS 10 (by ESRI - Environmental System Research Institute). Security data was tabulated using Perl scripts to parse the logs in order to create representations of event frequency and where they occurred on the network. The tabulated security data was then added as attributes of the geography. Exploration of the data and subsequent analysis of the meaning and impact of the cyber security events was made possible using the GeoViz Toolkit.","AuthorNames-Deduped":"Nicklaus A. Giacobe;Sen Xu","AuthorNames":"Nicklaus A. Giacobe;Sen Xu","AuthorAffiliation":"College of Information Sciences and Technology, The Pennsylvania State University, USA;Department of Geography and GeoVISTA Center, The Pennsylvania State University, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":7,"AminerCitationCount_06-2020":11,"XploreCitationCount - 2020-01":6,"PubsCited":6,"Award":null,"image":"6102491-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2011,"Title":"Guiding security analysis through visualization","DOI":"10.1109/VAST.2011.6102492","Link":"http://dx.doi.org/10.1109/VAST.2011.6102492","FirstPage":317,"LastPage":318,"PaperType":"M","Abstract":"We present a multiple views visualization for the security data in the VAST 2010 Mini Challenge 2. The visualization is used to monitor log event activity on the network log data included in the challenge. Interactions are provided that allow analysts to investigate suspicious activity and escalate events as needed. Additionally, a database application is used to allow SQL queries for more detailed investigation.","AuthorNames-Deduped":"Lane Harrison;Wenwen Dou;Aidong Lu;William Ribarsky;Xiaoyu Wang","AuthorNames":"Lane Harrison;Wenwen Dou;Aidong Lu;William Ribarsky;Xiaoyu Wang","AuthorAffiliation":"Computer Science, UNC-Charlotte, USA;Computer Science, UNC-Charlotte, USA;Computer Science, UNC-Charlotte, USA;Computer Science, UNC-Charlotte, USA;Computer Science, UNC-Charlotte, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":0,"PubsCited":3,"Award":null,"image":"6102492-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2011,"Title":"An integrated visualization on network events VAST 2011 mini challenge #2 award: \"Outstanding integrated overview display\"","DOI":"10.1109/VAST.2011.6102493","Link":"http://dx.doi.org/10.1109/VAST.2011.6102493","FirstPage":319,"LastPage":321,"PaperType":"M","Abstract":"To visualize security trends for the data set provided by the VAST 2011 Mini Challenge #2 a custom tool has been developed. Open source tools [1,2], web programming languages [4,7] and an open source database [3] has been used to work with the data and create a visualization for security log files containing network security trends. In this paper, the tools and methods used for the analysis are described. The methods include the log synchronization with different timezone and the development of heat maps and parallel coordinates charts. To develop the visualization, Processing and Canvas [4,7] was used.","AuthorNames-Deduped":"Walter Marcelo Lamagna","AuthorNames":"Walter Marcelo Lamagna","AuthorAffiliation":"Universidad de Buenos Aires, Master on Datamining and Knowledge Discovery, Spain","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":1,"PubsCited":8,"Award":null,"image":"6102493-fig-1a-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Analyst's workspace: Protecting vastopolis","DOI":"10.1109/VAST.2011.6102495","Link":"http://dx.doi.org/10.1109/VAST.2011.6102495","FirstPage":323,"LastPage":324,"PaperType":"M","Abstract":"Analyst's Workspace is a sensemaking environment designed specifically for use of large, high-resolution displays. It employs a spatial workspace to integrate foraging and synthesis activities into a unified process. In this paper we describe how Analyst's Workspace solved the VAST 2011 mini-challenge #3 and discuss some of the unique features of the environment.","AuthorNames-Deduped":"Christopher Andrews;Mahmud Shahriar Hossain;Samah Gad;Naren Ramakrishnan;Chris North","AuthorNames":"Christopher Andrews;M. Shahriar Hossain;Samah Gad;Naren Ramakrishnan;Chris North","AuthorAffiliation":"Virginia Tech, USA;Virginia Tech, USA;Virginia Tech, USA;Virginia Tech, USA;Virginia Tech, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":4,"Award":null,"image":"6102495-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Jigsaw to save vastopolis","DOI":"10.1109/VAST.2011.6102496","Link":"http://dx.doi.org/10.1109/VAST.2011.6102496","FirstPage":325,"LastPage":326,"PaperType":"M","Abstract":"This article describes our analytic process and experience of using the Jigsaw system in working on the VAST 2011 Mini Challenge 3. We describe how we extracted and worked with entities from the documents, and how Jigsaw's computational analysis capabilities and visualizations scaffolded the investigation. Based on our experiences, we discuss desirable features that would enhance the analytic power of Jigsaw.","AuthorNames-Deduped":"Elizabeth Braunstein;Carsten Görg;Zhicheng Liu;John T. Stasko","AuthorNames":"Elizabeth Braunstein;Carsten Görg;Zhicheng Liu;John Stasko","AuthorAffiliation":"Mercyhurst College, USA;Univ. of Colorado Denver, USA;Georgia Tech, USA;Georgia Tech, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":0,"PubsCited":3,"Award":null,"image":"6102496-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2011,"Title":"Interactive data analysis with nSpace2(c)","DOI":"10.1109/VAST.2011.6102497","Link":"http://dx.doi.org/10.1109/VAST.2011.6102497","FirstPage":327,"LastPage":328,"PaperType":"M","Abstract":"nSpace2 is an innovative visual analytics tool that was the primary platform used to search, evaluate, and organize the data in the VAST 2011 Mini Challenge #3 dataset. nSpace2 is a web-based tool that is designed to facilitate the back-and-forth flow of the multiple steps of an analysis process, including search, data triage, organization, sense-making, and reporting. This paper describes how nSpace2 was used to assist every step of the analysis process for this VAST challenge.","AuthorNames-Deduped":"Casey M. Canfield;David Sheffield","AuthorNames":"Casey M. Canfield;David Sheffield","AuthorAffiliation":"Oculus Info Inc., USA;Oculus Info Inc., USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":null,"XploreCitationCount - 2020-01":0,"PubsCited":3,"Award":null,"image":"6102497-fig-1-source-large.gif"},{"Conference":"VAST","Year":2011,"Title":"Visual analytics of terrorist activities related to epidemics","DOI":"10.1109/VAST.2011.6102498","Link":"http://dx.doi.org/10.1109/VAST.2011.6102498","FirstPage":329,"LastPage":330,"PaperType":"M","Abstract":"The task of the VAST 2011 Grand Challenge was to investigate potential terrorist activities and their relation to the spread of an epidemic. Three different data sets were provided as part of three Mini Challenges (MCs). MC 1 was about analyzing geo-tagged microblogging (Twitter) messages to characterize the spread of an epidemic. MC 2 required analyzing threats to a computer network using a situational awareness approach. In MC 3 possible criminal and terrorist activities were to be analyzed based on a collection of news articles. To solve the Grand Challenge, insight from each of the individual MCs had to be integrated appropriately.","AuthorNames-Deduped":"Enrico Bertini;Juri Buchmüller;Fabian Fischer 0001;Stephan Huber;Thomas Lindemeier;Fabian Maass;Florian Mansmann;Thomas Ramm;Michael Regenscheit;Christian Rohrdantz;Christian Scheible;Tobias Schreck;Stephan Sellien;Florian Stoffel;Mark Tautzenberger;Matthias Zieker;Daniel A. Keim","AuthorNames":"Enrico Bertini;Christian Scheible;Tobias Schreck;Stephan Sellien;Florian Stoffel;Mark Tautzenberger;Matthias Zieker;Daniel A. Keim;Juri Buchmüller;Fabian Fischer;Stephan Huber;Thomas Lindemeier;Fabian Maaß;Florian Mansmann;Thomas Ramm;Michael Regenscheit;Christian Rohrdantz","AuthorAffiliation":"Data Analysis and Visualization Group, University of Konstanz, Germany;Data Analysis and Visualization Group, University of Konstanz, Germany;Data Analysis and Visualization Group, University of Konstanz, Germany;Data Analysis and Visualization Group, University of Konstanz, Germany;Data Analysis and Visualization Group, University of Konstanz, Germany;Data Analysis and Visualization Group, University of Konstanz, Germany;Data Analysis and Visualization Group, University of Konstanz, Germany;Data Analysis and Visualization Group, University of Konstanz, Germany;Data Analysis and Visualization Group, University of Konstanz, Germany;Data Analysis and Visualization Group, University of Konstanz, Germany;Data Analysis and Visualization Group, University of Konstanz, Germany;Data Analysis and Visualization Group, University of Konstanz, Germany;Data Analysis and Visualization Group, University of Konstanz, Germany;Data Analysis and Visualization Group, University of Konstanz, Germany;Data Analysis and Visualization Group, University of Konstanz, Germany;Data Analysis and Visualization Group, University of Konstanz, Germany;Data Analysis and Visualization Group, University of Konstanz, Germany","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":6,"AminerCitationCount_06-2020":7,"XploreCitationCount - 2020-01":3,"PubsCited":13,"Award":null,"image":"6102498-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2012,"Title":"A Visual Analytics Approach to Multiscale Exploration of Environmental Time Series","DOI":"10.1109/TVCG.2012.191","Link":"http://dx.doi.org/10.1109/TVCG.2012.191","FirstPage":2899,"LastPage":2907,"PaperType":"J","Abstract":"We present a Visual Analytics approach that addresses the detection of interesting patterns in numerical time series, specifically from environmental sciences. Crucial for the detection of interesting temporal patterns are the time scale and the starting points one is looking at. Our approach makes no assumption about time scale and starting position of temporal patterns and consists of three main steps: an algorithm to compute statistical values for all possible time scales and starting positions of intervals, visual identification of potentially interesting patterns in a matrix visualization, and interactive exploration of detected patterns. We demonstrate the utility of this approach in two scientific scenarios and explain how it allowed scientists to gain new insight into the dynamics of environmental systems.","AuthorNames-Deduped":"Mike Sips;Patrick Köthur;Andrea Unger;Hans-Christian Hege;Doris Dransch","AuthorNames":"Mike Sips;Patrick Köthur;Andrea Unger;Hans-Christian Hege;Doris Dransch","AuthorAffiliation":"German Research Center for GeoSciences GFZ;German Research Center for GeoSciences GFZ;German Research Center for GeoSciences GFZ;Zuse Institute Berlin;German Research Center for GeoSciences GFZ","InternalReferences":"10.1109/INFVIS.2001.963273;10.1109/INFVIS.1995.528685","AuthorKeywords":"Time series analysis, multiscale visualization, visual analytics","AminerCitationCount_02-2020":20,"AminerCitationCount_06-2020":27,"XploreCitationCount - 2020-01":24,"PubsCited":35,"Award":null,"image":"6327296-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"An Affordance-Based Framework for Human Computation and Human-Computer Collaboration","DOI":"10.1109/TVCG.2012.195","Link":"http://dx.doi.org/10.1109/TVCG.2012.195","FirstPage":2859,"LastPage":2868,"PaperType":"J","Abstract":"Visual Analytics is “the science of analytical reasoning facilitated by visual interactive interfaces” [70]. The goal of this field is to develop tools and methodologies for approaching problems whose size and complexity render them intractable without the close coupling of both human and machine analysis. Researchers have explored this coupling in many venues: VAST, Vis, InfoVis, CHI, KDD, IUI, and more. While there have been myriad promising examples of human-computer collaboration, there exists no common language for comparing systems or describing the benefits afforded by designing for such collaboration. We argue that this area would benefit significantly from consensus about the design attributes that define and distinguish existing techniques. In this work, we have reviewed 1,271 papers from many of the top-ranking conferences in visual analytics, human-computer interaction, and visualization. From these, we have identified 49 papers that are representative of the study of human-computer collaborative problem-solving, and provide a thorough overview of the current state-of-the-art. Our analysis has uncovered key patterns of design hinging on humanand machine-intelligence affordances, and also indicates unexplored avenues in the study of this area. The results of this analysis provide a common framework for understanding these seemingly disparate branches of inquiry, which we hope will motivate future work in the field.","AuthorNames-Deduped":"R. Jordan Crouser;Remco Chang","AuthorNames":"R. Jordon Crouser;Remco Chang","AuthorAffiliation":"Tufts University;Tufts University","InternalReferences":"10.1109/VAST.2010.5652398;10.1109/VAST.2011.6102461;10.1109/TVCG.2009.199;10.1109/VAST.2010.5652910;10.1109/VAST.2010.5652484;10.1109/VAST.2009.5332584;10.1109/VAST.2010.5652885;10.1109/VAST.2009.5333564;10.1109/VAST.2010.5652392;10.1109/VAST.2009.5332586;10.1109/VAST.2011.6102451;10.1109/VAST.2009.5333023;10.1109/VAST.2009.5333020;10.1109/VAST.2009.5332628;10.1109/TVCG.2011.173;10.1109/TVCG.2011.218;10.1109/TVCG.2011.231;10.1109/VAST.2010.5652443;10.1109/VAST.2010.5653598;10.1109/VAST.2011.6102447","AuthorKeywords":"Human computation, human complexity, theory, framework","AminerCitationCount_02-2020":21,"AminerCitationCount_06-2020":29,"XploreCitationCount - 2020-01":22,"PubsCited":83,"Award":null,"image":"6327292-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Enterprise Data Analysis and Visualization: An Interview Study","DOI":"10.1109/TVCG.2012.219","Link":"http://dx.doi.org/10.1109/TVCG.2012.219","FirstPage":2917,"LastPage":2926,"PaperType":"J","Abstract":"Organizations rely on data analysts to model customer engagement, streamline operations, improve production, inform business decisions, and combat fraud. Though numerous analysis and visualization tools have been built to improve the scale and efficiency at which analysts can work, there has been little research on how analysis takes place within the social and organizational context of companies. To better understand the enterprise analysts' ecosystem, we conducted semi-structured interviews with 35 data analysts from 25 organizations across a variety of sectors, including healthcare, retail, marketing and finance. Based on our interview data, we characterize the process of industrial data analysis and document how organizational features of an enterprise impact it. We describe recurring pain points, outstanding challenges, and barriers to adoption for visual analytic tools. Finally, we discuss design implications and opportunities for visual analysis research.","AuthorNames-Deduped":"Sean Kandel;Andreas Paepcke;Joseph M. Hellerstein;Jeffrey Heer","AuthorNames":"Sean Kandel;Andreas Paepcke;Joseph M. Hellerstein;Jeffrey Heer","AuthorAffiliation":"Stanford University;Stanford University;University of California, Berkeley;Stanford University","InternalReferences":"10.1109/TVCG.2008.137;10.1109/VAST.2008.4677365;10.1109/VAST.2011.6102438;10.1109/INFVIS.2005.1532136;10.1109/VAST.2010.5652880;10.1109/VAST.2009.5333878;10.1109/VAST.2007.4389011;10.1109/VAST.2011.6102435","AuthorKeywords":"Data, analysis, visualization, enterprise","AminerCitationCount_02-2020":138,"AminerCitationCount_06-2020":185,"XploreCitationCount - 2020-01":105,"PubsCited":37,"Award":"HM","image":"6327298-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Examining the Use of a Visual Analytics System for Sensemaking Tasks: Case Studies with Domain Experts","DOI":"10.1109/TVCG.2012.224","Link":"http://dx.doi.org/10.1109/TVCG.2012.224","FirstPage":2869,"LastPage":2878,"PaperType":"J","Abstract":"While the formal evaluation of systems in visual analytics is still relatively uncommon, particularly rare are case studies of prolonged system use by domain analysts working with their own data. Conducting case studies can be challenging, but it can be a particularly effective way to examine whether visual analytics systems are truly helping expert users to accomplish their goals. We studied the use of a visual analytics system for sensemaking tasks on documents by six analysts from a variety of domains. We describe their application of the system along with the benefits, issues, and problems that we uncovered. Findings from the studies identify features that visual analytics systems should emphasize as well as missing capabilities that should be addressed. These findings inform design implications for future systems.","AuthorNames-Deduped":"Youn ah Kang;John T. Stasko","AuthorNames":"Youn-ah Kang;John Stasko","AuthorAffiliation":"Google Inc.;Georgia Institute of Technology","InternalReferences":"10.1109/VAST.2008.4677362;10.1109/VAST.2006.261416;10.1109/INFVIS.2004.5;10.1109/VAST.2011.6102438;10.1109/VAST.2012.6400559;10.1109/VAST.2007.4389006;10.1109/VAST.2009.5333878","AuthorKeywords":"Visual analytics, case study, qualitative evaluation","AminerCitationCount_02-2020":17,"AminerCitationCount_06-2020":25,"XploreCitationCount - 2020-01":21,"PubsCited":42,"Award":null,"image":"6327293-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Reinventing the Contingency Wheel: Scalable Visual Analytics of Large Categorical Data","DOI":"10.1109/TVCG.2012.254","Link":"http://dx.doi.org/10.1109/TVCG.2012.254","FirstPage":2849,"LastPage":2858,"PaperType":"J","Abstract":"Contingency tables summarize the relations between categorical variables and arise in both scientific and business domains. Asymmetrically large two-way contingency tables pose a problem for common visualization methods. The Contingency Wheel has been recently proposed as an interactive visual method to explore and analyze such tables. However, the scalability and readability of this method are limited when dealing with large and dense tables. In this paper we present Contingency Wheel++, new visual analytics methods that overcome these major shortcomings: (1) regarding automated methods, a measure of association based on Pearson's residuals alleviates the bias of the raw residuals originally used, (2) regarding visualization methods, a frequency-based abstraction of the visual elements eliminates overlapping and makes analyzing both positive and negative associations possible, and (3) regarding the interactive exploration environment, a multi-level overview+detail interface enables exploring individual data items that are aggregated in the visualization or in the table using coordinated views. We illustrate the applicability of these new methods with a use case and show how they enable discovering and analyzing nontrivial patterns and associations in large categorical data.","AuthorNames-Deduped":"Bilal Alsallakh;Wolfgang Aigner;Silvia Miksch;M. Eduard Gröller","AuthorNames":"Bilal Alsallakh;Wolfgang Aigner;Silvia Miksch;M. Eduard Gröller","AuthorAffiliation":"Vienna University of Technology;Vienna University of Technology;Vienna University of Technology;Vienna University of Technology","InternalReferences":"10.1109/INFVIS.2005.1532139;10.1109/VISUAL.2005.1532819;10.1109/INFVIS.2002.1173156;10.1109/INFVIS.2003.1249016;10.1109/INFVIS.2002.1173157","AuthorKeywords":"Large categorical data, contingency table analysis, information interfaces and representation, visual analytics","AminerCitationCount_02-2020":16,"AminerCitationCount_06-2020":19,"XploreCitationCount - 2020-01":13,"PubsCited":42,"Award":"HM","image":"6327291-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2012,"Title":"Scatter/Gather Clustering: Flexibly Incorporating User Feedback to Steer Clustering Results","DOI":"10.1109/TVCG.2012.258","Link":"http://dx.doi.org/10.1109/TVCG.2012.258","FirstPage":2829,"LastPage":2838,"PaperType":"J","Abstract":"Significant effort has been devoted to designing clustering algorithms that are responsive to user feedback or that incorporate prior domain knowledge in the form of constraints. However, users desire more expressive forms of interaction to influence clustering outcomes. In our experiences working with diverse application scientists, we have identified an interaction style scatter/gather clustering that helps users iteratively restructure clustering results to meet their expectations. As the names indicate, scatter and gather are dual primitives that describe whether clusters in a current segmentation should be broken up further or, alternatively, brought back together. By combining scatter and gather operations in a single step, we support very expressive dynamic restructurings of data. Scatter/gather clustering is implemented using a nonlinear optimization framework that achieves both locality of clusters and satisfaction of user-supplied constraints. We illustrate the use of our scatter/gather clustering approach in a visual analytic application to study baffle shapes in the bat biosonar (ears and nose) system. We demonstrate how domain experts are adept at supplying scatter/gather constraints, and how our framework incorporates these constraints effectively without requiring numerous instance-level constraints.","AuthorNames-Deduped":"Mahmud Shahriar Hossain;Praveen Kumar Reddy Ojili;Cindy Grimm;Rolf Mueller;Layne T. Watson;Naren Ramakrishnan","AuthorNames":"M. Shahriar Hossain;Praveen Kumar Reddy Ojili;Cindy Grimm;Rolf Müller;Layne T. Watson;Naren Ramakrishnan","AuthorAffiliation":null,"InternalReferences":"10.1109/VAST.2009.5332584;10.1109/VAST.2007.4388999;10.1109/VAST.2008.4677350;10.1109/INFVIS.1998.729559;10.1109/VAST.2009.5332629","AuthorKeywords":"Scatter/gather clustering, alternative clustering, constrained clustering","AminerCitationCount_02-2020":12,"AminerCitationCount_06-2020":14,"XploreCitationCount - 2020-01":13,"PubsCited":64,"Award":null,"image":"6327289-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Semantic Interaction for Sensemaking: Inferring Analytical Reasoning for Model Steering","DOI":"10.1109/TVCG.2012.260","Link":"http://dx.doi.org/10.1109/TVCG.2012.260","FirstPage":2879,"LastPage":2888,"PaperType":"J","Abstract":"Visual analytic tools aim to support the cognitively demanding task of sensemaking. Their success often depends on the ability to leverage capabilities of mathematical models, visualization, and human intuition through flexible, usable, and expressive interactions. Spatially clustering data is one effective metaphor for users to explore similarity and relationships between information, adjusting the weighting of dimensions or characteristics of the dataset to observe the change in the spatial layout. Semantic interaction is an approach to user interaction in such spatializations that couples these parametric modifications of the clustering model with users' analytic operations on the data (e.g., direct document movement in the spatialization, highlighting text, search, etc.). In this paper, we present results of a user study exploring the ability of semantic interaction in a visual analytic prototype, ForceSPIRE, to support sensemaking. We found that semantic interaction captures the analytical reasoning of the user through keyword weighting, and aids the user in co-creating a spatialization based on the user's reasoning and intuition.","AuthorNames-Deduped":"Alex Endert;Patrick Fiaux;Chris North","AuthorNames":"Alex Endert;Patrick Fiaux;Chris North","AuthorAffiliation":"Virginia Polytechnic Institute and State University;Virginia Polytechnic Institute and State University;Virginia Polytechnic Institute and State University","InternalReferences":"10.1109/INFVIS.1995.528686;10.1109/VAST.2012.6400559;10.1109/VAST.2011.6102449;10.1109/VAST.2011.6102438;10.1109/VAST.2007.4389006","AuthorKeywords":"User Interaction, visualization, sensemaking, analytic reasoning, visual analytics","AminerCitationCount_02-2020":48,"AminerCitationCount_06-2020":69,"XploreCitationCount - 2020-01":49,"PubsCited":36,"Award":null,"image":"6327294-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"The User Puzzle---Explaining the Interaction with Visual Analytics Systems","DOI":"10.1109/TVCG.2012.273","Link":"http://dx.doi.org/10.1109/TVCG.2012.273","FirstPage":2908,"LastPage":2916,"PaperType":"J","Abstract":"Visual analytics emphasizes the interplay between visualization, analytical procedures performed by computers and human perceptual and cognitive activities. Human reasoning is an important element in this context. There are several theories in psychology and HCI explaining open-ended and exploratory reasoning. Five of these theories (sensemaking theories, gestalt theories, distributed cognition, graph comprehension theories and skill-rule-knowledge models) are described in this paper. We discuss their relevance for visual analytics. In order to do this more systematically, we developed a schema of categories relevant for visual analytics research and evaluation. All these theories have strengths but also weaknesses in explaining interaction with visual analytics systems. A possibility to overcome the weaknesses would be to combine two or more of these theories.","AuthorNames-Deduped":"Margit Pohl;Michael Smuc;Eva Mayr","AuthorNames":"Margit Pohl;Michael Smuc;Eva Mayr","AuthorAffiliation":"Vienna University of Technology;Danube University Krems;Danube University Krems","InternalReferences":"10.1109/TVCG.2008.121;10.1109/TVCG.2007.70515;10.1109/VAST.2010.5653598;10.1109/VAST.2008.4677361;10.1109/VAST.2011.6102445","AuthorKeywords":"Cognitive theory, visual knowledge discovery, interaction design, reasoning, problem solving","AminerCitationCount_02-2020":18,"AminerCitationCount_06-2020":null,"XploreCitationCount - 2020-01":18,"PubsCited":65,"Award":null,"image":"6327297-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Visual Analytics Methodology for Eye Movement Studies","DOI":"10.1109/TVCG.2012.276","Link":"http://dx.doi.org/10.1109/TVCG.2012.276","FirstPage":2889,"LastPage":2898,"PaperType":"J","Abstract":"Eye movement analysis is gaining popularity as a tool for evaluation of visual displays and interfaces. However, the existing methods and tools for analyzing eye movements and scanpaths are limited in terms of the tasks they can support and effectiveness for large data and data with high variation. We have performed an extensive empirical evaluation of a broad range of visual analytics methods used in analysis of geographic movement data. The methods have been tested for the applicability to eye tracking data and the capability to extract useful knowledge about users' viewing behaviors. This allowed us to select the suitable methods and match them to possible analysis tasks they can support. The paper describes how the methods work in application to eye tracking data and provides guidelines for method selection depending on the analysis tasks.","AuthorNames-Deduped":"Gennady L. Andrienko;Natalia V. Andrienko;Michael Burch;Daniel Weiskopf","AuthorNames":"Gennady Andrienko;Natalia Andrienko;Michael Burch;Daniel Weiskopf","AuthorAffiliation":"Fraunhofer Institute IAIS;Fraunhofer Institute IAIS;University of Stuttgart;University of Stuttgart","InternalReferences":"10.1109/VAST.2009.5332593;10.1109/TVCG.2011.193;10.1109/INFVIS.2005.1532150","AuthorKeywords":"Visual analytics, eye tracking, movement data, trajectory analysis","AminerCitationCount_02-2020":93,"AminerCitationCount_06-2020":115,"XploreCitationCount - 2020-01":80,"PubsCited":33,"Award":"BP","image":"6327295-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Visual Classifier Training for Text Document Retrieval","DOI":"10.1109/TVCG.2012.277","Link":"http://dx.doi.org/10.1109/TVCG.2012.277","FirstPage":2839,"LastPage":2848,"PaperType":"J","Abstract":"Performing exhaustive searches over a large number of text documents can be tedious, since it is very hard to formulate search queries or define filter criteria that capture an analyst's information need adequately. Classification through machine learning has the potential to improve search and filter tasks encompassing either complex or very specific information needs, individually. Unfortunately, analysts who are knowledgeable in their field are typically not machine learning specialists. Most classification methods, however, require a certain expertise regarding their parametrization to achieve good results. Supervised machine learning algorithms, in contrast, rely on labeled data, which can be provided by analysts. However, the effort for labeling can be very high, which shifts the problem from composing complex queries or defining accurate filters to another laborious task, in addition to the need for judging the trained classifier's quality. We therefore compare three approaches for interactive classifier training in a user study. All of the approaches are potential candidates for the integration into a larger retrieval system. They incorporate active learning to various degrees in order to reduce the labeling effort as well as to increase effectiveness. Two of them encompass interactive visualization for letting users explore the status of the classifier in context of the labeled documents, as well as for judging the quality of the classifier in iterative feedback loops. We see our work as a step towards introducing user controlled classification methods in addition to text search and filtering for increasing recall in analytics scenarios involving large corpora.","AuthorNames-Deduped":"Florian Heimerl;Steffen Koch;Harald Bosch;Thomas Ertl","AuthorNames":"Florian Heimerl;Steffen Koch;Harald Bosch;Thomas Ertl","AuthorAffiliation":"Universoty of Stuttgart;Universoty of Stuttgart;Universoty of Stuttgart;Universoty of Stuttgart","InternalReferences":"10.1109/VAST.2011.6102449;10.1109/VAST.2011.6102453;10.1109/VAST.2007.4389006;10.1109/VAST.2012.6400492","AuthorKeywords":"Visual analytics, human computer interaction, information retrieval, active learning, classification, user evaluation","AminerCitationCount_02-2020":58,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":59,"PubsCited":48,"Award":null,"image":"6327290-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2012,"Title":"Relative N-gram signatures: Document visualization at the level of character N-grams","DOI":"10.1109/VAST.2012.6400484","Link":"http://dx.doi.org/10.1109/VAST.2012.6400484","FirstPage":103,"LastPage":112,"PaperType":"C","Abstract":"The Common N-Gram (CNG) classifier is a text classification algorithm based on the comparison of frequencies of character n-grams (strings of characters of length n) that are the most common in the considered documents and classes of documents. We present a text analytic visualization system that employs the CNG approach for text classification and uses the differences in frequency values of common n-grams in order to visually compare documents at the sub-word level. The visualization method provides both an insight into n-gram characteristics of documents or classes of documents and a visual interpretation of the workings of the CNG classifier.","AuthorNames-Deduped":"Magdalena Jankowska;Vlado Keselj;Evangelos E. Milios","AuthorNames":"Magdalena Jankowska;Vlado Kešelj;Evangelos Milios","AuthorAffiliation":"Faculty of Computer Science, Dalhousie University;Faculty of Computer Science, Dalhousie University;Faculty of Computer Science, Dalhousie University","InternalReferences":"10.1109/VAST.2009.5333443;10.1109/VAST.2007.4389004","AuthorKeywords":"Visual analytics, visual text analysis, text classification","AminerCitationCount_02-2020":7,"AminerCitationCount_06-2020":8,"XploreCitationCount - 2020-01":5,"PubsCited":36,"Award":null,"image":"6400484-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"LeadLine: Interactive visual analysis of text data through event identification and exploration","DOI":"10.1109/VAST.2012.6400485","Link":"http://dx.doi.org/10.1109/VAST.2012.6400485","FirstPage":93,"LastPage":102,"PaperType":"C","Abstract":"Text data such as online news and microblogs bear valuable insights regarding important events and responses to such events. Events are inherently temporal, evolving over time. Existing visual text analysis systems have provided temporal views of changes based on topical themes extracted from text data. But few have associated topical themes with events that cause the changes. In this paper, we propose an interactive visual analytics system, LeadLine, to automatically identify meaningful events in news and social media data and support exploration of the events. To characterize events, LeadLine integrates topic modeling, event detection, and named entity recognition techniques to automatically extract information regarding the investigative 4 Ws: who, what, when, and where for each event. To further support analysis of the text corpora through events, LeadLine allows users to interactively examine meaningful events using the 4 Ws to develop an understanding of how and why. Through representing large-scale text corpora in the form of meaningful events, LeadLine provides a concise summary of the corpora. LeadLine also supports the construction of simple narratives through the exploration of events. To demonstrate the efficacy of LeadLine in identifying events and supporting exploration, two case studies were conducted using news and social media data.","AuthorNames-Deduped":"Wenwen Dou;Xiaoyu Wang;Drew Skau;William Ribarsky;Michelle X. Zhou","AuthorNames":"Wenwen Dou;Xiaoyu Wang;Drew Skau;William Ribarsky;Michelle X. Zhou","AuthorAffiliation":"University of North Carolina at Charlotte;University of North Carolina at Charlotte;University of North Carolina at Charlotte;University of North Carolina at Charlotte;IBM Almaden Research Center","InternalReferences":"10.1109/VAST.2011.6102456;10.1109/VAST.2010.5652931;10.1109/TVCG.2011.179;10.1109/TVCG.2011.239;10.1109/VAST.2011.6102461;10.1109/TVCG.2011.185;10.1109/TVCG.2010.179;10.1109/VAST.2007.4389006;10.1109/INFVIS.2000.885098","AuthorKeywords":null,"AminerCitationCount_02-2020":87,"AminerCitationCount_06-2020":117,"XploreCitationCount - 2020-01":76,"PubsCited":43,"Award":null,"image":"6400485-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2012,"Title":"Dis-function: Learning distance functions interactively","DOI":"10.1109/VAST.2012.6400486","Link":"http://dx.doi.org/10.1109/VAST.2012.6400486","FirstPage":83,"LastPage":92,"PaperType":"C","Abstract":"The world's corpora of data grow in size and complexity every day, making it increasingly difficult for experts to make sense out of their data. Although machine learning offers algorithms for finding patterns in data automatically, they often require algorithm-specific parameters, such as an appropriate distance function, which are outside the purview of a domain expert. We present a system that allows an expert to interact directly with a visual representation of the data to define an appropriate distance function, thus avoiding direct manipulation of obtuse model parameters. Adopting an iterative approach, our system first assumes a uniformly weighted Euclidean distance function and projects the data into a two-dimensional scatterplot view. The user can then move incorrectly-positioned data points to locations that reflect his or her understanding of the similarity of those data points relative to the other data points. Based on this input, the system performs an optimization to learn a new distance function and then re-projects the data to redraw the scatter-plot. We illustrate empirically that with only a few iterations of interaction and optimization, a user can achieve a scatterplot view and its corresponding distance function that reflect the user's knowledge of the data. In addition, we evaluate our system to assess scalability in data size and data dimension, and show that our system is computationally efficient and can provide an interactive or near-interactive user experience.","AuthorNames-Deduped":"Eli T. Brown;Jingjing Liu;Carla E. Brodley;Remco Chang","AuthorNames":"Eli T. Brown;Jingjing Liu;Carla E. Brodley;Remco Chang","AuthorAffiliation":"Department of Computer Science Tufts University;Department of Computer Science Tufts University;Department of Computer Science Tufts University;Department of Computer Science Tufts University","InternalReferences":"10.1109/VISUAL.1990.146402;10.1109/VAST.2011.6102449;10.1109/VAST.2007.4388999;10.1109/VAST.2009.5332584;10.1109/VAST.2011.6102448;10.1109/VAST.2008.4677352;10.1109/VAST.2010.5652443","AuthorKeywords":null,"AminerCitationCount_02-2020":79,"AminerCitationCount_06-2020":108,"XploreCitationCount - 2020-01":70,"PubsCited":40,"Award":null,"image":"6400486-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Just-in-time annotation of clusters, outliers, and trends in point-based data visualizations","DOI":"10.1109/VAST.2012.6400487","Link":"http://dx.doi.org/10.1109/VAST.2012.6400487","FirstPage":73,"LastPage":82,"PaperType":"C","Abstract":"We introduce the concept of just-in-time descriptive analytics as a novel application of computational and statistical techniques performed at interaction-time to help users easily understand the structure of data as seen in visualizations. Fundamental to just-intime descriptive analytics is (a) identifying visual features, such as clusters, outliers, and trends, user might observe in visualizations automatically, (b) determining the semantics of such features by performing statistical analysis as the user is interacting, and (c) enriching visualizations with annotations that not only describe semantics of visual features but also facilitate interaction to support high-level understanding of data. In this paper, we demonstrate just-in-time descriptive analytics applied to a point-based multi-dimensional visualization technique to identify and describe clusters, outliers, and trends. We argue that it provides a novel user experience of computational techniques working alongside of users allowing them to build faster qualitative mental models of data by demonstrating its application on a few use-cases. Techniques used to facilitate just-in-time descriptive analytics are described in detail along with their runtime performance characteristics. We believe this is just a starting point and much remains to be researched, as we discuss open issues and opportunities in improving accessibility and collaboration.","AuthorNames-Deduped":"Eser Kandogan","AuthorNames":"Eser Kandogan","AuthorAffiliation":"IBM Center for Advanced Visualization, IBM Research","InternalReferences":"10.1109/INFVIS.2003.1249015;10.1109/INFVIS.2005.1532142;10.1109/INFVIS.2004.3;10.1109/TVCG.2011.220;10.1109/INFVIS.2004.15;10.1109/INFVIS.1998.729559;10.1109/VAST.2006.261423;10.1109/TVCG.2009.153;10.1109/VAST.2010.5652885;10.1109/VAST.2009.5332628;10.1109/TVCG.2011.229","AuthorKeywords":"Just-in-time descriptive analytics, feature identification and characterization, point-based visualizations","AminerCitationCount_02-2020":35,"AminerCitationCount_06-2020":42,"XploreCitationCount - 2020-01":25,"PubsCited":41,"Award":null,"image":"6400487-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Subspace search and visualization to make sense of alternative clusterings in high-dimensional data","DOI":"10.1109/VAST.2012.6400488","Link":"http://dx.doi.org/10.1109/VAST.2012.6400488","FirstPage":63,"LastPage":72,"PaperType":"C","Abstract":"In explorative data analysis, the data under consideration often resides in a high-dimensional (HD) data space. Currently many methods are available to analyze this type of data. So far, proposed automatic approaches include dimensionality reduction and cluster analysis, whereby visual-interactive methods aim to provide effective visual mappings to show, relate, and navigate HD data. Furthermore, almost all of these methods conduct the analysis from a singular perspective, meaning that they consider the data in either the original HD data space, or a reduced version thereof. Additionally, HD data spaces often consist of combined features that measure different properties, in which case the particular relationships between the various properties may not be clear to the analysts a priori since it can only be revealed if appropriate feature combinations (subspaces) of the data are taken into consideration. Considering just a single subspace is, however, often not sufficient since different subspaces may show complementary, conjointly, or contradicting relations between data items. Useful information may consequently remain embedded in sets of subspaces of a given HD input data space. Relying on the notion of subspaces, we propose a novel method for the visual analysis of HD data in which we employ an interestingness-guided subspace search algorithm to detect a candidate set of subspaces. Based on appropriately defined subspace similarity functions, we visualize the subspaces and provide navigation facilities to interactively explore large sets of subspaces. Our approach allows users to effectively compare and relate subspaces with respect to involved dimensions and clusters of objects. We apply our approach to synthetic and real data sets. We thereby demonstrate its support for understanding HD data from different perspectives, effectively yielding a more complete view on HD data.","AuthorNames-Deduped":"Andrada Tatu;Fabian Maass;Ines Färber;Enrico Bertini;Tobias Schreck;Thomas Seidl 0001;Daniel A. Keim","AuthorNames":"Andrada Tatu;Fabian Maaß;Ines Färber;Enrico Bertini;Tobias Schreck;Thomas Seidl;Daniel Keim","AuthorAffiliation":"University of Konstanz, Germany;University of Konstanz, Germany;RWTH Aachen University Germany;University of Konstanz, Germany;University of Konstanz, Germany;RWTH Aachen University Germany;University of Konstanz Germany","InternalReferences":"10.1109/INFVIS.2005.1532142;10.1109/TVCG.2010.138;10.1109/VAST.2010.5652392;10.1109/INFVIS.2004.71;10.1109/VAST.2010.5652450;10.1109/VAST.2011.6102439;10.1109/TVCG.2011.188;10.1109/INFVIS.2004.3;10.1109/TVCG.2009.153","AuthorKeywords":null,"AminerCitationCount_02-2020":46,"AminerCitationCount_06-2020":60,"XploreCitationCount - 2020-01":36,"PubsCited":35,"Award":null,"image":"6400488-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"iLAMP: Exploring high-dimensional spacing through backward multidimensional projection","DOI":"10.1109/VAST.2012.6400489","Link":"http://dx.doi.org/10.1109/VAST.2012.6400489","FirstPage":53,"LastPage":62,"PaperType":"C","Abstract":"Ever improving computing power and technological advances are greatly augmenting data collection and scientific observation. This has directly contributed to increased data complexity and dimensionality, motivating research of exploration techniques for multidimensional data. Consequently, a recent influx of work dedicated to techniques and tools that aid in understanding multidimensional datasets can be observed in many research fields, including biology, engineering, physics and scientific computing. While the effectiveness of existing techniques to analyze the structure and relationships of multidimensional data varies greatly, few techniques provide flexible mechanisms to simultaneously visualize and actively explore high-dimensional spaces. In this paper, we present an inverse linear affine multidimensional projection, coined iLAMP, that enables a novel interactive exploration technique for multidimensional data. iLAMP operates in reverse to traditional projection methods by mapping low-dimensional information into a high-dimensional space. This allows users to extrapolate instances of a multidimensional dataset while exploring a projection of the data to the planar domain. We present experimental results that validate iLAMP, measuring the quality and coherence of the extrapolated data; as well as demonstrate the utility of iLAMP to hypothesize the unexplored regions of a high-dimensional space.","AuthorNames-Deduped":"Elisa Portes dos Santos;Emilio Vital Brazil;Joel Daniels II;Paulo Joia;Luis Gustavo Nonato;Mario Costa Sousa","AuthorNames":"Elisa Portes dos Santos Amorim;Emilio Vital Brazil;Joel Daniels;Paulo Joia;Luis Gustavo Nonato;Mario Costa Sousa","AuthorAffiliation":"University of Calgary;University of Calgary;NYU Polytechnic Institute;University of Sao Paulo;University of Sao Paulo;University of Calgary","InternalReferences":"10.1109/INFVIS.2005.1532138;10.1109/TVCG.2008.116;10.1109/TVCG.2010.213;10.1109/TVCG.2009.140;10.1109/TVCG.2011.220;10.1109/VISUAL.1999.809866;10.1109/TVCG.2006.170;10.1109/INFVIS.2000.885086;10.1109/INFVIS.2004.15;10.1109/TVCG.2008.153;10.1109/INFVIS.2002.1173159;10.1109/INFVIS.2003.1249015;10.1109/VISUAL.1990.146402;10.1109/VISUAL.1996.567787;10.1109/TVCG.2010.170;10.1109/TVCG.2007.70580;10.1109/TVCG.2010.207;10.1109/INFVIS.2002.1173161","AuthorKeywords":null,"AminerCitationCount_02-2020":13,"AminerCitationCount_06-2020":19,"XploreCitationCount - 2020-01":8,"PubsCited":51,"Award":null,"image":"6400489-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Visual pattern discovery using random projections","DOI":"10.1109/VAST.2012.6400490","Link":"http://dx.doi.org/10.1109/VAST.2012.6400490","FirstPage":43,"LastPage":52,"PaperType":"C","Abstract":"An essential element of exploratory data analysis is the use of revealing low-dimensional projections of high-dimensional data. Projection Pursuit has been an effective method for finding interesting low-dimensional projections of multidimensional spaces by optimizing a score function called a projection pursuit index. However, the technique is not scalable to high-dimensional spaces. Here, we introduce a novel method for discovering noteworthy views of high-dimensional data spaces by using binning and random projections. We define score functions, akin to projection pursuit indices, that characterize visual patterns of the low-dimensional projections that constitute feature subspaces. We also describe an analytic, multivariate visualization platform based on this algorithm that is scalable to extremely large problems.","AuthorNames-Deduped":"Anushka Anand;Leland Wilkinson;Tommy Dang","AuthorNames":"Anushka Anand;Leland Wilkinson;Tuan Nhon Dang","AuthorAffiliation":"Department of Computer Science, University of Illinois at Chicago;Department of Computer Science, University of Illinois at Chicago;Department of Computer Science, University of Illinois at Chicago","InternalReferences":"10.1109/VAST.2010.5652433;10.1109/TVCG.2011.178;10.1109/VAST.2011.6102437;10.1109/VAST.2007.4389006;10.1109/VAST.2010.5652392;10.1109/INFVIS.2005.1532142;10.1109/VAST.2009.5332629","AuthorKeywords":"Random Projections, High-dimensional Data","AminerCitationCount_02-2020":17,"AminerCitationCount_06-2020":25,"XploreCitationCount - 2020-01":16,"PubsCited":49,"Award":null,"image":"6400490-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"A correlative analysis process in a visual analytics environment","DOI":"10.1109/VAST.2012.6400491","Link":"http://dx.doi.org/10.1109/VAST.2012.6400491","FirstPage":33,"LastPage":42,"PaperType":"C","Abstract":"Finding patterns and trends in spatial and temporal datasets has been a long studied problem in statistics and different domains of science. This paper presents a visual analytics approach for the interactive exploration and analysis of spatiotemporal correlations among multivariate datasets. Our approach enables users to discover correlations and explore potentially causal or predictive links at different spatiotemporal aggregation levels among the datasets, and allows them to understand the underlying statistical foundations that precede the analysis. Our technique utilizes the Pearson's product-moment correlation coefficient and factors in the lead or lag between different datasets to detect trends and periodic patterns amongst them.","AuthorNames-Deduped":"Abish Malik;Ross Maciejewski;Niklas Elmqvist;Yun Jang;David S. Ebert;Whitney Huang","AuthorNames":"Abish Malik;Ross Maciejewski;Niklas Elmqvist;Yun Jang;David S. Ebert;Whitney Huang","AuthorAffiliation":"Purdue University, USA;Arizona State University, USA;Purdue University, USA;Sejong University, South Korea;Purdue University, USA;Purdue University, USA","InternalReferences":"10.1109/INFVIS.2005.1532148;10.1109/TVCG.2011.179;10.1109/TVCG.2010.193;10.1109/INFVIS.1999.801851;10.1109/INFVIS.1999.801851;10.1109/VAST.2007.4389006;10.1109/TVCG.2007.70539;10.1109/TVCG.2010.162;10.1109/TVCG.2011.195","AuthorKeywords":"Visual analytics, correlative analysis","AminerCitationCount_02-2020":21,"AminerCitationCount_06-2020":26,"XploreCitationCount - 2020-01":19,"PubsCited":34,"Award":null,"image":"6400491-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2012,"Title":"Inter-active learning of ad-hoc classifiers for video visual analytics","DOI":"10.1109/VAST.2012.6400492","Link":"http://dx.doi.org/10.1109/VAST.2012.6400492","FirstPage":23,"LastPage":32,"PaperType":"C","Abstract":"Learning of classifiers to be used as filters within the analytical reasoning process leads to new and aggravates existing challenges. Such classifiers are typically trained ad-hoc, with tight time constraints that affect the amount and the quality of annotation data and, thus, also the users' trust in the classifier trained. We approach the challenges of ad-hoc training by inter-active learning, which extends active learning by integrating human experts' background knowledge to greater extent. In contrast to active learning, not only does inter-active learning include the users' expertise by posing queries of data instances for labeling, but it also supports the users in comprehending the classifier model by visualization. Besides the annotation of manually or automatically selected data instances, users are empowered to directly adjust complex classifier models. Therefore, our model visualization facilitates the detection and correction of inconsistencies between the classifier model trained by examples and the user's mental model of the class definition. Visual feedback of the training process helps the users assess the performance of the classifier and, thus, build up trust in the filter created. We demonstrate the capabilities of inter-active learning in the domain of video visual analytics and compare its performance with the results of random sampling and uncertainty sampling of training sets.","AuthorNames-Deduped":"Benjamin Höferlin;Rudolf Netzel;Markus Höferlin;Daniel Weiskopf;Gunther Heidemann","AuthorNames":"Benjamin Höferlin;Rudolf Netzel;Markus Höferlin;Daniel Weiskopf;Gunther Heidemann","AuthorAffiliation":"Institute of Cognitive Science, University of Osnabrück;Visualization Research Center (VISUS), University of Stuttgart;Visualization Research Center (VISUS), University of Stuttgart;Visualization Research Center (VISUS), University of Stuttgart;Institute of Cognitive Science, University of Osnabrück","InternalReferences":"10.1109/VAST.2010.5652398;10.1109/TVCG.2012.277","AuthorKeywords":null,"AminerCitationCount_02-2020":22,"AminerCitationCount_06-2020":33,"XploreCitationCount - 2020-01":14,"PubsCited":43,"Award":null,"image":"6400492-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"An adaptive parameter space-filling algorithm for highly interactive cluster exploration","DOI":"10.1109/VAST.2012.6400493","Link":"http://dx.doi.org/10.1109/VAST.2012.6400493","FirstPage":13,"LastPage":22,"PaperType":"C","Abstract":"For a user to perceive continuous interactive response time in a visualization tool, the rule of thumb is that it must process, deliver, and display rendered results for any given interaction in under 100 milliseconds. In many visualization systems, successive interactions trigger independent queries and caching of results. Consequently, computationally expensive queries like multidimensional clustering cannot keep up with rapid sequences of interactions, precluding visual benefits such as motion parallax. In this paper, we describe a heuristic prefetching technique to improve the interactive response time of KMeans clustering in dynamic query visualizations of multidimensional data. We address the tradeoff between high interaction and intense query computation by observing how related interactions on overlapping data subsets produce similar clustering results, and characterizing these similarities within a parameter space of interaction. We focus on the two-dimensional parameter space defined by the minimum and maximum values of a time range manipulated by dragging and stretching a one-dimensional filtering lens over a plot of time series data. Using calculation of nearest neighbors of interaction points in parameter space, we reuse partial query results from prior interaction sequences to calculate both an immediate best-effort clustering result and to schedule calculation of an exact result. The method adapts to user interaction patterns in the parameter space by reprioritizing the interaction neighbors of visited points in the parameter space. A performance study on Mesonet meteorological data demonstrates that the method is a significant improvement over the baseline scheme in which interaction triggers on-demand, exact-range clustering with LRU caching. We also present initial evidence that approximate, temporary clustering results are sufficiently accurate (compared to exact results) to convey useful cluster structure during rapid and protracted interaction.","AuthorNames-Deduped":"Zafar Ahmed;Chris Weaver","AuthorNames":"Zafar Ahmed;Chris Weaver","AuthorAffiliation":"School of Computer Science and Center for Spatial Analysis The University of Oklahoma;School of Computer Science and Center for Spatial Analysis The University of Oklahoma","InternalReferences":"10.1109/TVCG.2007.70515;10.1109/INFVIS.2004.12;10.1109/VAST.2009.5332629;10.1109/VAST.2008.4677357;10.1109/TVCG.2011.188;10.1109/VISUAL.1994.346302;10.1109/INFVIS.1998.729559;10.1109/VAST.2007.4388999","AuthorKeywords":null,"AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":7,"XploreCitationCount - 2020-01":4,"PubsCited":31,"Award":null,"image":"6400493-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2012,"Title":"Visual cluster exploration of web clickstream data","DOI":"10.1109/VAST.2012.6400494","Link":"http://dx.doi.org/10.1109/VAST.2012.6400494","FirstPage":3,"LastPage":12,"PaperType":"C","Abstract":"Web clickstream data are routinely collected to study how users browse the web or use a service. It is clear that the ability to recognize and summarize user behavior patterns from such data is valuable to e-commerce companies. In this paper, we introduce a visual analytics system to explore the various user behavior patterns reflected by distinct clickstream clusters. In a practical analysis scenario, the system first presents an overview of clickstream clusters using a Self-Organizing Map with Markov chain models. Then the analyst can interactively explore the clusters through an intuitive user interface. He can either obtain summarization of a selected group of data or further refine the clustering result. We evaluated our system using two different datasets from eBay. Analysts who were working on the same data have confirmed the system's effectiveness in extracting user behavior patterns from complex datasets and enhancing their ability to reason.","AuthorNames-Deduped":"Jishang Wei;Zeqian Shen;Neel Sundaresan;Kwan-Liu Ma","AuthorNames":"Jishang Wei;Zeqian Shen;Neel Sundaresan;Kwan-Liu Ma","AuthorAffiliation":"University of California, Davis;eBay Research Labs;eBay Research Labs;University of California, Davis","InternalReferences":"10.1109/INFVIS.2005.1532145;10.1109/VAST.2007.4389008;10.1109/VAST.2011.6102462;10.1109/VISUAL.1991.175815","AuthorKeywords":null,"AminerCitationCount_02-2020":44,"AminerCitationCount_06-2020":56,"XploreCitationCount - 2020-01":33,"PubsCited":28,"Award":null,"image":"6400494-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"LensingWikipedia: Parsing text for the interactive visualization of human history","DOI":"10.1109/VAST.2012.6400530","Link":"http://dx.doi.org/10.1109/VAST.2012.6400530","FirstPage":247,"LastPage":248,"PaperType":"M","Abstract":"Extracting information from text is challenging. Most current practices treat text as a bag of words or word clusters, ignoring valuable linguistic information. Leveraging this linguistic information, we propose a novel approach to visualize textual information. The novelty lies in using state-of-the-art Natural Language Processing (NLP) tools to automatically annotate text which provides a basis for new and powerful interactive visualizations. Using NLP tools, we built a web-based interactive visual browser for human history articles from Wikipedia.","AuthorNames-Deduped":"Ravikiran Vadlapudi;Maryam Siahbani;Anoop Sarkar;John Dill","AuthorNames":"Ravikiran Vadlapudi;Maryam Siahbani;Anoop Sarkar;John Dill","AuthorAffiliation":"Simon Fraser University;Simon Fraser University;Simon Fraser University;Simon Fraser University","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":4,"Award":null,"image":"6400530-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"VDQAM: A toolkit for database quality evaluation based on visual morphology","DOI":"10.1109/VAST.2012.6400531","Link":"http://dx.doi.org/10.1109/VAST.2012.6400531","FirstPage":245,"LastPage":246,"PaperType":"M","Abstract":"Data quality evaluation is one of the most critical steps during the data mining processes. Data with poor quality often leads to poor performance in data mining, low efficiency in data analysis, wrong decision which bring great economic loss to users and organizations further. Although many researches have been carried out from various aspects of the extracting, transforming, and loading processes in data mining, most researches pay more attention to analysis automation than to data quality evaluation. To address the data quality evaluation issues, we propose an approach to combine human beings' powerful cognitive abilities in data quality evaluation with the high efficiency ability of computer, and develop a visual analysis method for data quality evaluation based on visual morphology.","AuthorNames-Deduped":"Dongxing Teng;Haiyan Yang;CuiXia Ma;Hongan Wang","AuthorNames":"Dongxing Teng;Haiyan Yang;Cuixia Ma;Hongan Wang","AuthorAffiliation":"Institute of Software, Chinese Academy of Sciences, Beijing, China;Institute of Software, Chinese Academy of Sciences, Beijing, China;Institute of Software, Chinese Academy of Sciences, Beijing, China;Institute of Software, Chinese Academy of Sciences, Beijing, China","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":7,"Award":null,"image":"6400531-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2012,"Title":"A case study: Tracking and visualizing the evolution of dark matter halos and groups of satellite halos in cosmology simulations","DOI":"10.1109/VAST.2012.6400532","Link":"http://dx.doi.org/10.1109/VAST.2012.6400532","FirstPage":243,"LastPage":244,"PaperType":"M","Abstract":"In this poster, we track the evolution of cosmic structures and higher level host structures in cosmological simulation as they interact with each other. The structures found in these simulations are made up of groups of dark matter tracer particles called satellite halos and groups of satellite halos called host halos. We implement a multilevel tracking model to track dark matter tracer particles, satellite halos and host halos to understand their behaviour and show how the different structures are formed over time. We also represent the evolution of halos in the form of merger trees for detailed analysis by cosmologists.","AuthorNames-Deduped":"Jay Takle;Deborah Silver;Katrin Heitmann","AuthorNames":"Jay Takle;Deborah Silver;Katrin Heitmann","AuthorAffiliation":"Department of Electrical & Computer Engineering, Rutgers University;Department of Electrical & Computer Engineering, Rutgers University;High Energy Physics Division, Argonne National Laboratory","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":5,"XploreCitationCount - 2020-01":5,"PubsCited":7,"Award":null,"image":""},{"Conference":"VAST","Year":2012,"Title":"Infographics at the Congressional Budget Office","DOI":"10.1109/VAST.2012.6400533","Link":"http://dx.doi.org/10.1109/VAST.2012.6400533","FirstPage":241,"LastPage":242,"PaperType":"M","Abstract":"The Congressional Budget Office (CBO) is an agency of the federal government with about 240 employees that provides the U.S. Congress with timely, nonpartisan analysis of important budgetary and economic issues. Recently, CBO began producing static infographics to present its headline stories and to provide information to the Congress in different ways.","AuthorNames-Deduped":"Jonathan A. Schwabish","AuthorNames":"Jonathan A. Schwabish","AuthorAffiliation":"Congressional Budget Office","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":2,"Award":null,"image":"6400533-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Visual exploration of local interest points in sets of time series","DOI":"10.1109/VAST.2012.6400534","Link":"http://dx.doi.org/10.1109/VAST.2012.6400534","FirstPage":239,"LastPage":240,"PaperType":"M","Abstract":"Visual analysis of time series data is an important, yet challenging task with many application examples in fields such as financial or news stream data analysis. Many visual time series analysis approaches consider a global perspective on the time series. Fewer approaches consider visual analysis of local patterns in time series, and often rely on interactive specification of the local area of interest. We present initial results of an approach that is based on automatic detection of local interest points. We follow an overview-first approach to find useful parameters for the interest point detection, and details-on-demand to relate the found patterns. We present initial results and detail possible extensions of the approach.","AuthorNames-Deduped":"Tobias Schreck;Lyubka Sharalieva;Franz Wanner;Jürgen Bernard;Tobias Ruppert;Tatiana von Landesberger;Benjamin Bustos","AuthorNames":"Tobias Schreck;Lyubka Sharalieva;Franz Wanner;Jürgen Bernard;Tobias Ruppert;Tatiana von Landesberger;Benjamin Bustos","AuthorAffiliation":"University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;Fraunhofer IGD Darmstadt, Germany;Fraunhofer IGD Darmstadt, Germany;TU Darmstadt Germany;Universidad de Chile Santiago de Chile","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":5,"XploreCitationCount - 2020-01":2,"PubsCited":8,"Award":null,"image":"6400534-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Priming Locus of Control to affect performance","DOI":"10.1109/VAST.2012.6400535","Link":"http://dx.doi.org/10.1109/VAST.2012.6400535","FirstPage":237,"LastPage":238,"PaperType":"M","Abstract":"Recent research suggests that the personality trait Locus of Control (LOC) can be a reliable predictor of performance when learning a new visualization tool. While these results are compelling and have direct implications to visualization design, the relationship between a user's LOC measure and their performance is not well understood. We hypothesize that there is a dependent relationship between LOC and performance; specifically, a person's orientation on the LOC scale directly influences their performance when learning new visualizations. To test this hypothesis, we conduct an experiment with 300 subjects using Amazon's Mechanical Turk. We adapt techniques from personality psychology to manipulate a user's LOC so that users are either primed to be more internally or externally oriented on the LOC scale. Replicating previous studies investigating the effect of LOC on performance, we measure users' speed and accuracy as they use visualizations with varying visual metaphors. Our findings demonstrate that changing a user's LOC impacts their performance. We find that a change in users' LOC results in performance changes.","AuthorNames-Deduped":"Alvitta Ottley;R. Jordan Crouser;Caroline Ziemkiewicz;Remco Chang","AuthorNames":"Alvitta Ottley;R. Jordan Crouser;Caroline Ziemkiewicz;Remco Chang","AuthorAffiliation":"Tufts University;Tufts University;Brown University;Tufts University","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":4,"Award":null,"image":"6400535-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"The spatiotemporal multivariate hypercube for discovery of patterns in event data","DOI":"10.1109/VAST.2012.6400536","Link":"http://dx.doi.org/10.1109/VAST.2012.6400536","FirstPage":235,"LastPage":236,"PaperType":"M","Abstract":"Event data can hold valuable decision making information, yet detecting interesting patterns in this type of data is not an easy task because the data is usually rich and contains spatial, temporal as well as multivariate dimensions. Research into visual analytics tools to support the discovery of patterns in event data often focuses on the spatiotemporal or spatiomultivariate dimension of the data only. Few research efforts focus on all three dimensions in one framework. An integral view on all three dimensions is, however, required to unlock the full potential of event datasets. In this poster, we present an event visualization, transition, and interaction framework that enables an integral view on all dimensions of spatiotemporal multivariate event data. The framework is built around the notion that the event data space can be considered a spatiotemporal multivariate hypercube. Results of a case study we performed suggest that a visual analytics tool based on the proposed framework is indeed capable to support users in the discovery of multidimensional spatiotemporal multivariate patterns in event data.","AuthorNames-Deduped":"Fred Olislagers;Marcel Worring","AuthorNames":"Fred Olislagers;Marcel Worring","AuthorAffiliation":"Intelligent Systems Lab Amsterdam, University of Amsterdam, The Netherlands;Intelligent Systems Lab Amsterdam, University of Amsterdam, The Netherlands","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":3,"Award":null,"image":"6400536-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"A generic model for the integration of interactive visualization and statistical computing using R","DOI":"10.1109/VAST.2012.6400537","Link":"http://dx.doi.org/10.1109/VAST.2012.6400537","FirstPage":233,"LastPage":234,"PaperType":"M","Abstract":"This poster describes general concepts of integrating the statistical computation package R into a coordinated multiple views framework. The integration is based on a cyclic analysis workflow. In this model, interactive selections are a key aspect to trigger and control computations in R. Dynamic updates of data columns are a generic mechanism to transfer computational results back to the interactive visualization. Further aspects include the integration of the R console and an R object browser as views in our system. We illustrate our approach by means of an interactive modeling process.","AuthorNames-Deduped":"Johannes Kehrer;Roland N. Boubela;Peter Filzmoser;Harald Piringer","AuthorNames":"Johannes Kehrer;Roland N. Boubela;Peter Filzmoser;Harald Piringer","AuthorAffiliation":"VRVis Research Center, Vienna, Austria;Dept. of Statistics and Probability Theory, Vienna University of Technology, Austria;Dept. of Statistics and Probability Theory, Vienna University of Technology, Austria;VRVis Research Center, Vienna, Austria","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":6,"AminerCitationCount_06-2020":7,"XploreCitationCount - 2020-01":6,"PubsCited":7,"Award":null,"image":"6400537-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Using visual analytics to detect problems in datasets collected from photo-sharing services","DOI":"10.1109/VAST.2012.6400538","Link":"http://dx.doi.org/10.1109/VAST.2012.6400538","FirstPage":231,"LastPage":232,"PaperType":"M","Abstract":"Datasets that are collected for research often contain millions of records and may carry hidden pitfalls that are hard to detect. This work demonstrates how visual analytics can be used for identifying problems in the spatial distribution of crawled photographic data in different datasets: Picasa Web Albums, Panoramio, Flickr and Geograph, chosen to be potential data sources for ongoing doctoral research. This poster summary describes a number of problems found in the datasets using visual analytics and suggests that greater attention should be paid to assessing the quality of data gathered from user-generated photographic content. This work is the first part of a three-year PhD project aimed at producing a pedestrian-routing system that can suggest attractive pathways extracted from user-generated photographic content.","AuthorNames-Deduped":"Alexander Kachkaev;Jo Wood","AuthorNames":"Alexander Kachkaev;Jo Wood","AuthorAffiliation":"giCentre, City University London;giCentre, City University London","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":7,"Award":null,"image":"6400538-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Visualizing flows of images in social media","DOI":"10.1109/VAST.2012.6400539","Link":"http://dx.doi.org/10.1109/VAST.2012.6400539","FirstPage":229,"LastPage":230,"PaperType":"M","Abstract":"Mass and social media provide flows of images for real world events. It is sometimes difficult to represent realities and impressions of events using only text. However, even a single photo might remind us complex events. Along with events in the real world, there are representative images, such as design of products and commercial pictures. We can therefore recognize changes in trends of people's ideas, experiences, and interests through observing the flows of such representative images. This paper presents a novel 3D visualization system to explore temporal changes in trends using images associating with different topics, called Image Bricks. We show case studies using images extracted from our six-year blog archive. We first extract clusters of images as topics related to given keywords. We then visualize them on multiple timelines in a 3D space. Users can visually read stories of topics through exploring visualized images.","AuthorNames-Deduped":"Masahiko Itoh;Masashi Toyoda;Tetsuya Kamijo;Masaru Kitsuregawa","AuthorNames":"Masahiko Itoh;Masashi Toyoda;Tetsuya Kamijo;Masaru Kitsuregawa","AuthorAffiliation":"Institute of Industrial Science, University of Tokyo;Institute of Industrial Science, University of Tokyo;Rakuten, Inc.;Institute of Industrial Science, University of Tokyo","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":6,"Award":null,"image":"6400539-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Exploring the impact of emotion on visual judgement","DOI":"10.1109/VAST.2012.6400540","Link":"http://dx.doi.org/10.1109/VAST.2012.6400540","FirstPage":227,"LastPage":228,"PaperType":"M","Abstract":"Existing research suggests that individual personality differences can influence performance with visualizations. In addition to stable traits such as locus of control, research in psychology has found that temporary changes in affect (emotion) can significantly impact individual performance on cognitive tasks. We examine the relationship between fundamental visual judgement tasks and affect through a crowdsourced user study that combines affective-priming techniques from psychology with longstanding graphical perception experiments. Our results suggest that affective-priming can significantly influence accuracy in visual judgements, and that some chart types may be more affected than others.","AuthorNames-Deduped":"Lane Harrison;Remco Chang;Aidong Lu","AuthorNames":"Lane Harrison;Remco Chang;Aidong Lu","AuthorAffiliation":"UNC-Charlotte;Tufts University;UNC-Charlotte","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":5,"XploreCitationCount - 2020-01":1,"PubsCited":8,"Award":null,"image":"6400540-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Exploring cyber physical data streams using Radial Pixel Visualizations","DOI":"10.1109/VAST.2012.6400541","Link":"http://dx.doi.org/10.1109/VAST.2012.6400541","FirstPage":225,"LastPage":226,"PaperType":"M","Abstract":"Cyber physical systems (CPS), such as smart buildings and data centers, are richly instrumented systems composed of tightly coupled computational and physical elements that generate large amounts of data. To explore CPS data and obtain actionable insights, we construct a Radial Pixel Visualization (RPV) system, which uses multiple concentric rings to show the data in a compact circular layout of small polygons (pixel cells), each of which represents an individual data value. RPV provides an effective visual representation of locality and periodicity of the high volume, multivariate data streams, and seamlessly combines them with the results of an automated analysis. In the outermost ring the results of correlation analysis and peak point detection are highlighted. Our explorations demonstrates how RPV can help administrators to identify periodic thermal hot spots, understand data center energy consumption, and optimize IT workload.","AuthorNames-Deduped":"Ming C. Hao;Manish Marwah;Sebastian Mittelstädt;Halldór Janetzko;Daniel A. Keim;Umeshwar Dayal;Cullen Bash;Carlos J. Felix;Chandrakant D. Patel;Meichun Hsu;Yuan Chen 0001","AuthorNames":"M. Hao;Y. Chen;M. Marwah;S. Mittelstadt;H. Janetzko;D. Keim;U. Dayal;C. Bash;C. Felix;C. Patel;M. Hsu","AuthorAffiliation":"Hewlett-Packard Laboratories, Palo Alto, CA;Hewlett-Packard Laboratories, Palo Alto, CA;Hewlett-Packard Laboratories, Palo Alto, CA;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;Hewlett-Packard Laboratories, Palo Alto, CA;Hewlett-Packard Laboratories, Palo Alto, CA;Hewlett-Packard Laboratories, Palo Alto, CA;Hewlett-Packard Laboratories, Palo Alto, CA;Hewlett-Packard Laboratories, Palo Alto, CA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":2,"Award":null,"image":"6400541-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2012,"Title":"Incorporating GOMS analysis into the design of an EEG data visual analysis tool","DOI":"10.1109/VAST.2012.6400542","Link":"http://dx.doi.org/10.1109/VAST.2012.6400542","FirstPage":223,"LastPage":224,"PaperType":"M","Abstract":"In this paper, we present a case study where we incorporate GOMS (Goals, Operators, Methods, and Selectors) [2] task analysis into the design process of a visual analysis tool. We performed GOMS analysis on an Electroencephalography (EEG) analyst's current data analysis strategy to identify important user tasks and unnecessary user actions in his current workflow. We then designed an EEG data visual analysis tool based on the GOMS analysis result. Evaluation results show that the tool we have developed, EEGVis, allows the user to analyze EEG data with reduced subjective cognitive load, faster speed and increased confidence in the analysis quality. The positive evaluation results suggest that our design process demonstrates an effective application of GOMS analysis to discover opportunities for designing better tools to support the user's visual analysis process.","AuthorNames-Deduped":"Hua Guo;Diem Tran;David H. Laidlaw","AuthorNames":"Hua Guo;Diem Tran;David H. Laidlaw","AuthorAffiliation":"Department of Computer Science Brown University;Department of Computer Science Brown University;Department of Computer Science Brown University","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":1,"PubsCited":7,"Award":null,"image":"6400542-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Using translational science in visual analytics","DOI":"10.1109/VAST.2012.6400543","Link":"http://dx.doi.org/10.1109/VAST.2012.6400543","FirstPage":221,"LastPage":222,"PaperType":"M","Abstract":"We introduce translational science, a research discipline from medicine, and show how adapting it for visual analytics can improve the design and evaluation of visual analytics interfaces. Translational science “translates” knowledge from the lab to the real-world to “ground truth” by incorporating a 3 phase program of research. Phase 1 &amp; 2 include protocols for research in the lab and field and Phase 3 focuses on dissemination and documentation. We discuss these phases and how they may be applied to visual analytics research.","AuthorNames-Deduped":"Tera Marie Green;Brian D. Fisher","AuthorNames":"Tera Marie Green;Brian Fisher","AuthorAffiliation":"School of Interactive Arts + Science, Simon Fraser University;School of Interactive Arts + Science, Simon Fraser University","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":1,"PubsCited":7,"Award":null,"image":"6400543-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Optimizing an SPT-tree for visual analytics","DOI":"10.1109/VAST.2012.6400544","Link":"http://dx.doi.org/10.1109/VAST.2012.6400544","FirstPage":219,"LastPage":220,"PaperType":"M","Abstract":"Despite the extensive work done in the scientific visualization community on the creation and optimization of spatial data structures, there has been little adaptation of these structures in visual analytics and information visualization. In this work we present how we modify a space-partioning time (SPT) tree - a structure normally used in direct-volume rendering - for geospatial-temporal visualizations. We also present optimization techniques to improve the traversal speed of our structure through locational codes and bitwise comparisons. Finally, we present the results of an experiment that quantitatively evaluates our modified SPT tree with and without our optimizations. Our results indicate that retrieval was nearly three times faster when using our optimizations, and are consistent across multiple trials. Our finding could have implications for performance in using our modified SPT tree in large-scale geospatial temporal visual analytics software.","AuthorNames-Deduped":"Connor Gramazio;Remco Chang","AuthorNames":"Connor Gramazio;Remco Chang","AuthorAffiliation":"Department of Computer Science Brown University &amp; Tufts University;Department of Computer Science, Tufts University","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":0,"PubsCited":6,"Award":null,"image":"6400544-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Visualising variations in household energy consumption","DOI":"10.1109/VAST.2012.6400545","Link":"http://dx.doi.org/10.1109/VAST.2012.6400545","FirstPage":217,"LastPage":218,"PaperType":"M","Abstract":"There is limited understanding of the relationship between neighbourhoods, demographic characteristics and domestic energy consumption habits. We report upon research that combines datasets relating to household energy use with geodemographics to enable better understanding of UK energy user types. A novel interactive interface is planned to evaluate the performance of specifically created energy-based data classifications. The research aims to help local governments and the energy industry in targeting households and populations for new energy saving schemes and in improving efforts to promote sustainable energy consumption. The new classifications may also stimulate consumption awareness amongst domestic users. This poster reports on initial visual findings and describes the research methodology, data sources and future visualisation requirements.","AuthorNames-Deduped":"Sarah Goodwin;Jason Dykes","AuthorNames":"Sarah Goodwin;Jason Dykes","AuthorAffiliation":"giCentre, City University London, UK;giCentre, City University London, UK","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":6,"AminerCitationCount_06-2020":6,"XploreCitationCount - 2020-01":2,"PubsCited":10,"Award":null,"image":"6400545-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Time-oriented visualization and anticipation","DOI":"10.1109/VAST.2012.6400546","Link":"http://dx.doi.org/10.1109/VAST.2012.6400546","FirstPage":215,"LastPage":216,"PaperType":"M","Abstract":"Temporal awareness is pivotal to successful real-time dynamic decision making in a wide range of command and control situations; particularly in safety-critical environments. However, little explicit support for operators' temporal awareness is provided by decision support systems (DSS) for time-critical decisions. In the context of functional simulations of naval anti-air warfare and emergency response management, the present study compares operator support provided by two display formats. In both environments, we contrast a baseline condition to a condition in which a temporal display was integrated to the original interface to support operators' temporal awareness. We also wish to establish whether the implementation of time-based DSSs may also come with drawbacks on cognitive functioning and performance.","AuthorNames-Deduped":"Cindy Chamberland;François Vachon;Jean-François Gagnon;Simon P. Banbury;Sébastien Tremblay","AuthorNames":"Cindy Chamberland;François Vachon;Jean-François Gagnon;Simon Banbury;Sébastien Tremblay","AuthorAffiliation":"Université Laval;Université Laval;Université Laval;Looking Glass HF, Inc.;Université Laval","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":8,"Award":null,"image":"6400546-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Augmenting visual representation of affectively charged information using sound graphs","DOI":"10.1109/VAST.2012.6400547","Link":"http://dx.doi.org/10.1109/VAST.2012.6400547","FirstPage":213,"LastPage":214,"PaperType":"M","Abstract":"Within the Visual Analytics research agenda there is an interest on studying the applicability of multimodal information representation and interaction techniques for the analytical reasoning process. The present study summarizes a pilot experiment conducted to understand the effects of augmenting visualizations of affectively-charged information using auditory graphs. We designed an audiovisual representation of social comments made to different news posted on a popular website, and their affective dimension using a sentiment analysis tool for short texts. Participants of the study were asked to create an assessment of the affective valence trend (positive or negative) of the news articles using for it, the visualizations and sonifications. The conditions were tested looking for speed/accuracy trade off comparing the visual representation with an audiovisual one. We discuss our preliminary findings regarding the design of augmented information-representation.","AuthorNames-Deduped":"Nadya A. Calderón;Bernhard E. Riecke;Brian D. Fisher","AuthorNames":"Nadya A. Calderon;Bernhard E. Riecke;Brian Fisher","AuthorAffiliation":"School of Interactive Arts and Technology Simon Fraser University;School of Interactive Arts and Technology Simon Fraser University;School of Interactive Arts and Technology Simon Fraser University","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":9,"Award":null,"image":"6400547-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Feature-similarity visualization of MRI cortical surface data","DOI":"10.1109/VAST.2012.6400548","Link":"http://dx.doi.org/10.1109/VAST.2012.6400548","FirstPage":211,"LastPage":212,"PaperType":"M","Abstract":"We present an analytics-based framework for simultaneous visualization of large surface data collections arising in clinical neuroimaging studies. Termed Informatics Visualization for Neuroimaging (INVIZIAN), this framework allows the visualization of both cortical surfaces characteristics and feature relatedness in unison. It also uses dimension reduction methods to derive new coordinate systems using a Jensen-Shannon divergence metric for positioning cortical surfaces in a metric space such that the proximity in location is proportional to neuroanatomical similarity. Feature data such as thickness and volume are colored on the cortical surfaces and used to display both subject-specific feature values and global trends within the population. Additionally, a query-based framework allows the neuroscience researcher to investigate probable correlations between neuroanatomical and subject patient attribute values such as age and diagnosis.","AuthorNames-Deduped":"Ian Bowman;Shantanu H. Joshi;Vaughan Greer;John D. Van Horn","AuthorNames":"Ian Bowman;Shantanu H. Joshi;Vaughan Greer;John Darrell Van Horn","AuthorAffiliation":"Laboratory of Neuro Imaging, UCLA School of Medicine, Los Angeles 90095;Laboratory of Neuro Imaging, UCLA School of Medicine, Los Angeles 90095;Laboratory of Neuro Imaging, UCLA School of Medicine, Los Angeles 90095;Laboratory of Neuro Imaging, UCLA School of Medicine, Los Angeles 90095","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":2,"Award":null,"image":"6400548-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Matrix-based visual correlation analysis on large timeseries data","DOI":"10.1109/VAST.2012.6400549","Link":"http://dx.doi.org/10.1109/VAST.2012.6400549","FirstPage":209,"LastPage":210,"PaperType":"M","Abstract":"In recent years, the quantity of time series data generated in a wide variety of domains grown consistently. Thus, it is difficult for analysts to process and understand this overwhelming amount of data. In the specific case of time series data another problem arises: time series can be highly interrelated. This problem becomes even more challenging when a set of parameters influences the progression of a time series. However, while most visual analysis techniques support the analysis of short time periods, e.g. one day or one week, they fail to visualize large-scale time series, ranging over one year or more. In our approach we present a time series matrix visualization that tackles this problem. Its primary advantages are that it scales to a large number of time series with different start and end points and allows for the visual comparison / correlation analysis of a set of influencing factors. To evaluate our approach, we applied our technique to a real-world data set, showing the impact of local weather conditions on the efficiency of photovoltaic power plants.","AuthorNames-Deduped":"Michael Behrisch 0001;James Davey;Tobias Schreck;Daniel A. Keim;Jörn Kohlhammer","AuthorNames":"Michael Behrisch;James Davey;Tobias Schreck;Daniel Keim;Jörn Kohlhammer","AuthorAffiliation":"Universität Konstanz;Fraunhofer IGD;Universität Konstanz;Universität Konstanz;Fraunhofer IGD","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":3,"PubsCited":7,"Award":null,"image":"6400549-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"A visual analytics approach to understanding cycling behaviour","DOI":"10.1109/VAST.2012.6400550","Link":"http://dx.doi.org/10.1109/VAST.2012.6400550","FirstPage":207,"LastPage":208,"PaperType":"M","Abstract":"Existing research into cycling behaviours has either relied on detailed ethnographic studies or larger public attitude surveys [1] [9]. Instead, following recent contributions from information visualization [13] and data mining [5] [7], this design study uses visual analytics techniques to identify, describe and explain cycling behaviours within a large and attribute rich transactional dataset. Using data from London's bike share scheme&lt;sup&gt;1&lt;/sup&gt;, customer level classifications will be created, which consider the regularity of scheme use, journey length and travel times. Monitoring customer usage over time, user classifications will attend to the dynamics of cycling behaviour, asking substantive questions about how behaviours change under varying conditions. The 3-year PhD project will contribute to academic and strategic discussions around sustainable travel policy. A programme of research is outlined, along with an early visual analytics prototype for rapidly querying customer journeys.","AuthorNames-Deduped":"Roger Beecham;Jo Wood;Audrey Bowerman","AuthorNames":"Roger Beecham;Jo Wood;Audrey Bowerman","AuthorAffiliation":"City University London;City University London;Transport for London","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":1,"PubsCited":13,"Award":null,"image":"6400550-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2012,"Title":"Information retrieval failure analysis: Visual analytics as a support for interactive \"what-if\" investigation","DOI":"10.1109/VAST.2012.6400551","Link":"http://dx.doi.org/10.1109/VAST.2012.6400551","FirstPage":204,"LastPage":206,"PaperType":"M","Abstract":"This poster provides an analytical model for examining performances of IR systems, based on the discounted cumulative gain family of metrics, and visualization for interacting and exploring the performances of the system under examination. Moreover, we propose machine learning approach to learn the ranking model of the examined system in order to be able to conduct a “what-if” analysis and visually explore what can happen if you adopt a given solution before having to actually implement it.","AuthorNames-Deduped":"Marco Angelini;Nicola Ferro 0001;Guido Granato;Giuseppe Santucci;Gianmaria Silvello","AuthorNames":"Marco Angelini;Nicola Ferro;Guido Granato;Guiseppe Santucci;Gianmaria Silvello","AuthorAffiliation":"Sapienza University of Roma, Italy;University of Padua, Italy;Sapienza University of Roma, Italy;Sapienza University of Roma, Italy;University of Padua, Italy","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":2,"PubsCited":3,"Award":null,"image":"6400551-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Watch this: A taxonomy for dynamic data visualization","DOI":"10.1109/VAST.2012.6400552","Link":"http://dx.doi.org/10.1109/VAST.2012.6400552","FirstPage":193,"LastPage":202,"PaperType":"C","Abstract":"Visualizations embody design choices about data access, data transformation, visual representation, and interaction. To interpret a static visualization, a person must identify the correspondences between the visual representation and the underlying data. These correspondences become moving targets when a visualization is dynamic. Dynamics may be introduced in a visualization at any point in the analysis and visualization process. For example, the data itself may be streaming, shifting subsets may be selected, visual representations may be animated, and interaction may modify presentation. In this paper, we focus on the impact of dynamic data. We present a taxonomy and conceptual framework for understanding how data changes influence the interpretability of visual representations. Visualization techniques are organized into categories at various levels of abstraction. The salient characteristics of each category and task suitability are discussed through examples from the scientific literature and popular practices. Examining the implications of dynamically updating visualizations warrants attention because it directly impacts the interpretability (and thus utility) of visualizations. The taxonomy presented provides a reference point for further exploration of dynamic data visualization techniques.","AuthorNames-Deduped":"Joseph A. Cottam;Andrew Lumsdaine;Chris Weaver","AuthorNames":"Joseph A. Cottam;Andrew Lumsdaine;Chris Weaver","AuthorAffiliation":"Indiana University;Indiana University;University of Oklahoma","InternalReferences":"10.1109/TVCG.2009.123;10.1109/INFVIS.2004.65;10.1109/TVCG.2007.70539;10.1109/TVCG.2008.125;10.1109/INFVIS.2000.885092","AuthorKeywords":"Dynamic Data, Interpretation","AminerCitationCount_02-2020":16,"AminerCitationCount_06-2020":16,"XploreCitationCount - 2020-01":10,"PubsCited":49,"Award":null,"image":"6400552-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Visual analytics methods for categoric spatio-temporal data","DOI":"10.1109/VAST.2012.6400553","Link":"http://dx.doi.org/10.1109/VAST.2012.6400553","FirstPage":183,"LastPage":192,"PaperType":"C","Abstract":"We focus on visual analysis of space- and time-referenced categorical data, which describe possible states of spatial (geographical) objects or locations and their changes over time. The analysis of these data is difficult as there are only limited possibilities to analyze the three aspects (location, time and category) simultaneously. We present a new approach which interactively combines (a) visualization of categorical changes over time; (b) various spatial data displays; (c) computational techniques for task-oriented selection of time steps. They provide an expressive visualization with regard to either the overall evolution over time or unusual changes. We apply our approach on two use cases demonstrating its usefulness for a wide variety of tasks. We analyze data from movement tracking and meteorologic areas. Using our approach, expected events could be detected and new insights were gained.","AuthorNames-Deduped":"Tatiana von Landesberger;Sebastian Bremm;Natalia V. Andrienko;Gennady L. Andrienko;Maria Tekusova","AuthorNames":"T. von Landesberger;Sebastian Bremm;Natalia Andrienko;Gennady Andrienko;Mária Tekušová","AuthorAffiliation":"TU Darmstadt Darmstadt, Germany;TU Darmstadt Darmstadt, Germany;Fraunhofer IAIS Bonn, Germany;Fraunhofer IAIS Bonn, Germany;SHMU Bratislava, Slovakia","InternalReferences":"10.1109/TVCG.2011.174;10.1109/TVCG.2009.117;10.1109/TVCG.2009.181;10.1109/INFVIS.2000.885098;10.1109/TVCG.2010.138;10.1109/VAST.2010.5652530;10.1109/INFVIS.2004.27;10.1109/INFVIS.2005.1532152;10.1109/INFVIS.2001.963281;10.1109/TVCG.2008.165;10.1109/TVCG.2009.153","AuthorKeywords":null,"AminerCitationCount_02-2020":33,"AminerCitationCount_06-2020":37,"XploreCitationCount - 2020-01":22,"PubsCited":43,"Award":null,"image":"6400553-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Visual analytics for the big data era---A comparative review of state-of-the-art commercial systems","DOI":"10.1109/VAST.2012.6400554","Link":"http://dx.doi.org/10.1109/VAST.2012.6400554","FirstPage":173,"LastPage":182,"PaperType":"C","Abstract":"Visual analytics (VA) system development started in academic research institutions where novel visualization techniques and open source toolkits were developed. Simultaneously, small software companies, sometimes spin-offs from academic research institutions, built solutions for specific application domains. In recent years we observed the following trend: some small VA companies grew exponentially; at the same time some big software vendors such as IBM and SAP started to acquire successful VA companies and integrated the acquired VA components into their existing frameworks. Generally the application domains of VA systems have broadened substantially. This phenomenon is driven by the generation of more and more data of high volume and complexity, which leads to an increasing demand for VA solutions from many application domains. In this paper we survey a selection of state-of-the-art commercial VA frameworks, complementary to an existing survey on open source VA tools. From the survey results we identify several improvement opportunities as future research directions.","AuthorNames-Deduped":"Leishi Zhang;Andreas Stoffel;Michael Behrisch 0001;Sebastian Mittelstädt;Tobias Schreck;René Pompl;Stefan Weber 0004;Holger Last;Daniel A. Keim","AuthorNames":"Leishi Zhang;Andreas Stoffel;Michael Behrisch;Sebastian Mittelstadt;Tobias Schreck;René Pompl;Stefan Weber;Holger Last;Daniel Keim","AuthorAffiliation":"University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;Siemens AG;Siemens AG;Siemens AG;University of Konstanz, Germany","InternalReferences":"10.1109/INFVIS.2004.12;10.1109/INFVIS.2004.64;10.1109/INFVIS.2000.885098","AuthorKeywords":null,"AminerCitationCount_02-2020":60,"AminerCitationCount_06-2020":76,"XploreCitationCount - 2020-01":51,"PubsCited":29,"Award":null,"image":"6400554-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Smart super views---A knowledge-assisted interface for medical visualization","DOI":"10.1109/VAST.2012.6400555","Link":"http://dx.doi.org/10.1109/VAST.2012.6400555","FirstPage":163,"LastPage":172,"PaperType":"C","Abstract":"Due to the ever growing volume of acquired data and information, users have to be constantly aware of the methods for their exploration and for interaction. Of these, not each might be applicable to the data at hand or might reveal the desired result. Owing to this, innovations may be used inappropriately and users may become skeptical. In this paper we propose a knowledge-assisted interface for medical visualization, which reduces the necessary effort to use new visualization methods, by providing only the most relevant ones in a smart way. Consequently, we are able to expand such a system with innovations without the users to worry about when, where, and especially how they may or should use them. We present an application of our system in the medical domain and give qualitative feedback from domain experts.","AuthorNames-Deduped":"Gabriel Mistelbauer;Hamed Bouzari;Rüdiger Schernthaner;Ivan Baclija;Arnold Köchl;Stefan Bruckner;Milos Srámek;M. Eduard Gröller","AuthorNames":"Gabriel Mistelbauer;Arnold Köchl;Rudiger Schernthaner;Ivan Baclija;Rüdiger Schernthaner;Stefan Bruckner;Milos Sramek;Meister Eduard Gröller","AuthorAffiliation":"Vienna University of Technology, Austria;Kaiser-Franz-Josef Hospital Vienna, Austria;Austrian Academy of Sciences;Vienna University of Technology Austria;Medical University of Vienna, Austria;Austrian Academy of Sciences;Kaiser-Franz-Josef Hospital Vienna, Austria;Vienna University of Technology, Austria","InternalReferences":"10.1109/VISUAL.2003.1250400;10.1109/TVCG.2006.152;10.1109/TVCG.2007.70576;10.1109/TVCG.2007.70591;10.1109/VISUAL.2002.1183754;10.1109/VISUAL.2005.1532856;10.1109/TVCG.2011.183;10.1109/VISUAL.2005.1532818;10.1109/TVCG.2006.148","AuthorKeywords":"Visualization, Fuzzy Logic, Interaction","AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":5,"XploreCitationCount - 2020-01":4,"PubsCited":40,"Award":null,"image":"6400555-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"AlVis: Situation awareness in the surveillance of road tunnels","DOI":"10.1109/VAST.2012.6400556","Link":"http://dx.doi.org/10.1109/VAST.2012.6400556","FirstPage":153,"LastPage":162,"PaperType":"C","Abstract":"In the surveillance of road tunnels, video data plays an important role for a detailed inspection and as an input to systems for an automated detection of incidents. In disaster scenarios like major accidents, however, the increased amount of detected incidents may lead to situations where human operators lose a sense of the overall meaning of that data, a problem commonly known as a lack of situation awareness. The primary contribution of this paper is a design study of AlVis, a system designed to increase situation awareness in the surveillance of road tunnels. The design of AlVis is based on a simplified tunnel model which enables an overview of the spatiotemporal development of scenarios in real-time. The visualization explicitly represents the present state, the history, and predictions of potential future developments. Concepts for situation-sensitive prioritization of information ensure scalability from normal operation to major disaster scenarios. The visualization enables an intuitive access to live and historic video for any point in time and space. We illustrate AlVis by means of a scenario and report qualitative feedback by tunnel experts and operators. This feedback suggests that AlVis is suitable to save time in recognizing dangerous situations and helps to maintain an overview in complex disaster scenarios.","AuthorNames-Deduped":"Harald Piringer;Matthias Buchetics;Rudolf Benedik","AuthorNames":"Harald Piringer;Matthias Buchetics;Rudolf Benedik","AuthorAffiliation":"VRVis Research Center;VRVis Research Center;Kapsch TrafficCom AG","InternalReferences":"10.1109/INFVIS.2002.1173149;10.1109/INFVIS.2005.1532134;10.1109/VAST.2011.6102456;10.1109/TVCG.2007.70544;10.1109/TVCG.2007.70521;10.1109/TVCG.2007.70621;10.1109/INFVIS.2004.27;10.1109/INFVIS.1995.528685;10.1109/TVCG.2008.185;10.1109/VAST.2007.4388994;10.1109/VAST.2007.4388998;10.1109/VAST.2008.4677353","AuthorKeywords":null,"AminerCitationCount_02-2020":13,"AminerCitationCount_06-2020":15,"XploreCitationCount - 2020-01":11,"PubsCited":40,"Award":null,"image":"6400556-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Spatiotemporal social media analytics for abnormal event detection and examination using seasonal-trend decomposition","DOI":"10.1109/VAST.2012.6400557","Link":"http://dx.doi.org/10.1109/VAST.2012.6400557","FirstPage":143,"LastPage":152,"PaperType":"C","Abstract":"Recent advances in technology have enabled social media services to support space-time indexed data, and internet users from all over the world have created a large volume of time-stamped, geo-located data. Such spatiotemporal data has immense value for increasing situational awareness of local events, providing insights for investigations and understanding the extent of incidents, their severity, and consequences, as well as their time-evolving nature. In analyzing social media data, researchers have mainly focused on finding temporal trends according to volume-based importance. Hence, a relatively small volume of relevant messages may easily be obscured by a huge data set indicating normal situations. In this paper, we present a visual analytics approach that provides users with scalable and interactive social media data analysis and visualization including the exploration and examination of abnormal topics and events within various social media data sources, such as Twitter, Flickr and YouTube. In order to find and understand abnormal events, the analyst can first extract major topics from a set of selected messages and rank them probabilistically using Latent Dirichlet Allocation. He can then apply seasonal trend decomposition together with traditional control chart methods to find unusual peaks and outliers within topic time series. Our case studies show that situational awareness can be improved by incorporating the anomaly and trend examination techniques into a highly interactive visual analysis process.","AuthorNames-Deduped":"Junghoon Chae;Dennis Thom;Harald Bosch;Yun Jang;Ross Maciejewski;David S. Ebert;Thomas Ertl","AuthorNames":"Junghoon Chae;Dennis Thom;Harald Bosch;Yun Jang;Ross Maciejewski;David S. Ebert;Thomas Ertl","AuthorAffiliation":"Purdue University;University of Stuttgart;University of Stuttgart;Sejong University;Arizona State University;Purdue University;University of Stuttgart","InternalReferences":"10.1109/VAST.2011.6102456;10.1109/VAST.2011.6102461;10.1109/TVCG.2008.175","AuthorKeywords":null,"AminerCitationCount_02-2020":104,"AminerCitationCount_06-2020":145,"XploreCitationCount - 2020-01":94,"PubsCited":39,"Award":null,"image":"6400557-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2012,"Title":"SocialNetSense: Supporting sensemaking of social and structural features in networks with interactive visualization","DOI":"10.1109/VAST.2012.6400558","Link":"http://dx.doi.org/10.1109/VAST.2012.6400558","FirstPage":133,"LastPage":142,"PaperType":"C","Abstract":"Increasingly, social network datasets contain social attribute information about actors and their relationship. Analyzing such network with social attributes requires making sense of not only its structural features, but also the relationship between social features in attributes and network structures. Existing social network analysis tools are usually weak in supporting complex analytical tasks involving both structural and social features, and often overlook users' needs for sensemaking tools that help to gather, synthesize, and organize information of these features. To address these challenges, we propose a sensemaking framework of social-network visual analytics in this paper. This framework considers both bottom-up processes, which are about constructing new understandings based on collected information, and top-down processes, which concern using prior knowledge to guide information collection, in analyzing social networks from both social and structural perspectives. The framework also emphasizes the externalization of sensemaking processes through interactive visualization. Guided by the framework, we develop a system, SocialNetSense, to support the sensemaking in visual analytics of social networks with social attributes. The example of using our system to analyze a scholar collaboration network shows that our approach can help users gain insight into social networks both structurally and socially, and enhance their process awareness in visual analytics.","AuthorNames-Deduped":"Liang Gou;Xiaolong Zhang 0001;Airong Luo;Patricia F. Anderson","AuthorNames":"Liang Gou;Xiaolong Zhang;Airong Luo;Patricia F. Anderson","AuthorAffiliation":"The Pennsylvania State University;The Pennsylvania State University;University of Michigan;University of Michigan","InternalReferences":"10.1109/INFVIS.1999.801853;10.1109/TVCG.2011.247;10.1109/INFVIS.2005.1532126;10.1109/VISUAL.2005.1532788;10.1109/TVCG.2007.70582;10.1109/TVCG.2006.192;10.1109/VAST.2009.5333020;10.1109/VAST.2011.6102440;10.1109/VAST.2006.261426;10.1109/INFVIS.2004.2;10.1109/TVCG.2008.137;10.1109/TVCG.2006.166;10.1109/TVCG.2006.160;10.1109/VAST.2008.4677365;10.1109/TVCG.2006.147;10.1109/VAST.2007.4389006","AuthorKeywords":"Social network, visualization, sensemaking, visual analytics, SocialNetSense","AminerCitationCount_02-2020":6,"AminerCitationCount_06-2020":6,"XploreCitationCount - 2020-01":0,"PubsCited":48,"Award":null,"image":"6400558-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"Analyst's Workspace: An embodied sensemaking environment for large, high-resolution displays","DOI":"10.1109/VAST.2012.6400559","Link":"http://dx.doi.org/10.1109/VAST.2012.6400559","FirstPage":123,"LastPage":131,"PaperType":"C","Abstract":"Distributed cognition and embodiment provide compelling models for how humans think and interact with the environment. Our examination of the use of large, high-resolution displays from an embodied perspective has lead directly to the development of a new sensemaking environment called Analyst's Workspace (AW). AW leverages the embodied resources made more accessible through the physical nature of the display to create a spatial workspace. By combining spatial layout of documents and other artifacts with an entity-centric, explorative investigative approach, AW aims to allow the analyst to externalize elements of the sensemaking process as a part of the investigation, integrated into the visual representations of the data itself. In this paper, we describe the various capabilities of AW and discuss the key principles and concepts underlying its design, emphasizing unique design principles for designing visual analytic tools for large, high-resolution displays.","AuthorNames-Deduped":"Christopher Andrews;Chris North","AuthorNames":"Christopher Andrews;Chris North","AuthorAffiliation":"Virginia Tech;Virginia Tech","InternalReferences":"10.1109/TVCG.2008.121;10.1109/VAST.2008.4677362;10.1109/INFVIS.2004.27;10.1109/VAST.2008.4677358;10.1109/TVCG.2006.184;10.1109/VAST.2007.4388992;10.1109/VAST.2010.5652880;10.1109/VAST.2011.6102449;10.1109/VAST.2007.4389006;10.1109/VAST.2011.6102438;10.1109/VAST.2009.5333878","AuthorKeywords":"Embodiment, distributed cognition, large and high-resolution display, sensemaking, space","AminerCitationCount_02-2020":30,"AminerCitationCount_06-2020":37,"XploreCitationCount - 2020-01":26,"PubsCited":52,"Award":null,"image":"6400559-fig-1-source-large.gif"},{"Conference":"VAST","Year":2012,"Title":"The Deshredder: A visual analytic approach to reconstructing shredded documents","DOI":"10.1109/VAST.2012.6400560","Link":"http://dx.doi.org/10.1109/VAST.2012.6400560","FirstPage":113,"LastPage":122,"PaperType":"C","Abstract":"Reconstruction of shredded documents remains a significant challenge. Creating a better document reconstruction system enables not just recovery of information accidentally lost but also understanding our limitations against adversaries' attempts to gain access to information. Existing approaches to reconstructing shredded documents adopt either a predominantly manual (e.g., crowd-sourcing) or a near automatic approach. We describe Deshredder, a visual analytic approach that scales well and effectively incorporates user input to direct the reconstruction process. Deshredder represents shredded pieces as time series and uses nearest neighbor matching techniques that enable matching both the contours of shredded pieces as well as the content of shreds themselves. More importantly, Deshred-der's interface support visual analytics through user interaction with similarity matrices as well as higher level assembly through more complex stitching functions. We identify a functional task taxonomy leading to design considerations for constructing deshredding solutions, and describe how Deshredder applies to problems from the DARPA Shredder Challenge through expert evaluations.","AuthorNames-Deduped":"Patrick Butler;Prithwish Chakraborty;Naren Ramakrishnan","AuthorNames":"Patrick Butler;Prithwish Chakraborty;Naren Ramakrishan","AuthorAffiliation":"Department of Computer Science and Discovery Analytics Center, Virginia Tech, Blacksburg, VA 24061;Department of Computer Science and Discovery Analytics Center, Virginia Tech, Blacksburg, VA 24061;Department of Computer Science and Discovery Analytics Center, Virginia Tech, Blacksburg, VA 24061","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":15,"AminerCitationCount_06-2020":18,"XploreCitationCount - 2020-01":15,"PubsCited":37,"Award":null,"image":"6400560-fig-1-source-large.gif"},{"Conference":"VAST","Year":2013,"Title":"A Partition-Based Framework for Building and Validating Regression Models","DOI":"10.1109/TVCG.2013.125","Link":"http://dx.doi.org/10.1109/TVCG.2013.125","FirstPage":1962,"LastPage":1971,"PaperType":"J","Abstract":"Regression models play a key role in many application domains for analyzing or predicting a quantitative dependent variable based on one or more independent variables. Automated approaches for building regression models are typically limited with respect to incorporating domain knowledge in the process of selecting input variables (also known as feature subset selection). Other limitations include the identification of local structures, transformations, and interactions between variables. The contribution of this paper is a framework for building regression models addressing these limitations. The framework combines a qualitative analysis of relationship structures by visualization and a quantification of relevance for ranking any number of features and pairs of features which may be categorical or continuous. A central aspect is the local approximation of the conditional target distribution by partitioning 1D and 2D feature domains into disjoint regions. This enables a visual investigation of local patterns and largely avoids structural assumptions for the quantitative ranking. We describe how the framework supports different tasks in model building (e.g., validation and comparison), and we present an interactive workflow for feature subset selection. A real-world case study illustrates the step-wise identification of a five-dimensional model for natural gas consumption. We also report feedback from domain experts after two months of deployment in the energy sector, indicating a significant effort reduction for building and improving regression models.","AuthorNames-Deduped":"Thomas Mühlbacher;Harald Piringer","AuthorNames":"Thomas Mühlbacher;Harald Piringer","AuthorAffiliation":"VRVis Research Center;VRVis Research Center","InternalReferences":"10.1109/TVCG.2012.219;10.1109/TVCG.2009.128;10.1109/VISUAL.1993.398859;10.1109/VAST.2012.6400486;10.1109/VAST.2011.6102453;10.1109/VAST.2009.5333431;10.1109/TVCG.2010.213;10.1109/TVCG.2012.205;10.1109/VAST.2009.5332628;10.1109/VISUAL.1990.146402;10.1109/VAST.2011.6102450;10.1109/VAST.2008.4677368;10.1109/VAST.2010.5652460;10.1109/TVCG.2011.248;10.1109/INFVIS.2005.1532142;10.1109/VAST.2007.4388999;10.1109/INFVIS.2004.10;10.1109/TVCG.2009.110;10.1109/VAST.2011.6102448;10.1109/INFVIS.2004.3","AuthorKeywords":"Regression, model building, visual knowledge discovery, feature selection, data partitioning, guided visualization","AminerCitationCount_02-2020":48,"AminerCitationCount_06-2020":67,"XploreCitationCount - 2020-01":50,"PubsCited":50,"Award":"BP","image":"6634169-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2013,"Title":"An Extensible Framework for Provenance in Human Terrain Visual Analytics","DOI":"10.1109/TVCG.2013.132","Link":"http://dx.doi.org/10.1109/TVCG.2013.132","FirstPage":2139,"LastPage":2148,"PaperType":"J","Abstract":"We describe and demonstrate an extensible framework that supports data exploration and provenance in the context of Human Terrain Analysis (HTA). Working closely with defence analysts we extract requirements and a list of features that characterise data analysed at the end of the HTA chain. From these, we select an appropriate non-classified data source with analogous features, and model it as a set of facets. We develop ProveML, an XML-based extension of the Open Provenance Model, using these facets and augment it with the structures necessary to record the provenance of data, analytical process and interpretations. Through an iterative process, we develop and refine a prototype system for Human Terrain Visual Analytics (HTVA), and demonstrate means of storing, browsing and recalling analytical provenance and process through analytic bookmarks in ProveML. We show how these bookmarks can be combined to form narratives that link back to the live data. Throughout the process, we demonstrate that through structured workshops, rapid prototyping and structured communication with intelligence analysts we are able to establish requirements, and design schema, techniques and tools that meet the requirements of the intelligence community. We use the needs and reactions of defence analysts in defining and steering the methods to validate the framework.","AuthorNames-Deduped":"Rick Walker;Aidan Slingsby;Jason Dykes;Kai Xu 0003;Jo Wood;Phong H. Nguyen;Derek Stephens;B. L. William Wong;Yongjun Zheng","AuthorNames":"Rick Walker;Aiden Slingsby;Jason Dykes;Kai Xu;Jo Wood;Phong H. Nguyen;Derek Stephens;B.L. William Wong;Yongjun Zheng","AuthorAffiliation":"Middlesex University;City University;City University;Middlesex University;City University;Middlesex University;Loughborough University;Middlesex University;Middlesex University","InternalReferences":"10.1109/TVCG.2012.252;10.1109/TVCG.2010.191;10.1109/VAST.2007.4388992;10.1109/TVCG.2006.142;10.1109/VAST.2006.261431;10.1109/TVCG.2010.154;10.1109/TVCG.2012.213;10.1109/INFVIS.2000.885086;10.1109/TVCG.2007.70577;10.1109/TVCG.2009.111;10.1109/VAST.2008.4677366;10.1109/VAST.2008.4677365;10.1109/VAST.2007.4388992;10.1109/TVCG.2009.128;10.1109/VAST.2009.5332611;10.1109/TVCG.2010.183;10.1109/VAST.2009.5333919;10.1109/TVCG.2011.209;10.1109/TVCG.2009.139;10.1109/TVCG.2008.175","AuthorKeywords":"Human terrain analysis, provenance, framework, bookmarks, narratives","AminerCitationCount_02-2020":18,"AminerCitationCount_06-2020":23,"XploreCitationCount - 2020-01":18,"PubsCited":59,"Award":null,"image":"6634110-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2013,"Title":"Decision Exploration Lab: A Visual Analytics Solution for Decision Management","DOI":"10.1109/TVCG.2013.146","Link":"http://dx.doi.org/10.1109/TVCG.2013.146","FirstPage":1972,"LastPage":1981,"PaperType":"J","Abstract":"We present a visual analytics solution designed to address prevalent issues in the area of Operational Decision Management (ODM). In ODM, which has its roots in Artificial Intelligence (Expert Systems) and Management Science, it is increasingly important to align business decisions with business goals. In our work, we consider decision models (executable models of the business domain) as ontologies that describe the business domain, and production rules that describe the business logic of decisions to be made over this ontology. Executing a decision model produces an accumulation of decisions made over time for individual cases. We are interested, first, to get insight in the decision logic and the accumulated facts by themselves. Secondly and more importantly, we want to see how the accumulated facts reveal potential divergences between the reality as captured by the decision model, and the reality as captured by the executed decisions. We illustrate the motivation, added value for visual analytics, and our proposed solution and tooling through a business case from the car insurance industry.","AuthorNames-Deduped":"Bertjan Broeksema;Thomas Baudel;Arthur G. Telea;Paolo Crisafulli","AuthorNames":"Bertjan Broeksema;Thomas Baudel;Alex Telea;Paolo Crisafulli","AuthorAffiliation":"IBM France Center for Advanced Studies, Institute Johann Bernoulli, University of Groningen, The Netherlands andINRIA, University of Bordeaux, France;IBM France Center for Advanced Studies;Institute Johann Bernoulli, University of Groningen, The Netherlands;IBM France","InternalReferences":"10.1109/VISUAL.1991.175815;10.1109/VAST.2011.6102463;10.1109/VAST.2010.5652398;10.1109/VAST.2008.4677361;10.1109/VAST.2008.4677363;10.1109/TVCG.2011.185;10.1109/VAST.2011.6102457","AuthorKeywords":"Decision support systems, model validation and analysis, multivariate Statistics, program analysis","AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":6,"XploreCitationCount - 2020-01":6,"PubsCited":49,"Award":null,"image":"6634184-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2013,"Title":"Explainers: Expert Explorations with Crafted Projections","DOI":"10.1109/TVCG.2013.157","Link":"http://dx.doi.org/10.1109/TVCG.2013.157","FirstPage":2042,"LastPage":2051,"PaperType":"J","Abstract":"This paper introduces an approach to exploration and discovery in high-dimensional data that incorporates a user's knowledge and questions to craft sets of projection functions meaningful to them. Unlike most prior work that defines projections based on their statistical properties, our approach creates projection functions that align with user-specified annotations. Therefore, the resulting derived dimensions represent concepts defined by the user's examples. These especially crafted projection functions, or explainers, can help find and explain relationships between the data variables and user-designated concepts. They can organize the data according to these concepts. Sets of explainers can provide multiple perspectives on the data. Our approach considers tradeoffs in choosing these projection functions, including their simplicity, expressive power, alignment with prior knowledge, and diversity. We provide techniques for creating collections of explainers. The methods, based on machine learning optimization frameworks, allow exploring the tradeoffs. We demonstrate our approach on model problems and applications in text analysis.","AuthorNames-Deduped":"Michael Gleicher","AuthorNames":"Michael Gleicher","AuthorAffiliation":"Department of Computer Sciences, University of Wisconsin - Madison","InternalReferences":"10.1109/VAST.2012.6400487;10.1109/VAST.2012.6400486;10.1109/TVCG.2012.277;10.1109/INFVIS.2005.1532142;10.1109/INFVIS.2004.71;10.1109/TVCG.2012.256;10.1109/VAST.2010.5652392;10.1109/VAST.2012.6400490;10.1109/TVCG.2011.220;10.1109/INFVIS.1998.729559;10.1109/VAST.2011.6102448;10.1109/TVCG.2009.153","AuthorKeywords":"High-dimensional spaces, exploration, support vector machines","AminerCitationCount_02-2020":27,"AminerCitationCount_06-2020":39,"XploreCitationCount - 2020-01":26,"PubsCited":51,"Award":"HM","image":"6634124-fig-1-source-large.gif"},{"Conference":"VAST","Year":2013,"Title":"HierarchicalTopics: Visually Exploring Large Text Collections Using Topic Hierarchies","DOI":"10.1109/TVCG.2013.162","Link":"http://dx.doi.org/10.1109/TVCG.2013.162","FirstPage":2002,"LastPage":2011,"PaperType":"J","Abstract":"Analyzing large textual collections has become increasingly challenging given the size of the data available and the rate that more data is being generated. Topic-based text summarization methods coupled with interactive visualizations have presented promising approaches to address the challenge of analyzing large text corpora. As the text corpora and vocabulary grow larger, more topics need to be generated in order to capture the meaningful latent themes and nuances in the corpora. However, it is difficult for most of current topic-based visualizations to represent large number of topics without being cluttered or illegible. To facilitate the representation and navigation of a large number of topics, we propose a visual analytics system - HierarchicalTopic (HT). HT integrates a computational algorithm, Topic Rose Tree, with an interactive visual interface. The Topic Rose Tree constructs a topic hierarchy based on a list of topics. The interactive visual interface is designed to present the topic content as well as temporal evolution of topics in a hierarchical fashion. User interactions are provided for users to make changes to the topic hierarchy based on their mental model of the topic space. To qualitatively evaluate HT, we present a case study that showcases how HierarchicalTopics aid expert users in making sense of a large number of topics and discovering interesting patterns of topic groups. We have also conducted a user study to quantitatively evaluate the effect of hierarchical topic structure. The study results reveal that the HT leads to faster identification of large number of relevant topics. We have also solicited user feedback during the experiments and incorporated some suggestions into the current version of HierarchicalTopics.","AuthorNames-Deduped":"Wenwen Dou;Li Yu;Xiaoyu Wang;Zhiqiang Ma;William Ribarsky","AuthorNames":"Wenwen Dou;Li Yu;Xiaoyu Wang;Zhiqiang Ma;William Ribarsky","AuthorAffiliation":"University of North Carolina at Charlotte;University of North Carolina at Charlotte;University of North Carolina at Charlotte;University of North Carolina at Charlotte;University of North Carolina at Charlotte","InternalReferences":"10.1109/VAST.2010.5652931;10.1109/VAST.2012.6400557;10.1109/TVCG.2011.239;10.1109/VAST.2011.6102461;10.1109/VAST.2012.6400485","AuthorKeywords":"Hierarchical topic representation, topic modeling, visual analytics, rose tree","AminerCitationCount_02-2020":71,"AminerCitationCount_06-2020":88,"XploreCitationCount - 2020-01":58,"PubsCited":35,"Award":null,"image":"6634160-fig-1-source-large.gif"},{"Conference":"VAST","Year":2013,"Title":"Identifying Redundancy and Exposing Provenance in Crowdsourced Data Analysis","DOI":"10.1109/TVCG.2013.164","Link":"http://dx.doi.org/10.1109/TVCG.2013.164","FirstPage":2198,"LastPage":2206,"PaperType":"J","Abstract":"We present a system that lets analysts use paid crowd workers to explore data sets and helps analysts interactively examine and build upon workers' insights. We take advantage of the fact that, for many types of data, independent crowd workers can readily perform basic analysis tasks like examining views and generating explanations for trends and patterns. However, workers operating in parallel can often generate redundant explanations. Moreover, because workers have different competencies and domain knowledge, some responses are likely to be more plausible than others. To efficiently utilize the crowd's work, analysts must be able to quickly identify and consolidate redundant responses and determine which explanations are the most plausible. In this paper, we demonstrate several crowd-assisted techniques to help analysts make better use of crowdsourced explanations: (1) We explore crowd-assisted strategies that utilize multiple workers to detect redundant explanations. We introduce color clustering with representative selection-a strategy in which multiple workers cluster explanations and we automatically select the most-representative result-and show that it generates clusterings that are as good as those produced by experts. (2) We capture explanation provenance by introducing highlighting tasks and capturing workers' browsing behavior via an embedded web browser, and refine that provenance information via source-review tasks. We expose this information in an explanation-management interface that allows analysts to interactively filter and sort responses, select the most plausible explanations, and decide which to explore further.","AuthorNames-Deduped":"Wesley Willett;Shiry Ginosar;Avital Steinitz;Björn Hartmann;Maneesh Agrawala","AuthorNames":"Wesley Willett;Shiry Ginosar;Avital Steinitz;Björn Hartmann;Maneesh Agrawala","AuthorAffiliation":"INRIA;UC Berkeley;UC Berkeley;UC Berkeley;UC Berkeley","InternalReferences":"10.1109/TVCG.2007.70577","AuthorKeywords":"Crowdsourcing, social data analysis","AminerCitationCount_02-2020":13,"AminerCitationCount_06-2020":21,"XploreCitationCount - 2020-01":12,"PubsCited":23,"Award":null,"image":"6634191-fig-1-source-large.gif"},{"Conference":"VAST","Year":2013,"Title":"Interactive Exploration of Implicit and Explicit Relations in Faceted Datasets","DOI":"10.1109/TVCG.2013.167","Link":"http://dx.doi.org/10.1109/TVCG.2013.167","FirstPage":2080,"LastPage":2089,"PaperType":"J","Abstract":"Many datasets, such as scientific literature collections, contain multiple heterogeneous facets which derive implicit relations, as well as explicit relational references between data items. The exploration of this data is challenging not only because of large data scales but also the complexity of resource structures and semantics. In this paper, we present PivotSlice, an interactive visualization technique which provides efficient faceted browsing as well as flexible capabilities to discover data relationships. With the metaphor of direct manipulation, PivotSlice allows the user to visually and logically construct a series of dynamic queries over the data, based on a multi-focus and multi-scale tabular view that subdivides the entire dataset into several meaningful parts with customized semantics. PivotSlice further facilitates the visual exploration and sensemaking process through features including live search and integration of online data, graphical interaction histories and smoothly animated visual state transitions. We evaluated PivotSlice through a qualitative lab study with university researchers and report the findings from our observations and interviews. We also demonstrate the effectiveness of PivotSlice using a scenario of exploring a repository of information visualization literature.","AuthorNames-Deduped":"Jian Zhao 0010;Christopher Collins 0001;Fanny Chevalier;Ravin Balakrishnan","AuthorNames":"Jian Zhao;Christopher Collins;Fanny Chevalier;Ravin Balakrishnan","AuthorAffiliation":"University of Toronto;University of Ontario Institute of Technology;University of Toronto, Canada;University of Toronto, Canada","InternalReferences":"10.1109/TVCG.2008.137;10.1109/VAST.2011.6102440;10.1109/TVCG.2011.213;10.1109/TVCG.2010.154;10.1109/VAST.2006.261426;10.1109/INFVIS.2005.1532136;10.1109/TVCG.2010.205;10.1109/TVCG.2012.252;10.1109/TVCG.2006.166;10.1109/INFVIS.2000.885086","AuthorKeywords":"Faceted browsing, network exploration, dynamic query, interaction, information visualization, visual analytics","AminerCitationCount_02-2020":34,"AminerCitationCount_06-2020":49,"XploreCitationCount - 2020-01":35,"PubsCited":39,"Award":null,"image":"6634163-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2013,"Title":"Interactive Exploration of Surveillance Video through Action Shot Summarization and Trajectory Visualization","DOI":"10.1109/TVCG.2013.168","Link":"http://dx.doi.org/10.1109/TVCG.2013.168","FirstPage":2119,"LastPage":2128,"PaperType":"J","Abstract":"We propose a novel video visual analytics system for interactive exploration of surveillance video data. Our approach consists of providing analysts with various views of information related to moving objects in a video. To do this we first extract each object's movement path. We visualize each movement by (a) creating a single action shot image (a still image that coalesces multiple frames), (b) plotting its trajectory in a space-time cube and (c) displaying an overall timeline view of all the movements. The action shots provide a still view of the moving object while the path view presents movement properties such as speed and location. We also provide tools for spatial and temporal filtering based on regions of interest. This allows analysts to filter out large amounts of movement activities while the action shot representation summarizes the content of each movement. We incorporated this multi-part visual representation of moving objects in sViSIT, a tool to facilitate browsing through the video content by interactive querying and retrieval of data. Based on our interaction with security personnel who routinely interact with surveillance video data, we identified some of the most common tasks performed. This resulted in designing a user study to measure time-to-completion of the various tasks. These generally required searching for specific events of interest (targets) in videos. Fourteen different tasks were designed and a total of 120 min of surveillance video were recorded (indoor and outdoor locations recording movements of people and vehicles). The time-to-completion of these tasks were compared against a manual fast forward video browsing guided with movement detection. We demonstrate how our system can facilitate lengthy video exploration and significantly reduce browsing time to find events of interest. Reports from expert users identify positive aspects of our approach which we summarize in our recommendations for future video visual analytics systems.","AuthorNames-Deduped":"Amir H. Meghdadi;Pourang Irani","AuthorNames":"Amir H. Meghdadi;Pourang Irani","AuthorAffiliation":"Department of Computer Science, University of Manitoba;Department of Computer Science, University of Manitoba","InternalReferences":"10.1109/INFVIS.2004.27;10.1109/TVCG.2012.222;10.1109/VISUAL.2003.1250401","AuthorKeywords":"Video visual analytics, surveillance video, video visualization, video summarization, video browsing and exploration","AminerCitationCount_02-2020":24,"AminerCitationCount_06-2020":36,"XploreCitationCount - 2020-01":27,"PubsCited":33,"Award":null,"image":""},{"Conference":"VAST","Year":2013,"Title":"MotionExplorer: Exploratory Search in Human Motion Capture Data Based on Hierarchical Aggregation","DOI":"10.1109/TVCG.2013.178","Link":"http://dx.doi.org/10.1109/TVCG.2013.178","FirstPage":2257,"LastPage":2266,"PaperType":"J","Abstract":"We present MotionExplorer, an exploratory search and analysis system for sequences of human motion in large motion capture data collections. This special type of multivariate time series data is relevant in many research fields including medicine, sports and animation. Key tasks in working with motion data include analysis of motion states and transitions, and synthesis of motion vectors by interpolation and combination. In the practice of research and application of human motion data, challenges exist in providing visual summaries and drill-down functionality for handling large motion data collections. We find that this domain can benefit from appropriate visual retrieval and analysis support to handle these tasks in presence of large motion data. To address this need, we developed MotionExplorer together with domain experts as an exploratory search system based on interactive aggregation and visualization of motion states as a basis for data navigation, exploration, and search. Based on an overview-first type visualization, users are able to search for interesting sub-sequences of motion based on a query-by-example metaphor, and explore search results by details on demand. We developed MotionExplorer in close collaboration with the targeted users who are researchers working on human motion synthesis and analysis, including a summative field study. Additionally, we conducted a laboratory design study to substantially improve MotionExplorer towards an intuitive, usable and robust design. MotionExplorer enables the search in human motion capture data with only a few mouse clicks. The researchers unanimously confirm that the system can efficiently support their work.","AuthorNames-Deduped":"Jürgen Bernard;Nils Wilhelm;Björn Krüger;Thorsten May;Tobias Schreck;Jörn Kohlhammer","AuthorNames":"Jürgen Bernard;Nils Wilhelm;Björn Krüger;Thorsten May;Tobias Schreck;Jörn Kohlhammer","AuthorAffiliation":"Fraunhofer Institute for Computer Graphics Research Darmstadt;Fraunhofer Institute for Computer Graphics Research Darmstadt;Institute of Computer Science, Universität Bonn;Fraunhofer Institute for Computer Graphics Research Darmstadt;Data Analysis and Visualization Group, Universität Konstanz;Fraunhofer Institute for Computer Graphics Research Darmstadt","InternalReferences":"10.1109/VISUAL.1999.809865;10.1109/VAST.2008.4677350;10.1109/TVCG.2006.120;10.1109/TVCG.2011.181;10.1109/TVCG.2011.188","AuthorKeywords":"Visual analytics, exploratory search, multivariate time series, motion capture data, data aggregation, cluster glyph","AminerCitationCount_02-2020":32,"AminerCitationCount_06-2020":50,"XploreCitationCount - 2020-01":36,"PubsCited":45,"Award":null,"image":"6634102-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2013,"Title":"Open-Box Spectral Clustering: Applications to Medical Image Analysis","DOI":"10.1109/TVCG.2013.181","Link":"http://dx.doi.org/10.1109/TVCG.2013.181","FirstPage":2100,"LastPage":2108,"PaperType":"J","Abstract":"Spectral clustering is a powerful and versatile technique, whose broad range of applications includes 3D image analysis. However, its practical use often involves a tedious and time-consuming process of tuning parameters and making application-specific choices. In the absence of training data with labeled clusters, help from a human analyst is required to decide the number of clusters, to determine whether hierarchical clustering is needed, and to define the appropriate distance measures, parameters of the underlying graph, and type of graph Laplacian. We propose to simplify this process via an open-box approach, in which an interactive system visualizes the involved mathematical quantities, suggests parameter values, and provides immediate feedback to support the required decisions. Our framework focuses on applications in 3D image analysis, and links the abstract high-dimensional feature space used in spectral clustering to the three-dimensional data space. This provides a better understanding of the technique, and helps the analyst predict how well specific parameter settings will generalize to similar tasks. In addition, our system supports filtering outliers and labeling the final clusters in such a way that user actions can be recorded and transferred to different data in which the same structures are to be found. Our system supports a wide range of inputs, including triangular meshes, regular grids, and point clouds. We use our system to develop segmentation protocols in chest CT and brain MRI that are then successfully applied to other datasets in an automated manner.","AuthorNames-Deduped":"Thomas Schultz 0001;Gordon L. Kindlmann","AuthorNames":"Thomas Schultz;Gordon L. Kindlmann","AuthorAffiliation":"University of Bonn;University of Chicago","InternalReferences":"10.1109/VISUAL.2005.1532820;10.1109/VAST.2010.5652926;10.1109/VISUAL.2000.885740;10.1109/VAST.2012.6400488;10.1109/TVCG.2009.141;10.1109/TVCG.2009.112;10.1109/TVCG.2009.177;10.1109/TVCG.2010.199;10.1109/TVCG.2009.199;10.1109/TVCG.2011.248;10.1109/TVCG.2011.253","AuthorKeywords":"Image segmentation, spectral clustering, high-dimensional embeddings, linked views, programming with example","AminerCitationCount_02-2020":21,"AminerCitationCount_06-2020":24,"XploreCitationCount - 2020-01":19,"PubsCited":53,"Award":null,"image":"6634089-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2013,"Title":"ScatterBlogs2: Real-Time Monitoring of Microblog Messages through User-Guided filtering","DOI":"10.1109/TVCG.2013.186","Link":"http://dx.doi.org/10.1109/TVCG.2013.186","FirstPage":2022,"LastPage":2031,"PaperType":"J","Abstract":"The number of microblog posts published daily has reached a level that hampers the effective retrieval of relevant messages, and the amount of information conveyed through services such as Twitter is still increasing. Analysts require new methods for monitoring their topic of interest, dealing with the data volume and its dynamic nature. It is of particular importance to provide situational awareness for decision making in time-critical tasks. Current tools for monitoring microblogs typically filter messages based on user-defined keyword queries and metadata restrictions. Used on their own, such methods can have drawbacks with respect to filter accuracy and adaptability to changes in trends and topic structure. We suggest ScatterBlogs2, a new approach to let analysts build task-tailored message filters in an interactive and visual manner based on recorded messages of well-understood previous events. These message filters include supervised classification and query creation backed by the statistical distribution of terms and their co-occurrences. The created filter methods can be orchestrated and adapted afterwards for interactive, visual real-time monitoring and analysis of microblog feeds. We demonstrate the feasibility of our approach for analyzing the Twitter stream in emergency management scenarios.","AuthorNames-Deduped":"Harald Bosch;Dennis Thom;Florian Heimerl;Edwin Puttmann;Steffen Koch;Robert Krüger;Michael Wörner 0001;Thomas Ertl","AuthorNames":"Harald Bosch;Dennis Thom;Florian Heimerl;Edwin Püttmann;Steffen Koch;Robert Krüger;Michael Wörner;Thomas Ertl","AuthorAffiliation":"Visualization and Interactive Systems, University of Stuttgart;Visualization and Interactive Systems, University of Stuttgart;Visualization and Interactive Systems, University of Stuttgart;Visualization and Interactive Systems, University of Stuttgart;Visualization and Interactive Systems, University of Stuttgart;Visualization and Interactive Systems, University of Stuttgart;Visualization and Interactive Systems, University of Stuttgart;Visualization and Interactive Systems, University of Stuttgart","InternalReferences":"10.1109/VISUAL.2005.1532781;10.1109/VAST.2012.6400492;10.1109/VAST.2012.6400557;10.1109/TVCG.2012.291;10.1109/TVCG.2012.277;10.1109/VAST.2012.6400485;10.1109/VAST.2007.4389013;10.1109/VAST.2007.4389006;10.1109/VAST.2011.6102456;10.1109/TVCG.2008.175","AuthorKeywords":"Microblog analysis, Twitter, text analytics, social media monitoring, live monitoring, visual analytics, information visualization, filter construction, query construction, text classification","AminerCitationCount_02-2020":61,"AminerCitationCount_06-2020":83,"XploreCitationCount - 2020-01":63,"PubsCited":35,"Award":null,"image":"6634195-fig-1-source-large.gif"},{"Conference":"VAST","Year":2013,"Title":"Semantics of Directly Manipulating Spatializations","DOI":"10.1109/TVCG.2013.188","Link":"http://dx.doi.org/10.1109/TVCG.2013.188","FirstPage":2052,"LastPage":2059,"PaperType":"J","Abstract":"When high-dimensional data is visualized in a 2D plane by using parametric projection algorithms, users may wish to manipulate the layout of the data points to better reflect their domain knowledge or to explore alternative structures. However, few users are well-versed in the algorithms behind the visualizations, making parameter tweaking more of a guessing game than a series of decisive interactions. Translating user interactions into algorithmic input is a key component of Visual to Parametric Interaction (V2PI) [13]. Instead of adjusting parameters, users directly move data points on the screen, which then updates the underlying statistical model. However, we have found that some data points that are not moved by the user are just as important in the interactions as the data points that are moved. Users frequently move some data points with respect to some other 'unmoved' data points that they consider as spatially contextual. However, in current V2PI interactions, these points are not explicitly identified when directly manipulating the moved points. We design a richer set of interactions that makes this context more explicit, and a new algorithm and sophisticated weighting scheme that incorporates the importance of these unmoved data points into V2PI.","AuthorNames-Deduped":"Xinran Hu;Lauren Bradel;Dipayan Maiti;Leanna House;Chris North;Scotland Leman","AuthorNames":"Xinran Hu;Lauren Bradel;Dipayan Maiti;Leanna House;Chris North;Scotland Leman","AuthorAffiliation":"Virginia Tech;Virginia Tech;Virginia Tech;Virginia Tech;Virginia Tech;Virginia Tech","InternalReferences":"10.1109/VAST.2011.6102449;10.1109/INFVIS.1995.528686;10.1109/TVCG.2012.260;10.1109/VAST.2012.6400486;10.1109/VAST.2008.4677358","AuthorKeywords":"Visual to parametric interaction, visual analytics, statistical models","AminerCitationCount_02-2020":21,"AminerCitationCount_06-2020":25,"XploreCitationCount - 2020-01":20,"PubsCited":23,"Award":null,"image":""},{"Conference":"VAST","Year":2013,"Title":"SketchPadN-D: WYDIWYG Sculpting and Editing in High-Dimensional Space","DOI":"10.1109/TVCG.2013.190","Link":"http://dx.doi.org/10.1109/TVCG.2013.190","FirstPage":2060,"LastPage":2069,"PaperType":"J","Abstract":"High-dimensional data visualization has been attracting much attention. To fully test related software and algorithms, researchers require a diverse pool of data with known and desired features. Test data do not always provide this, or only partially. Here we propose the paradigm WYDIWYGS (What You Draw Is What You Get). Its embodiment, SketchPad&lt;sup&gt;ND&lt;/sup&gt;, is a tool that allows users to generate high-dimensional data in the same interface they also use for visualization. This provides for an immersive and direct data generation activity, and furthermore it also enables users to interactively edit and clean existing high-dimensional data from possible artifacts. SketchPad&lt;sup&gt;ND&lt;/sup&gt; offers two visualization paradigms, one based on parallel coordinates and the other based on a relatively new framework using an N-D polygon to navigate in high-dimensional space. The first interface allows users to draw arbitrary profiles of probability density functions along each dimension axis and sketch shapes for data density and connections between adjacent dimensions. The second interface embraces the idea of sculpting. Users can carve data at arbitrary orientations and refine them wherever necessary. This guarantees that the data generated is truly high-dimensional. We demonstrate our tool's usefulness in real data visualization scenarios.","AuthorNames-Deduped":"Bing Wang 0007;Puripant Ruchikachorn;Klaus Mueller","AuthorNames":"Bing Wang;Puripant Ruchikachorn;Klaus Mueller","AuthorAffiliation":"Visual Analytics and Imaging Laboratory, Computer Science Department, Stony Brook University;Visual Analytics and Imaging Laboratory, Computer Science Department, Stony Brook University, and Chulalongkorn Business School, Chulalongkorn University;Visual Analytics and Imaging Laboratory, Computer Science Department, Stony Brook University, and SUNY Korea","InternalReferences":"10.1109/TVCG.2011.237;10.1109/VAST.2012.6400489","AuthorKeywords":"Synthetic data generation, data editing, data acquisition and management, multivariate data, high-dimensional data, interaction, user interface, parallel coordinates, scatterplot, N-D navigation, multiple views","AminerCitationCount_02-2020":12,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":9,"PubsCited":25,"Award":null,"image":""},{"Conference":"VAST","Year":2013,"Title":"Space Transformation for Understanding Group Movement","DOI":"10.1109/TVCG.2013.193","Link":"http://dx.doi.org/10.1109/TVCG.2013.193","FirstPage":2169,"LastPage":2178,"PaperType":"J","Abstract":"We suggest a methodology for analyzing movement behaviors of individuals moving in a group. Group movement is analyzed at two levels of granularity: the group as a whole and the individuals it comprises. For analyzing the relative positions and movements of the individuals with respect to the rest of the group, we apply space transformation, in which the trajectories of the individuals are converted from geographical space to an abstract 'group space'. The group space reference system is defined by both the position of the group center, which is taken as the coordinate origin, and the direction of the group's movement. Based on the individuals' positions mapped onto the group space, we can compare the behaviors of different individuals, determine their roles and/or ranks within the groups, and, possibly, understand how group movement is organized. The utility of the methodology has been evaluated by applying it to a set of real data concerning movements of wild social animals and discussing the results with experts in animal ethology.","AuthorNames-Deduped":"Natalia V. Andrienko;Gennady L. Andrienko;Louise Barrett;Marcus Dostie;S. Peter Henzi","AuthorNames":"Natalia Andrienko;Gennady Andrienko;Louise Barrett;Marcus Dostie;Peter Henzi","AuthorAffiliation":"Fraunhofer Institute IAIS;Fraunhofer Institute IAIS;University of Lethbridge and University of South Africa;University of Lethbridge and University of South Africa;University of Lethbridge and University of South Africa","InternalReferences":"10.1109/INFVIS.2005.1532142;10.1109/INFVIS.2004.27","AuthorKeywords":"Visual analytics, movement data, collective movement","AminerCitationCount_02-2020":30,"AminerCitationCount_06-2020":37,"XploreCitationCount - 2020-01":35,"PubsCited":37,"Award":null,"image":"6634194-fig-1-source-large.gif"},{"Conference":"VAST","Year":2013,"Title":"Space-Time Visual Analytics of Eye-Tracking Data for Dynamic Stimuli","DOI":"10.1109/TVCG.2013.194","Link":"http://dx.doi.org/10.1109/TVCG.2013.194","FirstPage":2129,"LastPage":2138,"PaperType":"J","Abstract":"We introduce a visual analytics method to analyze eye movement data recorded for dynamic stimuli such as video or animated graphics. The focus lies on the analysis of data of several viewers to identify trends in the general viewing behavior, including time sequences of attentional synchrony and objects with strong attentional focus. By using a space-time cube visualization in combination with clustering, the dynamic stimuli and associated eye gazes can be analyzed in a static 3D representation. Shot-based, spatiotemporal clustering of the data generates potential areas of interest that can be filtered interactively. We also facilitate data drill-down: the gaze points are shown with density-based color mapping and individual scan paths as lines in the space-time cube. The analytical process is supported by multiple coordinated views that allow the user to focus on different aspects of spatial and temporal information in eye gaze data. Common eye-tracking visualization techniques are extended to incorporate the spatiotemporal characteristics of the data. For example, heat maps are extended to motion-compensated heat maps and trajectories of scan paths are included in the space-time visualization. Our visual analytics approach is assessed in a qualitative users study with expert users, which showed the usefulness of the approach and uncovered that the experts applied different analysis strategies supported by the system.","AuthorNames-Deduped":"Kuno Kurzhals;Daniel Weiskopf","AuthorNames":"Kuno Kurzhals;Daniel Weiskopf","AuthorAffiliation":"University of Stuttgart.;University of Stuttgart.","InternalReferences":"10.1109/TVCG.2010.149;10.1109/TVCG.2011.193;10.1109/TVCG.2012.276;10.1109/TVCG.2006.194","AuthorKeywords":"Eye-tracking, space-time cube, dynamic areas of interest, spatiotemporal clustering, motion-compensated heat map","AminerCitationCount_02-2020":40,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":32,"PubsCited":47,"Award":null,"image":"6634139-fig-1-source-large.gif"},{"Conference":"VAST","Year":2013,"Title":"Supporting Awareness through Collaborative Brushing and Linking of Tabular Data","DOI":"10.1109/TVCG.2013.197","Link":"http://dx.doi.org/10.1109/TVCG.2013.197","FirstPage":2189,"LastPage":2197,"PaperType":"J","Abstract":"Maintaining an awareness of collaborators' actions is critical during collaborative work, including during collaborative visualization activities. Particularly when collaborators are located at a distance, it is important to know what everyone is working on in order to avoid duplication of effort, share relevant results in a timely manner and build upon each other's results. Can a person's brushing actions provide an indication of their queries and interests in a data set? Can these actions be revealed to a collaborator without substantially disrupting their own independent work? We designed a study to answer these questions in the context of distributed collaborative visualization of tabular data. Participants in our study worked independently to answer questions about a tabular data set, while simultaneously viewing brushing actions of a fictitious collaborator, shown directly within a shared workspace. We compared three methods of presenting the collaborator's actions: brushing &amp;amp; linking (i.e. highlighting exactly what the collaborator would see), selection (i.e. showing only a selected item), and persistent selection (i.e. showing only selected items but having them persist for some time). Our results demonstrated that persistent selection enabled some awareness of the collaborator's activities while causing minimal interference with independent work. Other techniques were less effective at providing awareness, and brushing &amp;amp; linking caused substantial interference. These findings suggest promise for the idea of exploiting natural brushing actions to provide awareness in collaborative work.","AuthorNames-Deduped":"Amir Hossein Hajizadeh;Melanie Tory;Rock Leung","AuthorNames":"Amir Hossein Hajizadeh;Melanie Tory;Rock Leung","AuthorAffiliation":"University of Victoria;University of Victoria;SAP","InternalReferences":"10.1109/TVCG.2011.196;10.1109/TVCG.2007.70541;10.1109/TVCG.2011.185;10.1109/VAST.2010.5652880;10.1109/INFVIS.2003.1249020;10.1109/VAST.2007.4389011;10.1109/VAST.2011.6102447","AuthorKeywords":"Collaboration, awareness, attentionally ambient visualization, brushing and linking, linked views, user study","AminerCitationCount_02-2020":9,"AminerCitationCount_06-2020":14,"XploreCitationCount - 2020-01":9,"PubsCited":36,"Award":null,"image":"6634130-fig-1-source-large.gif"},{"Conference":"VAST","Year":2013,"Title":"Supporting the Visual Analysis of Dynamic Networks by Clustering associated Temporal Attributes","DOI":"10.1109/TVCG.2013.198","Link":"http://dx.doi.org/10.1109/TVCG.2013.198","FirstPage":2267,"LastPage":2276,"PaperType":"J","Abstract":"The visual analysis of dynamic networks is a challenging task. In this paper, we introduce a new approach supporting the discovery of substructures sharing a similar trend over time by combining computation, visualization and interaction. With existing techniques, their discovery would be a tedious endeavor because of the number of nodes, edges as well as time points to be compared. First, on the basis of the supergraph, we therefore group nodes and edges according to their associated attributes that are changing over time. Second, the supergraph is visualized to provide an overview of the groups of nodes and edges with similar behavior over time in terms of their associated attributes. Third, we provide specific interactions to explore and refine the temporal clustering, allowing the user to further steer the analysis of the dynamic network. We demonstrate our approach by the visual analysis of a large wireless mesh network.","AuthorNames-Deduped":"Steffen Hadlak;Heidrun Schumann;Clemens H. Cap;Till Wollenberg","AuthorNames":"Steffen Hadlak;Heidrun Schumann;Clemens H. Cap;Till Wollenberg","AuthorAffiliation":"University of Rostock;University of Rostock;University of Rostock;University of Rostock","InternalReferences":"10.1109/INFVIS.2005.1532151;10.1109/VAST.2010.5652530;10.1109/INFVIS.2004.18;10.1109/TVCG.2011.226;10.1109/TVCG.2011.213;10.1109/TVCG.2006.193;10.1109/VAST.2012.6400493;10.1109/INFVIS.1999.801851;10.1109/TVCG.2007.70529;10.1109/INFVIS.2002.1173160","AuthorKeywords":"Dynamic networks, visualization, supergraph clustering","AminerCitationCount_02-2020":14,"AminerCitationCount_06-2020":22,"XploreCitationCount - 2020-01":16,"PubsCited":34,"Award":null,"image":"6634085-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2013,"Title":"Temporal Event Sequence Simplification","DOI":"10.1109/TVCG.2013.200","Link":"http://dx.doi.org/10.1109/TVCG.2013.200","FirstPage":2227,"LastPage":2236,"PaperType":"J","Abstract":"Electronic Health Records (EHRs) have emerged as a cost-effective data source for conducting medical research. The difficulty in using EHRs for research purposes, however, is that both patient selection and record analysis must be conducted across very large, and typically very noisy datasets. Our previous work introduced EventFlow, a visualization tool that transforms an entire dataset of temporal event records into an aggregated display, allowing researchers to analyze population-level patterns and trends. As datasets become larger and more varied, however, it becomes increasingly difficult to provide a succinct, summarizing display. This paper presents a series of user-driven data simplifications that allow researchers to pare event records down to their core elements. Furthermore, we present a novel metric for measuring visual complexity, and a language for codifying disjoint strategies into an overarching simplification framework. These simplifications were used by real-world researchers to gain new and valuable insights from initially overwhelming datasets.","AuthorNames-Deduped":"Megan Monroe;Rongjian Lan;Hanseung Lee;Catherine Plaisant;Ben Shneiderman","AuthorNames":"Megan Monroe;Rongjian Lan;Hanseung Lee;Catherine Plaisant;Ben Shneiderman","AuthorAffiliation":"Department of Computer Science &amp; Human-Computer Interaction Lab, University of Maryland;Department of Computer Science &amp; Human-Computer Interaction Lab, University of Maryland;Department of Computer Science &amp; Human-Computer Interaction Lab, University of Maryland;Department of Computer Science &amp; Human-Computer Interaction Lab, University of Maryland;Department of Computer Science &amp; Human-Computer Interaction Lab, University of Maryland","InternalReferences":"10.1109/TVCG.2009.117;10.1109/TVCG.2012.213;10.1109/VAST.2010.5652890","AuthorKeywords":"Event sequences, simplification, electronic heath records, temporal query","AminerCitationCount_02-2020":102,"AminerCitationCount_06-2020":139,"XploreCitationCount - 2020-01":104,"PubsCited":33,"Award":"HM","image":"6634100-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2013,"Title":"The Impact of Physical Navigation on Spatial Organization for Sensemaking","DOI":"10.1109/TVCG.2013.205","Link":"http://dx.doi.org/10.1109/TVCG.2013.205","FirstPage":2207,"LastPage":2216,"PaperType":"J","Abstract":"Spatial organization has been proposed as a compelling approach to externalizing the sensemaking process. However, there are two ways in which space can be provided to the user: by creating a physical workspace that the user can interact with directly, such as can be provided by a large, high-resolution display, or through the use of a virtual workspace that the user navigates using virtual navigation techniques such as zoom and pan. In this study we explicitly examined the use of spatial sensemaking techniques within these two environments. The results demonstrate that these two approaches to providing sensemaking space are not equivalent, and that the greater embodiment afforded by the physical workspace changes how the space is perceived and used, leading to increased externalization of the sensemaking process.","AuthorNames-Deduped":"Christopher Andrews;Chris North","AuthorNames":"Christopher Andrews;Chris North","AuthorAffiliation":"Middlebury College;Virginia Tech","InternalReferences":"10.1109/VAST.2012.6400559;10.1109/VAST.2008.4677358;10.1109/VAST.2009.5333878","AuthorKeywords":"Sensemaking, visual analytics, physical navigation, embodiment, large and high-resolution displays","AminerCitationCount_02-2020":9,"AminerCitationCount_06-2020":13,"XploreCitationCount - 2020-01":8,"PubsCited":32,"Award":null,"image":"6634176-fig-1-source-large.gif"},{"Conference":"VAST","Year":2013,"Title":"TimeBench: A Data Model and Software Library for Visual Analytics of Time-Oriented Data","DOI":"10.1109/TVCG.2013.206","Link":"http://dx.doi.org/10.1109/TVCG.2013.206","FirstPage":2247,"LastPage":2256,"PaperType":"J","Abstract":"Time-oriented data play an essential role in many Visual Analytics scenarios such as extracting medical insights from collections of electronic health records or identifying emerging problems and vulnerabilities in network traffic. However, many software libraries for Visual Analytics treat time as a flat numerical data type and insufficiently tackle the complexity of the time domain such as calendar granularities and intervals. Therefore, developers of advanced Visual Analytics designs need to implement temporal foundations in their application code over and over again. We present TimeBench, a software library that provides foundational data structures and algorithms for time-oriented data in Visual Analytics. Its expressiveness and developer accessibility have been evaluated through application examples demonstrating a variety of challenges with time-oriented data and long-term developer studies conducted in the scope of research and student projects.","AuthorNames-Deduped":"Alexander Rind;Tim Lammarsch;Wolfgang Aigner;Bilal Alsallakh;Silvia Miksch","AuthorNames":"Alexander Rind;Tim Lammarsch;Wolfgang Aigner;Bilal Alsallakh;Silvia Miksch","AuthorAffiliation":"Vienna University of Technology, Institute of Software Technology & Interactive Systems;Vienna University of Technology, Institute of Software Technology & Interactive Systems;Vienna University of Technology, Institute of Software Technology & Interactive Systems;Vienna University of Technology, Institute of Software Technology & Interactive Systems;Vienna University of Technology, Institute of Software Technology & Interactive Systems","InternalReferences":"10.1109/TVCG.2009.174;10.1109/INFVIS.2004.12;10.1109/VAST.2011.6102446;10.1109/VAST.2006.261428;10.1109/INFVIS.2000.885086;10.1109/TVCG.2010.144;10.1109/TVCG.2006.178;10.1109/INFVIS.2004.64;10.1109/TVCG.2013.222;10.1109/INFVIS.2002.1173155;10.1109/TVCG.2011.185;10.1109/TVCG.2010.126;10.1109/INFVIS.1997.636792","AuthorKeywords":"Visual Analytics, information visualization, toolkits, software infrastructure, time, temporal data","AminerCitationCount_02-2020":13,"AminerCitationCount_06-2020":16,"XploreCitationCount - 2020-01":13,"PubsCited":52,"Award":null,"image":"6634096-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2013,"Title":"Transformation of an Uncertain Video Search Pipeline to a Sketch-Based Visual Analytics Loop","DOI":"10.1109/TVCG.2013.207","Link":"http://dx.doi.org/10.1109/TVCG.2013.207","FirstPage":2109,"LastPage":2118,"PaperType":"J","Abstract":"Traditional sketch-based image or video search systems rely on machine learning concepts as their core technology. However, in many applications, machine learning alone is impractical since videos may not be semantically annotated sufficiently, there may be a lack of suitable training data, and the search requirements of the user may frequently change for different tasks. In this work, we develop a visual analytics systems that overcomes the shortcomings of the traditional approach. We make use of a sketch-based interface to enable users to specify search requirement in a flexible manner without depending on semantic annotation. We employ active machine learning to train different analytical models for different types of search requirements. We use visualization to facilitate knowledge discovery at the different stages of visual analytics. This includes visualizing the parameter space of the trained model, visualizing the search space to support interactive browsing, visualizing candidature search results to support rapid interaction for active learning while minimizing watching videos, and visualizing aggregated information of the search results. We demonstrate the system for searching spatiotemporal attributes from sports video to identify key instances of the team and player performance.","AuthorNames-Deduped":"Philip A. Legg;David H. S. Chung;Matthew L. Parry;Rhodri Bown;Mark W. Jones;Iwan W. Griffiths;Min Chen 0001","AuthorNames":"Philip A. Legg;David H.S. Chung;Matthew L. Parry;Rhodri Bown;Mark W. Jones;Iwan W. Griffiths;Min Chen","AuthorAffiliation":"Department of Computer Science, Swansea University, University of Oxford;Department of Computer Science, Swansea University;Department of Computer Science, Swansea University;Welsh Rugby Union;Department of Computer Science, Swansea University;Department of Engineering, Swansea University;e-Research Centre, University of Oxford","InternalReferences":"10.1109/TVCG.2006.138;10.1109/VISUAL.2003.1250401;10.1109/VAST.2009.5332628;10.1109/TVCG.2010.184;10.1109/VAST.2007.4389001;10.1109/TVCG.2008.131;10.1109/INFVIS.1998.729559;10.1109/TVCG.2006.194;10.1109/TVCG.2011.208","AuthorKeywords":"Visual knowledge discovery, data clustering, machine learning, multimedia visualization","AminerCitationCount_02-2020":11,"AminerCitationCount_06-2020":18,"XploreCitationCount - 2020-01":16,"PubsCited":42,"Award":null,"image":"6634165-fig-1-source-large.gif"},{"Conference":"VAST","Year":2013,"Title":"Using Interactive Visual Reasoning to Support Sense-Making: Implications for Design","DOI":"10.1109/TVCG.2013.211","Link":"http://dx.doi.org/10.1109/TVCG.2013.211","FirstPage":2217,"LastPage":2226,"PaperType":"J","Abstract":"This research aims to develop design guidelines for systems that support investigators and analysts in the exploration and assembly of evidence and inferences. We focus here on the problem of identifying candidate 'influencers' within a community of practice. To better understand this problem and its related cognitive and interaction needs, we conducted a user study using a system called INVISQUE (INteractive Visual Search and QUery Environment) loaded with content from the ACM Digital Library. INVISQUE supports search and manipulation of results over a freeform infinite 'canvas'. The study focuses on the representations user create and their reasoning process. It also draws on some pre-established theories and frameworks related to sense-making and cognitive work in general, which we apply as a 'theoretical lenses' to consider findings and articulate solutions. Analysing the user-study data in the light of these provides some understanding of how the high-level problem of identifying key players within a domain can translate into lower-level questions and interactions. This, in turn, has informed our understanding of representation and functionality needs at a level of description which abstracts away from the specifics of the problem at hand to the class of problems of interest. We consider the study outcomes from the perspective of implications for design.","AuthorNames-Deduped":"Neesha Kodagoda;Simon Attfield;B. L. William Wong;Chris Rooney;Sharmin (Tinni) Choudhury","AuthorNames":"Neesha Kodagoda;Simon Attfield;B.L. William Wong;Chris Rooney;Sharmin Choudhury","AuthorAffiliation":"School of Science and Technology, Middlesex University;School of Science and Technology, Middlesex University;School of Science and Technology, Middlesex University;School of Science and Technology, Middlesex University;School of Science and Technology, Middlesex University","InternalReferences":"10.1109/VAST.2009.5333020;10.1109/VAST.2007.4389006;10.1109/TVCG.2012.252","AuthorKeywords":"Visual analytics, sense-making, dataframe mode, evaluation, reasoning, analysis, interaction, interface design","AminerCitationCount_02-2020":11,"AminerCitationCount_06-2020":14,"XploreCitationCount - 2020-01":8,"PubsCited":36,"Award":null,"image":"6651935-fig-1-source-large.gif"},{"Conference":"VAST","Year":2013,"Title":"UTOPIAN: User-Driven Topic Modeling Based on Interactive Nonnegative Matrix Factorization","DOI":"10.1109/TVCG.2013.212","Link":"http://dx.doi.org/10.1109/TVCG.2013.212","FirstPage":1992,"LastPage":2001,"PaperType":"J","Abstract":"Topic modeling has been widely used for analyzing text document collections. Recently, there have been significant advancements in various topic modeling techniques, particularly in the form of probabilistic graphical modeling. State-of-the-art techniques such as Latent Dirichlet Allocation (LDA) have been successfully applied in visual text analytics. However, most of the widely-used methods based on probabilistic modeling have drawbacks in terms of consistency from multiple runs and empirical convergence. Furthermore, due to the complicatedness in the formulation and the algorithm, LDA cannot easily incorporate various types of user feedback. To tackle this problem, we propose a reliable and flexible visual analytics system for topic modeling called UTOPIAN (User-driven Topic modeling based on Interactive Nonnegative Matrix Factorization). Centered around its semi-supervised formulation, UTOPIAN enables users to interact with the topic modeling method and steer the result in a user-driven manner. We demonstrate the capability of UTOPIAN via several usage scenarios with real-world document corpuses such as InfoVis/VAST paper data set and product review data sets.","AuthorNames-Deduped":"Jaegul Choo;Changhyun Lee;Chandan K. Reddy;Haesun Park","AuthorNames":"Jaegul Choo;Changhyun Lee;Chandan K. Reddy;Haesun Park","AuthorAffiliation":"Georgia Institute of Technology;Georgia Institute of Technology;Wayne State University;Georgia Institute of Technology","InternalReferences":"10.1109/TVCG.2012.258;10.1109/VAST.2009.5332629;10.1109/TVCG.2011.239;10.1109/VAST.2011.6102461;10.1109/VAST.2012.6400485;10.1109/VAST.2007.4388999;10.1109/VAST.2007.4389006;10.1109/TVCG.2008.138;10.1109/VAST.2010.5652443","AuthorKeywords":"Latent Dirichlet allocation, nonnegative matrix factorization, topic modeling, visual analytics, interactive clustering, text analytics","AminerCitationCount_02-2020":91,"AminerCitationCount_06-2020":126,"XploreCitationCount - 2020-01":87,"PubsCited":36,"Award":null,"image":"6634167-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2013,"Title":"VAICo: Visual Analysis for Image Comparison","DOI":"10.1109/TVCG.2013.213","Link":"http://dx.doi.org/10.1109/TVCG.2013.213","FirstPage":2090,"LastPage":2099,"PaperType":"J","Abstract":"Scientists, engineers, and analysts are confronted with ever larger and more complex sets of data, whose analysis poses special challenges. In many situations it is necessary to compare two or more datasets. Hence there is a need for comparative visualization tools to help analyze differences or similarities among datasets. In this paper an approach for comparative visualization for sets of images is presented. Well-established techniques for comparing images frequently place them side-by-side. A major drawback of such approaches is that they do not scale well. Other image comparison methods encode differences in images by abstract parameters like color. In this case information about the underlying image data gets lost. This paper introduces a new method for visualizing differences and similarities in large sets of images which preserves contextual information, but also allows the detailed analysis of subtle variations. Our approach identifies local changes and applies cluster analysis techniques to embed them in a hierarchy. The results of this process are then presented in an interactive web application which allows users to rapidly explore the space of differences and drill-down on particular features. We demonstrate the flexibility of our approach by applying it to multiple distinct domains.","AuthorNames-Deduped":"Johanna Schmidt;M. Eduard Gröller;Stefan Bruckner","AuthorNames":"Johanna Schmidt;M. Eduard Gröller;Stefan Bruckner","AuthorAffiliation":"Vienna University of Technology;Vienna University of Technology;University of Bergen","InternalReferences":"10.1109/TVCG.2007.70584;10.1109/TVCG.2007.70623;10.1109/VAST.2012.6400555;10.1109/TVCG.2010.190;10.1109/VISUAL.1999.809871;10.1109/VISUAL.1999.809873;10.1109/TVCG.2011.248;10.1109/VISUAL.2002.1183790","AuthorKeywords":"Comparative visualization, focus+context visualization, image set comparison","AminerCitationCount_02-2020":16,"AminerCitationCount_06-2020":23,"XploreCitationCount - 2020-01":23,"PubsCited":41,"Award":null,"image":"6634107-fig-1-source-large.gif"},{"Conference":"VAST","Year":2013,"Title":"Vis4Heritage: Visual Analytics Approach on Grotto Wall Painting Degradations","DOI":"10.1109/TVCG.2013.219","Link":"http://dx.doi.org/10.1109/TVCG.2013.219","FirstPage":1982,"LastPage":1991,"PaperType":"J","Abstract":"For preserving the grotto wall paintings and protecting these historic cultural icons from the damage and deterioration in nature environment, a visual analytics framework and a set of tools are proposed for the discovery of degradation patterns. In comparison with the traditional analysis methods that used restricted scales, our method provides users with multi-scale analytic support to study the problems on site, cave, wall and particular degradation area scales, through the application of multidimensional visualization techniques. Several case studies have been carried out using real-world wall painting data collected from a renowned World Heritage site, to verify the usability and effectiveness of the proposed method. User studies and expert reviews were also conducted through by domain experts ranging from scientists such as microenvironment researchers, archivists, geologists, chemists, to practitioners such as conservators, restorers and curators.","AuthorNames-Deduped":"Jiawan Zhang;Kai Kang;Dajian Liu;Ye Yuan;E. Yanli","AuthorNames":"Jiawan Zhang;Kai Kang;Dajian Liu;Ye Yuan;Yanli E.","AuthorAffiliation":"School of Computer Software and Information Technology Research Center for Cultural Heritage Conservation and Promotion, Tianjin University;School of Computer Software, Tianjin University;School of Computer Software, Tianjin University;School of Computer Software, Tianjin University;School of Computer Software, Tianjin University","InternalReferences":"10.1109/TVCG.2011.239;10.1109/INFVIS.2004.1;10.1109/TVCG.2008.173;10.1109/INFVIS.2005.1532138;10.1109/TVCG.2006.147;10.1109/TVCG.2012.244;10.1109/VAST.2007.4389013;10.1109/TVCG.2008.153;10.1109/INFVIS.2000.885098","AuthorKeywords":"Cultural heritage, wall paintings, degradation, visual analytics","AminerCitationCount_02-2020":8,"AminerCitationCount_06-2020":10,"XploreCitationCount - 2020-01":7,"PubsCited":46,"Award":null,"image":"6634172-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2013,"Title":"Visual Analysis of Higher-Order Conjunctive Relationships in Multidimensional Data Using a Hypergraph Query System","DOI":"10.1109/TVCG.2013.220","Link":"http://dx.doi.org/10.1109/TVCG.2013.220","FirstPage":2070,"LastPage":2079,"PaperType":"J","Abstract":"Visual exploration and analysis of multidimensional data becomes increasingly difficult with increasing dimensionality. We want to understand the relationships between dimensions of data, but lack flexible techniques for exploration beyond low-order relationships. Current visual techniques for multidimensional data analysis focus on binary conjunctive relationships between dimensions. Recent techniques, such as cross-filtering on an attribute relationship graph, facilitate the exploration of some higher-order conjunctive relationships, but require a great deal of care and precision to do so effectively. This paper provides a detailed analysis of the expressive power of existing visual querying systems and describes a more flexible approach in which users can explore n-ary conjunctive inter- and intra- dimensional relationships by interactively constructing queries as visual hypergraphs. In a hypergraph query, nodes represent subsets of values and hyperedges represent conjunctive relationships. Analysts can dynamically build and modify the query using sequences of simple interactions. The hypergraph serves not only as a query specification, but also as a compact visual representation of the interactive state. Using examples from several domains, focusing on the digital humanities, we describe the design considerations for developing the querying system and incorporating it into visual analysis tools. We analyze query expressiveness with regard to the kinds of questions it can and cannot pose, and describe how it simultaneously expands the expressiveness of and is complemented by cross-filtering.","AuthorNames-Deduped":"Rachel Shadoan;Chris Weaver","AuthorNames":"Rachel Shadoan;Chris Weaver","AuthorAffiliation":"Akashic Labs LLC;School of Computer Science and the Center for Spatial Analysis at University of Oklahoma","InternalReferences":"10.1109/TVCG.2006.160;10.1109/INFVIS.2004.12;10.1109/VAST.2011.6102440;10.1109/VAST.2007.4389006;10.1109/TVCG.2006.166;10.1109/VAST.2010.5652520","AuthorKeywords":"Graph search, graph query language, multidimensional data, attribute relationship graphs, multivariate data analysis, higher-order conjunctive queries, visual query language, digital humanities","AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":9,"XploreCitationCount - 2020-01":8,"PubsCited":30,"Award":null,"image":""},{"Conference":"VAST","Year":2013,"Title":"Visual Analysis of Topic Competition on Social Media","DOI":"10.1109/TVCG.2013.221","Link":"http://dx.doi.org/10.1109/TVCG.2013.221","FirstPage":2012,"LastPage":2021,"PaperType":"J","Abstract":"How do various topics compete for public attention when they are spreading on social media? What roles do opinion leaders play in the rise and fall of competitiveness of various topics? In this study, we propose an expanded topic competition model to characterize the competition for public attention on multiple topics promoted by various opinion leaders on social media. To allow an intuitive understanding of the estimated measures, we present a timeline visualization through a metaphoric interpretation of the results. The visual design features both topical and social aspects of the information diffusion process by compositing ThemeRiver with storyline style visualization. ThemeRiver shows the increase and decrease of competitiveness of each topic. Opinion leaders are drawn as threads that converge or diverge with regard to their roles in influencing the public agenda change over time. To validate the effectiveness of the visual analysis techniques, we report the insights gained on two collections of Tweets: the 2012 United States presidential election and the Occupy Wall Street movement.","AuthorNames-Deduped":"Panpan Xu;Yingcai Wu;Enxun Wei;Tai-Quan Peng;Shixia Liu;Jonathan J. H. Zhu;Huamin Qu","AuthorNames":"Panpan Xu;Yingcai Wu;Enxun Wei;Tai-Quan Peng;Shixia Liu;Jonathan J.H. Zhu;Huamin Qu","AuthorAffiliation":"Hong Kong University of Science and Technology;Microsoft Research Asia;Shanghai Jiao Tong University;Nanyang Technological University;Microsoft Research Asia;City University of Hong Kong;Hong Kong University of Science and Technology","InternalReferences":"10.1109/TVCG.2008.166;10.1109/TVCG.2011.239;10.1109/TVCG.2012.253;10.1109/TVCG.2012.225;10.1109/VAST.2009.5333437;10.1109/TVCG.2010.194;10.1109/TVCG.2012.291;10.1109/VAST.2010.5652931;10.1109/TVCG.2013.196;10.1109/INFVIS.2001.963273;10.1109/TVCG.2012.212;10.1109/VAST.2010.5652922;10.1109/TVCG.2010.129;10.1109/INFVIS.1999.801851","AuthorKeywords":"Social media visuaization, topic competition, information diffusion, information propagation, agenda-setting","AminerCitationCount_02-2020":61,"AminerCitationCount_06-2020":78,"XploreCitationCount - 2020-01":53,"PubsCited":50,"Award":null,"image":"6634134-fig-1-source-large.gif"},{"Conference":"VAST","Year":2013,"Title":"Visual Analytics for Model Selection in Time Series Analysis","DOI":"10.1109/TVCG.2013.222","Link":"http://dx.doi.org/10.1109/TVCG.2013.222","FirstPage":2237,"LastPage":2246,"PaperType":"J","Abstract":"Model selection in time series analysis is a challenging task for domain experts in many application areas such as epidemiology, economy, or environmental sciences. The methodology used for this task demands a close combination of human judgement and automated computation. However, statistical software tools do not adequately support this combination through interactive visual interfaces. We propose a Visual Analytics process to guide domain experts in this task. For this purpose, we developed the TiMoVA prototype that implements this process based on user stories and iterative expert feedback on user experience. The prototype was evaluated by usage scenarios with an example dataset from epidemiology and interviews with two external domain experts in statistics. The insights from the experts' feedback and the usage scenarios show that TiMoVA is able to support domain experts in model selection tasks through interactive visual interfaces with short feedback cycles.","AuthorNames-Deduped":"Markus Bögl;Wolfgang Aigner;Peter Filzmoser;Tim Lammarsch;Silvia Miksch;Alexander Rind","AuthorNames":"Markus Bögl;Wolfgang Aigner;Peter Filzmoser;Tim Lammarsch;Silvia Miksch;Alexander Rind","AuthorAffiliation":"Vienna University of Technology;Vienna University of Technology;Vienna University of Technology;Vienna University of Technology;Vienna University of Technology;Vienna University of Technology","InternalReferences":"10.1109/TVCG.2013.206;10.1109/TVCG.2012.213;10.1109/TVCG.2007.70539","AuthorKeywords":"Visual analytics, model selection, visual interaction, time series analysis, coordinated & multiple views","AminerCitationCount_02-2020":19,"AminerCitationCount_06-2020":24,"XploreCitationCount - 2020-01":21,"PubsCited":42,"Award":null,"image":"6634112-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2013,"Title":"Visual Analytics for Multimodal Social Network Analysis: A Design Study with Social Scientists","DOI":"10.1109/TVCG.2013.223","Link":"http://dx.doi.org/10.1109/TVCG.2013.223","FirstPage":2032,"LastPage":2041,"PaperType":"J","Abstract":"Social network analysis (SNA) is becoming increasingly concerned not only with actors and their relations, but also with distinguishing between different types of such entities. For example, social scientists may want to investigate asymmetric relations in organizations with strict chains of command, or incorporate non-actors such as conferences and projects when analyzing coauthorship patterns. Multimodal social networks are those where actors and relations belong to different types, or modes, and multimodal social network analysis (mSNA) is accordingly SNA for such networks. In this paper, we present a design study that we conducted with several social scientist collaborators on how to support mSNA using visual analytics tools. Based on an openended, formative design process, we devised a visual representation called parallel node-link bands (PNLBs) that splits modes into separate bands and renders connections between adjacent ones, similar to the list view in Jigsaw. We then used the tool in a qualitative evaluation involving five social scientists whose feedback informed a second design phase that incorporated additional network metrics. Finally, we conducted a second qualitative evaluation with our social scientist collaborators that provided further insights on the utility of the PNLBs representation and the potential of visual analytics for mSNA.","AuthorNames-Deduped":"Sohaib Ghani;Bum Chul Kwon;Seungyoon Lee;Ji Soo Yi;Niklas Elmqvist","AuthorNames":"Sohaib Ghani;Bum Chul Kwon;Seungyoon Lee;Ji Soo Yi;Niklas Elmqvist","AuthorAffiliation":"School of Electrical and Computer Engineering, Purdue University;School of Industrial Engineering, Purdue University;Brian Lamb School of Communication, Purdue University;School of Industrial Engineering, Purdue University;School of Electrical and Computer Engineering, Purdue University","InternalReferences":"10.1109/TVCG.2011.247;10.1109/VAST.2011.6102440;10.1109/TVCG.2012.213;10.1109/TVCG.2011.201;10.1109/VAST.2007.4389006;10.1109/TVCG.2007.70521;10.1109/INFVIS.2005.1532142;10.1109/VISUAL.1990.146402;10.1109/TVCG.2007.70535;10.1109/INFVIS.2002.1173155;10.1109/VAST.2006.261430;10.1109/TVCG.2006.166;10.1109/TVCG.2011.209","AuthorKeywords":"Design study, user-centered design, node-link diagrams, multimodal graphs, interaction, qualitative evaluation","AminerCitationCount_02-2020":24,"AminerCitationCount_06-2020":36,"XploreCitationCount - 2020-01":31,"PubsCited":55,"Award":null,"image":"6634091-fig-1-source-large.gif"},{"Conference":"VAST","Year":2013,"Title":"Visual Analytics for Spatial Clustering: Using a Heuristic Approach for Guided Exploration","DOI":"10.1109/TVCG.2013.224","Link":"http://dx.doi.org/10.1109/TVCG.2013.224","FirstPage":2179,"LastPage":2188,"PaperType":"J","Abstract":"We propose a novel approach of distance-based spatial clustering and contribute a heuristic computation of input parameters for guiding users in the search of interesting cluster constellations. We thereby combine computational geometry with interactive visualization into one coherent framework. Our approach entails displaying the results of the heuristics to users, as shown in Figure 1, providing a setting from which to start the exploration and data analysis. Addition interaction capabilities are available containing visual feedback for exploring further clustering options and is able to cope with noise in the data. We evaluate, and show the benefits of our approach on a sophisticated artificial dataset and demonstrate its usefulness on real-world data.","AuthorNames-Deduped":"Eli Packer;Peter Bak;Mikko Nikkilä;Valentin Polishchuk;Harold J. Ship","AuthorNames":"Eli Packer;Peter Bak;Mikko Nikkilä;Valentin Polishchuk;Harold J. Ship","AuthorAffiliation":"IBM Research – Haifa / Israel, Smarter Decision Solutions Group;IBM Research – Haifa / Israel, Smarter Decision Solutions Group;Helsinki Institute for Information Technology, Computer Science Department, University of Helsinki, Finland;Helsinki Institute for Information Technology, Computer Science Department, University of Helsinki, Finland;IBM Research – Haifa / Israel, Smarter Decision Solutions Group","InternalReferences":"10.1109/VAST.2011.6102449;10.1109/INFVIS.2003.1249015;10.1109/VAST.2012.6400486;10.1109/TVCG.2009.122;10.1109/VAST.2010.5652443;10.1109/TVCG.2011.186","AuthorKeywords":"Heuristic-based spatial clustering, interactive visual clustering, k-order a-(alpha)-shapes","AminerCitationCount_02-2020":6,"AminerCitationCount_06-2020":9,"XploreCitationCount - 2020-01":6,"PubsCited":38,"Award":null,"image":"6634158-fig-1-source-large.gif"},{"Conference":"VAST","Year":2013,"Title":"Visual Exploration of Big Spatio-Temporal Urban Data: A Study of New York City Taxi Trips","DOI":"10.1109/TVCG.2013.226","Link":"http://dx.doi.org/10.1109/TVCG.2013.226","FirstPage":2149,"LastPage":2158,"PaperType":"J","Abstract":"As increasing volumes of urban data are captured and become available, new opportunities arise for data-driven analysis that can lead to improvements in the lives of citizens through evidence-based decision making and policies. In this paper, we focus on a particularly important urban data set: taxi trips. Taxis are valuable sensors and information associated with taxi trips can provide unprecedented insight into many different aspects of city life, from economic activity and human behavior to mobility patterns. But analyzing these data presents many challenges. The data are complex, containing geographical and temporal components in addition to multiple variables associated with each trip. Consequently, it is hard to specify exploratory queries and to perform comparative analyses (e.g., compare different regions over time). This problem is compounded due to the size of the data-there are on average 500,000 taxi trips each day in NYC. We propose a new model that allows users to visually query taxi trips. Besides standard analytics queries, the model supports origin-destination queries that enable the study of mobility across the city. We show that this model is able to express a wide range of spatio-temporal queries, and it is also flexible in that not only can queries be composed but also different aggregations and visual representations can be applied, allowing users to explore and compare results. We have built a scalable system that implements this model which supports interactive response times; makes use of an adaptive level-of-detail rendering strategy to generate clutter-free visualization for large results; and shows hidden details to the users in a summary through the use of overlay heat maps. We present a series of case studies motivated by traffic engineers and economists that show how our model and system enable domain experts to perform tasks that were previously unattainable for them.","AuthorNames-Deduped":"Nivan Ferreira;Jorge Poco;Huy T. Vo;Juliana Freire;Cláudio T. Silva","AuthorNames":"Nivan Ferreira;Jorge Poco;Huy T. Vo;Juliana Freire;Cláudio T. Silva","AuthorAffiliation":"NYU Poly;NYU Poly;NYU CUSP;NYU Poly;NYU Poly and NYU CUSP","InternalReferences":"10.1109/INFVIS.2004.12;10.1109/VAST.2008.4677356;10.1109/VAST.2011.6102454;10.1109/TVCG.2007.70535;10.1109/VAST.2010.5652467;10.1109/INFVIS.2005.1532150;10.1109/VAST.2008.4677370;10.1109/INFVIS.2000.885086","AuthorKeywords":"Spatio-temporal queries, urban data, taxi movement data, visual exploration","AminerCitationCount_02-2020":184,"AminerCitationCount_06-2020":267,"XploreCitationCount - 2020-01":220,"PubsCited":40,"Award":null,"image":"6634127-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2013,"Title":"Visual Traffic Jam Analysis Based on Trajectory Data","DOI":"10.1109/TVCG.2013.228","Link":"http://dx.doi.org/10.1109/TVCG.2013.228","FirstPage":2159,"LastPage":2168,"PaperType":"J","Abstract":"In this work, we present an interactive system for visual analysis of urban traffic congestion based on GPS trajectories. For these trajectories we develop strategies to extract and derive traffic jam information. After cleaning the trajectories, they are matched to a road network. Subsequently, traffic speed on each road segment is computed and traffic jam events are automatically detected. Spatially and temporally related events are concatenated in, so-called, traffic jam propagation graphs. These graphs form a high-level description of a traffic jam and its propagation in time and space. Our system provides multiple views for visually exploring and analyzing the traffic condition of a large city as a whole, on the level of propagation graphs, and on road segment level. Case studies with 24 days of taxi GPS trajectories collected in Beijing demonstrate the effectiveness of our system.","AuthorNames-Deduped":"Zuchao Wang;Min Lu;Xiaoru Yuan;Junping Zhang;Huub van de Wetering","AuthorNames":"Zuchao Wang;Min Lu;Xiaoru Yuan;Junping Zhang;Huub van de Wetering","AuthorAffiliation":"Key Laboratory of Machine Perception (Ministry of Education), and School of EECS, Peking University;Key Laboratory of Machine Perception (Ministry of Education), and School of EECS, Peking University, and Center for Computational Science and Engineering, Peking University;Key Laboratory of Machine Perception (Ministry of Education), and School of EECS, Peking University, and Center for Computational Science and Engineering, Peking University;Shanghai Key Laboratory of Intelligent Information Processing, and School of Computer Science, Fudan University;Department of Mathematics and Computer Science, Technische Universiteit Eindhoven","InternalReferences":"10.1109/VISUAL.1997.663866;10.1109/VAST.2011.6102454;10.1109/TVCG.2009.145;10.1109/VAST.2012.6400556;10.1109/INFVIS.2004.27;10.1109/VAST.2008.4677356;10.1109/TVCG.2011.202;10.1109/VAST.2012.6400553;10.1109/TVCG.2012.265;10.1109/TVCG.2011.181;10.1109/VAST.2009.5332593;10.1109/TVCG.2008.125;10.1109/VAST.2011.6102455","AuthorKeywords":"Traffic visualization, traffic jam propagation","AminerCitationCount_02-2020":121,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":138,"PubsCited":54,"Award":null,"image":"6634174-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2014,"Title":"Knowledge Generation Model for Visual Analytics","DOI":"10.1109/TVCG.2014.2346481","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346481","FirstPage":1604,"LastPage":1613,"PaperType":"J","Abstract":"Visual analytics enables us to analyze huge information spaces in order to support complex decision making and data exploration. Humans play a central role in generating knowledge from the snippets of evidence emerging from visual data analysis. Although prior research provides frameworks that generalize this process, their scope is often narrowly focused so they do not encompass different perspectives at different levels. This paper proposes a knowledge generation model for visual analytics that ties together these diverse frameworks, yet retains previously developed models (e.g., KDD process) to describe individual segments of the overall visual analytic processes. To test its utility, a real world visual analytics system is compared against the model, demonstrating that the knowledge generation process model provides a useful guideline when developing and evaluating such systems. The model is used to effectively compare different data analysis systems. Furthermore, the model provides a common language and description of visual analytic processes, which can be used for communication between researchers. At the end, our model reflects areas of research that future researchers can embark on.","AuthorNames-Deduped":"Dominik Sacha;Andreas Stoffel;Florian Stoffel;Bum Chul Kwon;Geoffrey P. Ellis;Daniel A. Keim","AuthorNames":"Dominik Sacha;Andreas Stoffel;Florian Stoffel;Bum Chul Kwon;Geoffrey Ellis;Daniel A. Keim","AuthorAffiliation":"Data Analysis and Visualization Group, University of Konstanz;Data Analysis and Visualization Group, University of Konstanz;Data Analysis and Visualization Group, University of Konstanz;Data Analysis and Visualization Group, University of Konstanz;Data Analysis and Visualization Group, University of Konstanz;Data Analysis and Visualization Group, University of Konstanz","InternalReferences":"10.1109/VISUAL.2005.1532781;10.1109/TVCG.2013.124;10.1109/VAST.2009.5333023;10.1109/TVCG.2011.229;10.1109/TVCG.2008.109;10.1109/VAST.2008.4677361;10.1109/VAST.2008.4677365;10.1109/VAST.2010.5652879;10.1109/TVCG.2012.273;10.1109/VAST.2008.4677358;10.1109/TVCG.2008.121;10.1109/VAST.2007.4389006;10.1109/VAST.2011.6102435;10.1109/TVCG.2013.120","AuthorKeywords":"Visual Analytics, Knowledge Generation, Reasoning, Visualization Taxonomies and Models, Interaction","AminerCitationCount_02-2020":69,"AminerCitationCount_06-2020":113,"XploreCitationCount - 2020-01":95,"PubsCited":43,"Award":null,"image":"20tvcg12-sacha-2346481-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"INFUSE: Interactive Feature Selection for Predictive Modeling of High Dimensional Data","DOI":"10.1109/TVCG.2014.2346482","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346482","FirstPage":1614,"LastPage":1623,"PaperType":"J","Abstract":"Predictive modeling techniques are increasingly being used by data scientists to understand the probability of predicted outcomes. However, for data that is high-dimensional, a critical step in predictive modeling is determining which features should be included in the models. Feature selection algorithms are often used to remove non-informative features from models. However, there are many different classes of feature selection algorithms. Deciding which one to use is problematic as the algorithmic output is often not amenable to user interpretation. This limits the ability for users to utilize their domain expertise during the modeling process. To improve on this limitation, we developed INFUSE, a novel visual analytics system designed to help analysts understand how predictive features are being ranked across feature selection algorithms, cross-validation folds, and classifiers. We demonstrate how our system can lead to important insights in a case study involving clinical researchers predicting patient outcomes from electronic medical records.","AuthorNames-Deduped":"Josua Krause;Adam Perer;Enrico Bertini","AuthorNames":"Josua Krause;Adam Perer;Enrico Bertini","AuthorAffiliation":"NYU Polytechnic School of Engineering;IBM T.J. Watson Research Center;NYU Polytechnic School of Engineering","InternalReferences":"10.1109/INFVIS.2004.71;10.1109/VAST.2009.5332586;10.1109/INFVIS.2005.1532142;10.1109/TVCG.2011.229;10.1109/VAST.2011.6102448;10.1109/INFVIS.2003.1249015;10.1109/TVCG.2011.178;10.1109/VAST.2011.6102453;10.1109/TVCG.2013.125;10.1109/TVCG.2009.153;10.1109/VAST.2010.5652443","AuthorKeywords":"Predictive modeling, feature selection, classification, visual analytics, high-dimensional data","AminerCitationCount_02-2020":36,"AminerCitationCount_06-2020":55,"XploreCitationCount - 2020-01":39,"PubsCited":23,"Award":null,"image":"20tvcg12-krause-2346482-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Transforming Scagnostics to Reveal Hidden Features","DOI":"10.1109/TVCG.2014.2346572","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346572","FirstPage":1624,"LastPage":1632,"PaperType":"J","Abstract":"Scagnostics (Scatterplot Diagnostics) were developed by Wilkinson et al. based on an idea of Paul and John Tukey, in order to discern meaningful patterns in large collections of scatterplots. The Tukeys' original idea was intended to overcome the impediments involved in examining large scatterplot matrices (multiplicity of plots and lack of detail). Wilkinson's implementation enabled for the first time scagnostics computations on many points as well as many plots. Unfortunately, scagnostics are sensitive to scale transformations. We illustrate the extent of this sensitivity and show how it is possible to pair statistical transformations with scagnostics to enable discovery of hidden structures in data that are not discernible in untransformed visualizations.","AuthorNames-Deduped":"Tommy Dang;Leland Wilkinson","AuthorNames":"Tuan Nhon Dang;Leland Wilkinson","AuthorAffiliation":"Department of Computer Science, University of Illinois at Chicago;Department of Computer Science, Skytree Software Inc.","InternalReferences":"10.1109/TVCG.2006.163;10.1109/INFVIS.2005.1532142;10.1109/TVCG.2013.187;10.1109/TVCG.2011.167;10.1109/VAST.2006.261423;10.1109/TVCG.2010.184;10.1109/VAST.2011.6102437;10.1109/VAST.2007.4389006","AuthorKeywords":"Scagnostics, Scatterplot matrix, Transformation, High-Dimensional Visual Analytics","AminerCitationCount_02-2020":6,"AminerCitationCount_06-2020":12,"XploreCitationCount - 2020-01":11,"PubsCited":44,"Award":null,"image":"20tvcg12-dang-2346572-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Supporting Communication and Coordination in Collaborative Sensemaking","DOI":"10.1109/TVCG.2014.2346573","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346573","FirstPage":1633,"LastPage":1642,"PaperType":"J","Abstract":"When people work together to analyze a data set, they need to organize their findings, hypotheses, and evidence, share that information with their collaborators, and coordinate activities amongst team members. Sharing externalizations (recorded information such as notes) could increase awareness and assist with team communication and coordination. However, we currently know little about how to provide tool support for this sort of sharing. We explore how linked common work (LCW) can be employed within a `collaborative thinking space', to facilitate synchronous collaborative sensemaking activities in Visual Analytics (VA). Collaborative thinking spaces provide an environment for analysts to record, organize, share and connect externalizations. Our tool, CLIP, extends earlier thinking spaces by integrating LCW features that reveal relationships between collaborators' findings. We conducted a user study comparing CLIP to a baseline version without LCW. Results demonstrated that LCW significantly improved analytic outcomes at a collaborative intelligence task. Groups using CLIP were also able to more effectively coordinate their work, and held more discussion of their findings and hypotheses. LCW enabled them to maintain awareness of each other's activities and findings and link those findings to their own work, preventing disruptive oral awareness notifications.","AuthorNames-Deduped":"Narges Mahyar;Melanie Tory","AuthorNames":"Narges Mahyar;Melanie Tory","AuthorAffiliation":"University of Victoria;University of Victoria","InternalReferences":"10.1109/VAST.2009.5333245;10.1109/VAST.2006.261439;10.1109/VAST.2008.4677358;10.1109/TVCG.2013.197;10.1109/VAST.2011.6102438;10.1109/VAST.2009.5333878;10.1109/VAST.2006.261430;10.1109/VAST.2007.4389011;10.1109/VAST.2007.4389006;10.1109/VAST.2011.6102447","AuthorKeywords":"Sensemaking, Collaboration, Externalization, Linked common work, Collaborative thinking space","AminerCitationCount_02-2020":36,"AminerCitationCount_06-2020":47,"XploreCitationCount - 2020-01":36,"PubsCited":43,"Award":"BP","image":"20tvcg12-frey-2346573-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2014,"Title":"Progressive Visual Analytics: User-Driven Visual Exploration of In-Progress Analytics","DOI":"10.1109/TVCG.2014.2346574","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346574","FirstPage":1653,"LastPage":1662,"PaperType":"J","Abstract":"As datasets grow and analytic algorithms become more complex, the typical workflow of analysts launching an analytic, waiting for it to complete, inspecting the results, and then re-Iaunching the computation with adjusted parameters is not realistic for many real-world tasks. This paper presents an alternative workflow, progressive visual analytics, which enables an analyst to inspect partial results of an algorithm as they become available and interact with the algorithm to prioritize subspaces of interest. Progressive visual analytics depends on adapting analytical algorithms to produce meaningful partial results and enable analyst intervention without sacrificing computational speed. The paradigm also depends on adapting information visualization techniques to incorporate the constantly refining results without overwhelming analysts and provide interactions to support an analyst directing the analytic. The contributions of this paper include: a description of the progressive visual analytics paradigm; design goals for both the algorithms and visualizations in progressive visual analytics systems; an example progressive visual analytics system (Progressive Insights) for analyzing common patterns in a collection of event sequences; and an evaluation of Progressive Insights and the progressive visual analytics paradigm by clinical researchers analyzing electronic medical records.","AuthorNames-Deduped":"Charles D. Stolper;Adam Perer;David Gotz","AuthorNames":"Charles D. Stolper;Adam Perer;David Gotz","AuthorAffiliation":"School of Interactive Computing, Georgia Institute of Technology;IBM T.J. Watson Research Center;University of North Carolina at Chapel Hill","InternalReferences":"10.1109/VAST.2006.261421;10.1109/TVCG.2013.227;10.1109/TVCG.2009.187;10.1109/TVCG.2011.179;10.1109/INFVIS.2005.1532133;10.1109/TVCG.2012.225;10.1109/TVCG.2013.179;10.1109/INFVIS.2000.885097;10.1109/TVCG.2013.200","AuthorKeywords":"Progressive visual analytics, information visualization, interactive machine learning, electronic medical records","AminerCitationCount_02-2020":60,"AminerCitationCount_06-2020":101,"XploreCitationCount - 2020-01":72,"PubsCited":43,"Award":null,"image":"20tvcg12-stolper-2346574-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2014,"Title":"Finding Waldo: Learning about Users from their Interactions","DOI":"10.1109/TVCG.2014.2346575","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346575","FirstPage":1663,"LastPage":1672,"PaperType":"J","Abstract":"Visual analytics is inherently a collaboration between human and computer. However, in current visual analytics systems, the computer has limited means of knowing about its users and their analysis processes. While existing research has shown that a user's interactions with a system reflect a large amount of the user's reasoning process, there has been limited advancement in developing automated, real-time techniques that mine interactions to learn about the user. In this paper, we demonstrate that we can accurately predict a user's task performance and infer some user personality traits by using machine learning techniques to analyze interaction data. Specifically, we conduct an experiment in which participants perform a visual search task, and apply well-known machine learning algorithms to three encodings of the users' interaction data. We achieve, depending on algorithm and encoding, between 62% and 83% accuracy at predicting whether each user will be fast or slow at completing the task. Beyond predicting performance, we demonstrate that using the same techniques, we can infer aspects of the user's personality factors, including locus of control, extraversion, and neuroticism. Further analyses show that strong results can be attained with limited observation time: in one case 95% of the final accuracy is gained after a quarter of the average task completion time. Overall, our findings show that interactions can provide information to the computer about its human collaborator, and establish a foundation for realizing mixed-initiative visual analytics systems.","AuthorNames-Deduped":"Eli T. Brown;Alvitta Ottley;Helen Zhao;Quan Lin;Richard Souvenir;Alex Endert;Remco Chang","AuthorNames":"Eli T Brown;Alvitta Ottley;Helen Zhao;Quan Lin;Richard Souvenir;Alex Endert;Remco Chang","AuthorAffiliation":"Tufts U;Tufts U;Tufts U;Tufts U;U.N.C. Charlotte;Pacific Northwest National Lab;Tufts U","InternalReferences":"10.1109/TVCG.2012.204;10.1109/VAST.2010.5653587;10.1109/VAST.2009.5333020;10.1109/VAST.2012.6400486;10.1109/VISUAL.2005.1532788;10.1109/TVCG.2012.276;10.1109/VAST.2006.261436;10.1109/VAST.2008.4677352","AuthorKeywords":"User Interactions, Analytic Provenance, Visualization, Applied Machine Learning","AminerCitationCount_02-2020":32,"AminerCitationCount_06-2020":46,"XploreCitationCount - 2020-01":37,"PubsCited":47,"Award":null,"image":"20tvcg12-brown-2346575-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Opening the Black Box: Strategies for Increased User Involvement in Existing Algorithm Implementations","DOI":"10.1109/TVCG.2014.2346578","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346578","FirstPage":1643,"LastPage":1652,"PaperType":"J","Abstract":"An increasing number of interactive visualization tools stress the integration with computational software like MATLAB and R to access a variety of proven algorithms. In many cases, however, the algorithms are used as black boxes that run to completion in isolation which contradicts the needs of interactive data exploration. This paper structures, formalizes, and discusses possibilities to enable user involvement in ongoing computations. Based on a structured characterization of needs regarding intermediate feedback and control, the main contribution is a formalization and comparison of strategies for achieving user involvement for algorithms with different characteristics. In the context of integration, we describe considerations for implementing these strategies either as part of the visualization tool or as part of the algorithm, and we identify requirements and guidelines for the design of algorithmic APIs. To assess the practical applicability, we provide a survey of frequently used algorithm implementations within R regarding the fulfillment of these guidelines. While echoing previous calls for analysis modules which support data exploration more directly, we conclude that a range of pragmatic options for enabling user involvement in ongoing computations exists on both the visualization and algorithm side and should be used.","AuthorNames-Deduped":"Thomas Mühlbacher;Harald Piringer;Samuel Gratzl;Michael Sedlmair;Marc Streit","AuthorNames":"Thomas Mühlbacher;Harald Piringer;Samuel Gratzl;Michael Sedlmair;Marc Streit","AuthorAffiliation":"VRVis Research Center, Vienna, Austria;VRVis Research Center, Vienna, Austria;Johannes Kepler University, Linz, Austria;University of Vienna, Austria;Johannes Kepler University, Linz, Austria","InternalReferences":"10.1109/VAST.2012.6400486;10.1109/VAST.2007.4388999;10.1109/VAST.2011.6102449;10.1109/TVCG.2007.70515;10.1109/TVCG.2009.151;10.1109/TVCG.2014.2346321;10.1109/VAST.2008.4677350;10.1109/TVCG.2006.171;10.1109/TVCG.2013.212;10.1109/TVCG.2013.125;10.1109/INFVIS.2002.1173145;10.1109/TVCG.2009.110;10.1109/INFVIS.2004.60;10.1109/VAST.2011.6102453;10.1109/TVCG.2012.195;10.1109/INFVIS.2003.1249014;10.1109/TVCG.2011.229","AuthorKeywords":"Visual analytics infrastructures, integration, interactive algorithms, user involvement, problem subdivision","AminerCitationCount_02-2020":45,"AminerCitationCount_06-2020":61,"XploreCitationCount - 2020-01":51,"PubsCited":52,"Award":null,"image":"20tvcg12-muhlbacher-2346578-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Interactive Visual Analysis of Image-Centric Cohort Study Data","DOI":"10.1109/TVCG.2014.2346591","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346591","FirstPage":1673,"LastPage":1682,"PaperType":"J","Abstract":"Epidemiological population studies impose information about a set of subjects (a cohort) to characterize disease-specific risk factors. Cohort studies comprise heterogenous variables describing the medical condition as well as demographic and lifestyle factors and, more recently, medical image data. We propose an Interactive Visual Analysis (IVA) approach that enables epidemiologists to rapidly investigate the entire data pool for hypothesis validation and generation. We incorporate image data, which involves shape-based object detection and the derivation of attributes describing the object shape. The concurrent investigation of image-based and non-image data is realized in a web-based multiple coordinated view system, comprising standard views from information visualization and epidemiological data representations such as pivot tables. The views are equipped with brushing facilities and augmented by 3D shape renderings of the segmented objects, e.g., each bar in a histogram is overlaid with a mean shape of the associated subgroup of the cohort. We integrate an overview visualization, clustering of variables and object shape for data-driven subgroup definition and statistical key figures for measuring the association between variables. We demonstrate the IVA approach by validating and generating hypotheses related to lower back pain as part of a qualitative evaluation.","AuthorNames-Deduped":"Paul Klemm;Steffen Oeltze-Jafra;Kai Lawonn;Katrin Hegenscheid;Henry Völzke;Bernhard Preim","AuthorNames":"Paul Klemm;Steffen Oeltze-Jafra;Kai Lawonn;Katrin Hegenscheid;Henry Völzke;Bernhard Preim","AuthorAffiliation":"Otto-von-Guericke University Magdeburg, Germany;Otto-von-Guericke University Magdeburg, Germany;Otto-von-Guericke University Magdeburg, Germany;Ernst-Moritz-Arndt University Greifswald, Germany;Ernst-Moritz-Arndt University Greifswald, Germany;Otto-von-Guericke University Magdeburg, Germany","InternalReferences":"10.1109/TVCG.2013.160;10.1109/TVCG.2011.185;10.1109/VISUAL.2000.885739;10.1109/TVCG.2011.217;10.1109/TVCG.2007.70569","AuthorKeywords":"Interactive Visual Analysis, Epidemiology, Spine","AminerCitationCount_02-2020":14,"AminerCitationCount_06-2020":19,"XploreCitationCount - 2020-01":16,"PubsCited":44,"Award":null,"image":"20tvcg12-klemm-2346591-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Visual Abstraction and Exploration of Multi-class Scatterplots","DOI":"10.1109/TVCG.2014.2346594","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346594","FirstPage":1683,"LastPage":1692,"PaperType":"J","Abstract":"Scatterplots are widely used to visualize scatter dataset for exploring outliers, clusters, local trends, and correlations. Depicting multi-class scattered points within a single scatterplot view, however, may suffer from heavy overdraw, making it inefficient for data analysis. This paper presents a new visual abstraction scheme that employs a hierarchical multi-class sampling technique to show a feature-preserving simplification. To enhance the density contrast, the colors of multiple classes are optimized by taking the multi-class point distributions into account. We design a visual exploration system that supports visual inspection and quantitative analysis from different perspectives. We have applied our system to several challenging datasets, and the results demonstrate the efficiency of our approach.","AuthorNames-Deduped":"Haidong Chen;Wei Chen 0001;Honghui Mei;Zhiqi Liu;Kun Zhou;Weifeng Chen 0002;Wentao Gu;Kwan-Liu Ma","AuthorNames":"Haidong Chen;Wei Chen;Honghui Mei;Zhiqi Liu;Kun Zhou;Weifeng Chen;Wentao Gu;Kwan-Liu Ma","AuthorAffiliation":"State Key Lab of CAD&CG;State Key Lab of CAD&CG;State Key Lab of CAD&CG;State Key Lab of CAD&CG;State Key Lab of CAD&CG;Zhejiang University of Finance & Economics;Zhejiang GongShang University;University of California at Davis","InternalReferences":"10.1109/TVCG.2013.150;10.1109/TVCG.2008.119;10.1109/VISUAL.1998.745301;10.1109/TVCG.2008.120;10.1109/TVCG.2010.197;10.1109/TVCG.2006.187;10.1109/TVCG.2007.70623;10.1109/TVCG.2013.180;10.1109/INFVIS.2004.52;10.1109/VAST.2010.5652460;10.1109/TVCG.2009.112;10.1109/TVCG.2009.122;10.1109/TVCG.2011.181;10.1109/TVCG.2012.238;10.1109/TVCG.2010.176;10.1109/TVCG.2013.212;10.1109/TVCG.2011.261;10.1109/TVCG.2008.153;10.1109/TVCG.2013.183","AuthorKeywords":"Scatterplot, overdraw reduction, sampling, visual abstraction","AminerCitationCount_02-2020":20,"AminerCitationCount_06-2020":36,"XploreCitationCount - 2020-01":41,"PubsCited":48,"Award":null,"image":"20tvcg12-chen-2346594-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Cupid: Cluster-Based Exploration of Geometry Generators with Parallel Coordinates and Radial Trees","DOI":"10.1109/TVCG.2014.2346626","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346626","FirstPage":1693,"LastPage":1702,"PaperType":"J","Abstract":"Geometry generators are commonly used in video games and evaluation systems for computer vision to create geometric shapes such as terrains, vegetation or airplanes. The parameters of the generator are often sampled automatically which can lead to many similar or unwanted geometric shapes. In this paper, we propose a novel visual exploration approach that combines the abstract parameter space of the geometry generator with the resulting 3D shapes in a composite visualization. Similar geometric shapes are first grouped using hierarchical clustering and then nested within an illustrative parallel coordinates visualization. This helps the user to study the sensitivity of the generator with respect to its parameter space and to identify invalid parameter settings. Starting from a compact overview representation, the user can iteratively drill-down into local shape differences by clicking on the respective clusters. Additionally, a linked radial tree gives an overview of the cluster hierarchy and enables the user to manually split or merge clusters. We evaluate our approach by exploring the parameter space of a cup generator and provide feedback from domain experts.","AuthorNames-Deduped":"Michael Beham;Wolfgang Herzner;M. Eduard Gröller;Johannes Kehrer","AuthorNames":"Michael Beham;Wolfgang Herzner;M. Eduard Gröller;Johannes Kehrer","AuthorAffiliation":"Vienna University of Technology;Austrian Institute of Technology;Vienna University of Technology;Vienna University of Technology","InternalReferences":"10.1109/TVCG.2013.147;10.1109/TVCG.2013.213;10.1109/TVCG.2010.138;10.1109/TVCG.2009.155;10.1109/VISUAL.2005.1532856;10.1109/TVCG.2010.190;10.1109/TVCG.2006.147;10.1109/VISUAL.1993.398859;10.1109/VISUAL.1999.809866;10.1109/TVCG.2007.70581","AuthorKeywords":"Composite visualization, hierarchical clustering, illustrative parallel coordinates, radial trees, 3D shape analysis","AminerCitationCount_02-2020":15,"AminerCitationCount_06-2020":20,"XploreCitationCount - 2020-01":16,"PubsCited":43,"Award":null,"image":"20tvcg12-beham-2346626-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Visual Methods for Analyzing Probabilistic Classification Data","DOI":"10.1109/TVCG.2014.2346660","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346660","FirstPage":1703,"LastPage":1712,"PaperType":"J","Abstract":"Multi-class classifiers often compute scores for the classification samples describing probabilities to belong to different classes. In order to improve the performance of such classifiers, machine learning experts need to analyze classification results for a large number of labeled samples to find possible reasons for incorrect classification. Confusion matrices are widely used for this purpose. However, they provide no information about classification scores and features computed for the samples. We propose a set of integrated visual methods for analyzing the performance of probabilistic classifiers. Our methods provide insight into different aspects of the classification results for a large number of samples. One visualization emphasizes at which probabilities these samples were classified and how these probabilities correlate with classification error in terms of false positives and false negatives. Another view emphasizes the features of these samples and ranks them by their separation power between selected true and false classifications. We demonstrate the insight gained using our technique in a benchmarking classification dataset, and show how it enables improving classification performance by interactively defining and evaluating post-classification rules.","AuthorNames-Deduped":"Bilal Alsallakh;Allan Hanbury;Helwig Hauser;Silvia Miksch;Andreas Rauber","AuthorNames":"Bilal Alsallakh;Allan Hanbury;Helwig Hauser;Silvia Miksch;Andreas Rauber","AuthorAffiliation":"Vienna University of Technology;Vienna University of Technology;University of Bergen;Vienna University of Technology;Vienna University of Technology","InternalReferences":"10.1109/VISUAL.2000.885740;10.1109/VAST.2010.5652398;10.1109/VAST.2009.5332628;10.1109/TVCG.2012.277;10.1109/VAST.2012.6400486;10.1109/TVCG.2013.184;10.1109/TVCG.2012.254;10.1109/VAST.2011.6102448;10.1109/VAST.2011.6102453;10.1109/VAST.2012.6400492;10.1109/VAST.2010.5652443","AuthorKeywords":"Probabilistic classification, confusion analysis, feature evaluation and selection, visual inspection","AminerCitationCount_02-2020":18,"AminerCitationCount_06-2020":33,"XploreCitationCount - 2020-01":23,"PubsCited":43,"Award":null,"image":"20tvcg12-alsallakh-2346660-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2014,"Title":"A Five-Level Design Framework for Bicluster Visualizations","DOI":"10.1109/TVCG.2014.2346665","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346665","FirstPage":1713,"LastPage":1722,"PaperType":"J","Abstract":"Analysts often need to explore and identify coordinated relationships (e.g., four people who visited the same five cities on the same set of days) within some large datasets for sensemaking. Biclusters provide a potential solution to ease this process, because each computed bicluster bundles individual relationships into coordinated sets. By understanding such computed, structural, relations within biclusters, analysts can leverage their domain knowledge and intuition to determine the importance and relevance of the extracted relationships for making hypotheses. However, due to the lack of systematic design guidelines, it is still a challenge to design effective and usable visualizations of biclusters to enhance their perceptibility and interactivity for exploring coordinated relationships. In this paper, we present a five-level design framework for bicluster visualizations, with a survey of the state-of-the-art design considerations and applications that are related or that can be applied to bicluster visualizations. We summarize pros and cons of these design options to support user tasks at each of the five-level relationships. Finally, we discuss future research challenges for bicluster visualizations and their incorporation into visual analytics tools.","AuthorNames-Deduped":"Maoyuan Sun;Chris North;Naren Ramakrishnan","AuthorNames":"Maoyuan Sun;Chris North;Naren Ramakrishnan","AuthorAffiliation":"Department of Computer Science, Discovery Analytics Center, Virginia Tech;Department of Computer Science, Discovery Analytics Center, Virginia Tech;Department of Computer Science, Discovery Analytics Center, Virginia Tech","InternalReferences":"10.1109/TVCG.2006.147;10.1109/TVCG.2009.153;10.1109/INFVIS.2005.1532126;10.1109/TVCG.2010.138;10.1109/VISUAL.1990.146402;10.1109/TVCG.2011.250;10.1109/TVCG.2006.160;10.1109/TVCG.2009.122;10.1109/VISUAL.1999.809866;10.1109/VAST.2006.261426;10.1109/INFVIS.2004.1;10.1109/VAST.2011.6102449;10.1109/TVCG.2013.167;10.1109/TVCG.2006.170;10.1109/TVCG.2007.70582","AuthorKeywords":"Biclusters, interactive visual analytics, coordinated relationships, design framework","AminerCitationCount_02-2020":15,"AminerCitationCount_06-2020":21,"XploreCitationCount - 2020-01":18,"PubsCited":93,"Award":null,"image":"20tvcg12-sun-2346665-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"VarifocalReader -- In-Depth Visual Analysis of Large Text Documents","DOI":"10.1109/TVCG.2014.2346677","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346677","FirstPage":1723,"LastPage":1732,"PaperType":"J","Abstract":"Interactive visualization provides valuable support for exploring, analyzing, and understanding textual documents. Certain tasks, however, require that insights derived from visual abstractions are verified by a human expert perusing the source text. So far, this problem is typically solved by offering overview-detail techniques, which present different views with different levels of abstractions. This often leads to problems with visual continuity. Focus-context techniques, on the other hand, succeed in accentuating interesting subsections of large text documents but are normally not suited for integrating visual abstractions. With VarifocalReader we present a technique that helps to solve some of these approaches' problems by combining characteristics from both. In particular, our method simplifies working with large and potentially complex text documents by simultaneously offering abstract representations of varying detail, based on the inherent structure of the document, and access to the text itself. In addition, VarifocalReader supports intra-document exploration through advanced navigation concepts and facilitates visual analysis tasks. The approach enables users to apply machine learning techniques and search mechanisms as well as to assess and adapt these techniques. This helps to extract entities, concepts and other artifacts from texts. In combination with the automatic generation of intermediate text levels through topic segmentation for thematic orientation, users can test hypotheses or develop interesting new research questions. To illustrate the advantages of our approach, we provide usage examples from literature studies.","AuthorNames-Deduped":"Steffen Koch;Markus John;Michael Wörner 0001;Andreas Müller 0012;Thomas Ertl","AuthorNames":"Steffen Koch;Markus John;Michael Wörner;Andreas Müller;Thomas Ertl","AuthorAffiliation":"Institute of Visualization and Interactive Systems (VIS);Institute of Visualization and Interactive Systems (VIS);Institute of Visualization and Interactive Systems (VIS);Institute for Natural Language Processing (IMS);Institute of Visualization and Interactive Systems (VIS)","InternalReferences":"10.1109/VAST.2010.5652926;10.1109/TVCG.2008.172;10.1109/VAST.2012.6400485;10.1109/TVCG.2013.188;10.1109/TVCG.2007.70577;10.1109/VAST.2012.6400486;10.1109/TVCG.2012.277;10.1109/TVCG.2009.165;10.1109/TVCG.2013.162;10.1109/INFVIS.1995.528686;10.1109/VAST.2009.5333248;10.1109/TVCG.2012.260;10.1109/VAST.2007.4389006;10.1109/VAST.2009.5333919;10.1109/VAST.2007.4389004","AuthorKeywords":"visual analytics, document analysis, literary analysis, natural language processing, text mining, machine learning, distant reading","AminerCitationCount_02-2020":25,"AminerCitationCount_06-2020":33,"XploreCitationCount - 2020-01":22,"PubsCited":48,"Award":null,"image":"20tvcg12-koch-2346677-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"DecisionFlow: Visual Analytics for High-Dimensional Temporal Event Sequence Data","DOI":"10.1109/TVCG.2014.2346682","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346682","FirstPage":1783,"LastPage":1792,"PaperType":"J","Abstract":"Temporal event sequence data is increasingly commonplace, with applications ranging from electronic medical records to financial transactions to social media activity. Previously developed techniques have focused on low-dimensional datasets (e.g., with less than 20 distinct event types). Real-world datasets are often far more complex. This paper describes DecisionFlow, a visual analysis technique designed to support the analysis of high-dimensional temporal event sequence data (e.g., thousands of event types). DecisionFlow combines a scalable and dynamic temporal event data structure with interactive multi-view visualizations and ad hoc statistical analytics. We provide a detailed review of our methods, and present the results from a 12-person user study. The study results demonstrate that DecisionFlow enables the quick and accurate completion of a range of sequence analysis tasks for datasets containing thousands of event types and millions of individual events.","AuthorNames-Deduped":"David Gotz;Harry Stavropoulos","AuthorNames":"David Gotz;Harry Stavropoulos","AuthorAffiliation":"University of North Carolina at Chapel Hill;IBM T.J. Watson Research Center","InternalReferences":"10.1109/TVCG.2013.206;10.1109/TVCG.2012.225;10.1109/TVCG.2011.179;10.1109/INFVIS.2000.885097;10.1109/VAST.2009.5332595;10.1109/VAST.2010.5652890;10.1109/TVCG.2009.117;10.1109/VAST.2006.261421;10.1109/TVCG.2013.200","AuthorKeywords":"Information Visualization, Temporal Event Sequences, Visual Analytics, Flow Diagrams, Medical Informatics","AminerCitationCount_02-2020":45,"AminerCitationCount_06-2020":67,"XploreCitationCount - 2020-01":62,"PubsCited":34,"Award":null,"image":"20tvcg12-gotz-2346682-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2014,"Title":"Footprints: A Visual Search Tool that Supports Discovery and Coverage Tracking","DOI":"10.1109/TVCG.2014.2346743","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346743","FirstPage":1793,"LastPage":1802,"PaperType":"J","Abstract":"Searching a large document collection to learn about a broad subject involves the iterative process of figuring out what to ask, filtering the results, identifying useful documents, and deciding when one has covered enough material to stop searching. We are calling this activity “discoverage,” discovery of relevant material and tracking coverage of that material. We built a visual analytic tool called Footprints that uses multiple coordinated visualizations to help users navigate through the discoverage process. To support discovery, Footprints displays topics extracted from documents that provide an overview of the search space and are used to construct searches visuospatially. Footprints allows users to triage their search results by assigning a status to each document (To Read, Read, Useful), and those status markings are shown on interactive histograms depicting the user's coverage through the documents across dates, sources, and topics. Coverage histograms help users notice biases in their search and fill any gaps in their analytic process. To create Footprints, we used a highly iterative, user-centered approach in which we conducted many evaluations during both the design and implementation stages and continually modified the design in response to feedback.","AuthorNames-Deduped":"Ellen Isaacs;Kelly Domico;Shane Ahern;Eugene Bart;Mudita Singhal","AuthorNames":"Ellen Isaacs;Kelly Damico;Shane Ahern;Eugene Bart;Mudita Singhal","AuthorAffiliation":"Palo Alto Research Center (PARC);Palo Alto Research Center (PARC);Palo Alto Research Center (PARC);Palo Alto Research Center (PARC);Palo Alto Research Center (PARC)","InternalReferences":"10.1109/VAST.2009.5333443;10.1109/VAST.2008.4677365;10.1109/VAST.2007.4389006;10.1109/INFVIS.2001.963287;10.1109/TVCG.2007.70589;10.1109/VAST.2006.261426;10.1109/TVCG.2007.70577","AuthorKeywords":"discovery search visualization, visual cues, discoverage, coverage tracking, document triage, interactive histograms","AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":5,"XploreCitationCount - 2020-01":4,"PubsCited":35,"Award":null,"image":"20tvcg12-isaacs-2346743-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2014,"Title":"Visual Analytics for Complex Engineering Systems: Hybrid Visual Steering of Simulation Ensembles","DOI":"10.1109/TVCG.2014.2346744","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346744","FirstPage":1803,"LastPage":1812,"PaperType":"J","Abstract":"In this paper we propose a novel approach to hybrid visual steering of simulation ensembles. A simulation ensemble is a collection of simulation runs of the same simulation model using different sets of control parameters. Complex engineering systems have very large parameter spaces so a naïve sampling can result in prohibitively large simulation ensembles. Interactive steering of simulation ensembles provides the means to select relevant points in a multi-dimensional parameter space (design of experiment). Interactive steering efficiently reduces the number of simulation runs needed by coupling simulation and visualization and allowing a user to request new simulations on the fly. As system complexity grows, a pure interactive solution is not always sufficient. The new approach of hybrid steering combines interactive visual steering with automatic optimization. Hybrid steering allows a domain expert to interactively (in a visualization) select data points in an iterative manner, approximate the values in a continuous region of the simulation space (by regression) and automatically find the “best” points in this continuous region based on the specified constraints and objectives (by optimization). We argue that with the full spectrum of optimization options, the steering process can be improved substantially. We describe an integrated system consisting of a simulation, a visualization, and an optimization component. We also describe typical tasks and propose an interactive analysis workflow for complex engineering systems. We demonstrate our approach on a case study from automotive industry, the optimization of a hydraulic circuit in a high pressure common rail Diesel injection system.","AuthorNames-Deduped":"Kresimir Matkovic;Denis Gracanin;Rainer Splechtna;Mario Jelovic;Benedikt Stehno;Helwig Hauser;Werner Purgathofer","AuthorNames":"Kreŝimir Matković;Denis Gračanin;Rainer Splechtna;Mario Jelović;Benedikt Stehno;Helwig Hauser;Werner Purgathofer","AuthorAffiliation":"VRVis Research Center, Vienna, Austria;Virginia Tech, Blacksburg, VA, USA;VRVis Research Center, Vienna, Austria;AVL-AST Zagreb, Croatia;VRVis Research Center, Vienna, Austria;University of Bergen, Norway;Vienna University of Technology, Austria","InternalReferences":"10.1109/TVCG.2010.223;10.1109/TVCG.2012.280;10.1109/TVCG.2008.145;10.1109/TVCG.2009.110;10.1109/TVCG.2010.171","AuthorKeywords":"Interactive Visual Analysis, Integrated Design Environment, Simulation, Visual Steering, Automatic Optimization","AminerCitationCount_02-2020":11,"AminerCitationCount_06-2020":18,"XploreCitationCount - 2020-01":15,"PubsCited":42,"Award":null,"image":"20tvcg12-matkovi-2346744-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Visual Exploration of Sparse Traffic Trajectory Data","DOI":"10.1109/TVCG.2014.2346746","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346746","FirstPage":1813,"LastPage":1822,"PaperType":"J","Abstract":"In this paper, we present a visual analysis system to explore sparse traffic trajectory data recorded by transportation cells. Such data contains the movements of nearly all moving vehicles on the major roads of a city. Therefore it is very suitable for macro-traffic analysis. However, the vehicle movements are recorded only when they pass through the cells. The exact tracks between two consecutive cells are unknown. To deal with such uncertainties, we first design a local animation, showing the vehicle movements only in the vicinity of cells. Besides, we ignore the micro-behaviors of individual vehicles, and focus on the macro-traffic patterns. We apply existing trajectory aggregation techniques to the dataset, studying cell status pattern and inter-cell flow pattern. Beyond that, we propose to study the correlation between these two patterns with dynamic graph visualization techniques. It allows us to check how traffic congestion on one cell is correlated with traffic flows on neighbouring links, and with route selection in its neighbourhood. Case studies show the effectiveness of our system.","AuthorNames-Deduped":"Zuchao Wang;Tangzhi Ye;Min Lu;Xiaoru Yuan;Huamin Qu;Jacky Yuan;Qianliang Wu","AuthorNames":"Zuchao Wang;Tangzhi Ye;Min Lu;Xiaoru Yuan;Huamin Qu;Jacky Yuan;Qianliang Wu","AuthorAffiliation":"Peking University;Peking University;Peking University;Peking University;Hong Kong University of Science and Technology;Nanjing Intelligent Transportation Systems Co., Ltd;Nanjing Intelligent Transportation Systems Co., Ltd","InternalReferences":"10.1109/VAST.2012.6400556;10.1109/INFVIS.2004.27;10.1109/VAST.2008.4677356;10.1109/INFVIS.2005.1532151;10.1109/VAST.2011.6102458;10.1109/TVCG.2011.226;10.1109/TVCG.2013.228;10.1109/TVCG.2013.193;10.1109/VAST.2009.5332584;10.1109/TVCG.2013.226;10.1109/VAST.2011.6102454;10.1109/VAST.2011.6102455;10.1109/TVCG.2009.182;10.1109/TVCG.2012.265","AuthorKeywords":"Sparse Traffic Trajectory, Traffic Visualization, Dynamic Graph Visualization, Traffic Congestion","AminerCitationCount_02-2020":41,"AminerCitationCount_06-2020":57,"XploreCitationCount - 2020-01":54,"PubsCited":46,"Award":null,"image":"20tvcg12-wang-2346746-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"DIA2: Web-based Cyberinfrastructure for Visual Analysis of Funding Portfolios","DOI":"10.1109/TVCG.2014.2346747","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346747","FirstPage":1823,"LastPage":1832,"PaperType":"J","Abstract":"We present a design study of the Deep Insights Anywhere, Anytime (DIA2) platform, a web-based visual analytics system that allows program managers and academic staff at the U.S. National Science Foundation to search, view, and analyze their research funding portfolio. The goal of this system is to facilitate users' understanding of both past and currently active research awards in order to make more informed decisions of their future funding. This user group is characterized by high domain expertise yet not necessarily high literacy in visualization and visual analytics-they are essentially casual experts-and thus require careful visual and information design, including adhering to user experience standards, providing a self-instructive interface, and progressively refining visualizations to minimize complexity. We discuss the challenges of designing a system for casual experts and highlight how we addressed this issue by modeling the organizational structure and workflows of the NSF within our system. We discuss each stage of the design process, starting with formative interviews, prototypes, and finally live deployments and evaluation with stakeholders.","AuthorNames-Deduped":"Krishna P. C. Madhavan;Niklas Elmqvist;Mihaela Vorvoreanu;Xin Chen;Yuet Ling Wong;Hanjun Xian;Zhihua Dong;Aditya Johri","AuthorNames":"Krishna Madhavan;Niklas Elmqvist;Mihaela Vorvoreanu;Xin Chen;Yuetling Wong;Hanjun Xian;Zhihua Dong;Aditya Johri","AuthorAffiliation":"Purdue University;Purdue University;Purdue University;Purdue University;Purdue University;Microsoft Corporation;Microsoft Corporation;George Mason University","InternalReferences":"10.1109/TVCG.2007.70541;10.1109/TVCG.2011.174;10.1109/TVCG.2010.177;10.1109/TVCG.2012.255;10.1109/TVCG.2009.123;10.1109/TVCG.2013.223;10.1109/INFVIS.2001.963283;10.1109/TVCG.2012.213;10.1109/VAST.2008.4677361","AuthorKeywords":"visual analytics, portfolio mining, web-based visualization, casual visualization, design study","AminerCitationCount_02-2020":10,"AminerCitationCount_06-2020":15,"XploreCitationCount - 2020-01":15,"PubsCited":40,"Award":null,"image":"20tvcg12-madhavan-2346747-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2014,"Title":"Visual Analytics for Comparison of Ocean Model Output with Reference Data: Detecting and Analyzing Geophysical Processes Using Clustering Ensembles","DOI":"10.1109/TVCG.2014.2346751","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346751","FirstPage":1893,"LastPage":1902,"PaperType":"J","Abstract":"Researchers assess the quality of an ocean model by comparing its output to that of a previous model version or to observations. One objective of the comparison is to detect and to analyze differences and similarities between both data sets regarding geophysical processes, such as particular ocean currents. This task involves the analysis of thousands or hundreds of thousands of geographically referenced temporal profiles in the data. To cope with the amount of data, modelers combine aggregation of temporal profiles to single statistical values with visual comparison. Although this strategy is based on experience and a well-grounded body of expert knowledge, our discussions with domain experts have shown that it has two limitations: (1) using a single statistical measure results in a rather limited scope of the comparison and in significant loss of information, and (2) the decisions modelers have to make in the process may lead to important aspects being overlooked.","AuthorNames-Deduped":"Patrick Köthur;Mike Sips;Henryk Dobslaw;Doris Dransch","AuthorNames":"Patrick Köthur;Mike Sips;Henryk Dobslaw;Doris Dransch","AuthorAffiliation":"GFZ German Research Centre for Geosciences, Potsdam, Germany;GFZ German Research Centre for Geosciences, Potsdam, Germany;GFZ German Research Centre for Geosciences, Potsdam, Germany;GFZ German Research Centre for Geosciences, Potsdam, Germany","InternalReferences":"10.1109/TVCG.2012.190;10.1109/TVCG.2012.284;10.1109/TVCG.2008.139","AuthorKeywords":"Ocean modeling, model assessment, geospatial time series, cluster ensembles, visual comparison, visual analytics","AminerCitationCount_02-2020":10,"AminerCitationCount_06-2020":17,"XploreCitationCount - 2020-01":17,"PubsCited":43,"Award":null,"image":"20tvcg12-kothur-2346751-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"ConTour: Data-Driven Exploration of Multi-Relational Datasets for Drug Discovery","DOI":"10.1109/TVCG.2014.2346752","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346752","FirstPage":1883,"LastPage":1892,"PaperType":"J","Abstract":"Large scale data analysis is nowadays a crucial part of drug discovery. Biologists and chemists need to quickly explore and evaluate potentially effective yet safe compounds based on many datasets that are in relationship with each other. However, there is a lack of tools that support them in these processes. To remedy this, we developed ConTour, an interactive visual analytics technique that enables the exploration of these complex, multi-relational datasets. At its core ConTour lists all items of each dataset in a column. Relationships between the columns are revealed through interaction: selecting one or multiple items in one column highlights and re-sorts the items in other columns. Filters based on relationships enable drilling down into the large data space. To identify interesting items in the first place, ConTour employs advanced sorting strategies, including strategies based on connectivity strength and uniqueness, as well as sorting based on item attributes. ConTour also introduces interactive nesting of columns, a powerful method to show the related items of a child column for each item in the parent column. Within the columns, ConTour shows rich attribute data about the items as well as information about the connection strengths to other datasets. Finally, ConTour provides a number of detail views, which can show items from multiple datasets and their associated data at the same time. We demonstrate the utility of our system in case studies conducted with a team of chemical biologists, who investigate the effects of chemical compounds on cells and need to understand the underlying mechanisms.","AuthorNames-Deduped":"Christian Partl;Alexander Lex;Marc Streit;Hendrik Strobelt;Anne Mai Wassermann;Hanspeter Pfister;Dieter Schmalstieg","AuthorNames":"Christian Partl;Alexander Lex;Marc Streit;Hendrik Strobelt;Anne-Mai Wassermann;Hanspeter Pfister;Dieter Schmalstieg","AuthorAffiliation":"Graz University of Technology;Harvard University;Johannes Kepler University Linz;Harvard University;Novartis Institutes for BicMedical Research;Harvard University;Graz University of Technology","InternalReferences":"10.1109/TVCG.2013.167;10.1109/TVCG.2012.213;10.1109/TVCG.2012.252;10.1109/VAST.2007.4389006;10.1109/TVCG.2006.166;10.1109/TVCG.2013.223","AuthorKeywords":"Multi-relational data, visual data analysis, drug discovery","AminerCitationCount_02-2020":9,"AminerCitationCount_06-2020":12,"XploreCitationCount - 2020-01":14,"PubsCited":30,"Award":null,"image":"20tvcg12-partl-2346752-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2014,"Title":"Genotet: An Interactive Web-based Visual Exploration Framework to Support Validation of Gene Regulatory Networks","DOI":"10.1109/TVCG.2014.2346753","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346753","FirstPage":1903,"LastPage":1912,"PaperType":"J","Abstract":"Elucidation of transcriptional regulatory networks (TRNs) is a fundamental goal in biology, and one of the most important components of TRNs are transcription factors (TFs), proteins that specifically bind to gene promoter and enhancer regions to alter target gene expression patterns. Advances in genomic technologies as well as advances in computational biology have led to multiple large regulatory network models (directed networks) each with a large corpus of supporting data and gene-annotation. There are multiple possible biological motivations for exploring large regulatory network models, including: validating TF-target gene relationships, figuring out co-regulation patterns, and exploring the coordination of cell processes in response to changes in cell state or environment. Here we focus on queries aimed at validating regulatory network models, and on coordinating visualization of primary data and directed weighted gene regulatory networks. The large size of both the network models and the primary data can make such coordinated queries cumbersome with existing tools and, in particular, inhibits the sharing of results between collaborators. In this work, we develop and demonstrate a web-based framework for coordinating visualization and exploration of expression data (RNA-seq, microarray), network models and gene-binding data (ChIP-seq). Using specialized data structures and multiple coordinated views, we design an efficient querying model to support interactive analysis of the data. Finally, we show the effectiveness of our framework through case studies for the mouse immune system (a dataset focused on a subset of key cellular functions) and a model bacteria (a small genome with high data-completeness).","AuthorNames-Deduped":"Bowen Yu;Harish Doraiswamy;Xi Chen;Emily R. Miraldi;Mario Luis Arrieta-Ortiz;Christoph Hafemeister;Aviv Madar;Richard Bonneau;Cláudio T. Silva","AuthorNames":"Bowen Yu;Harish Doraiswamy;Xi Chen;Emily Miraldi;Mario Luis Arrieta-Ortiz;Christoph Hafemeister;Aviv Madar;Richard Bonneau;Cláudio T. Silva","AuthorAffiliation":"NYU Polytechnic School of Engineering;NYU Polytechnic School of Engineering;NYU Center for Genomics and Systems Biology;NYU Center for Genomics and Systems Biology;NYU Center for Genomics and Systems Biology;NYU Center for Genomics and Systems Biology;Cornell University;NYU Center for Genomics and Systems Biology;NYU Polytechnic School of Engineering","InternalReferences":"10.1109/TVCG.2008.117;10.1109/TVCG.2009.146;10.1109/TVCG.2011.185;10.1109/TVCG.2009.167","AuthorKeywords":"Web-based visualization, gene regulatory network","AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":8,"XploreCitationCount - 2020-01":8,"PubsCited":32,"Award":null,"image":"20tvcg12-yu-2346753-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2014,"Title":"The Spinel Explorer - Interactive Visual Analysis of Spinel Group Minerals","DOI":"10.1109/TVCG.2014.2346754","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346754","FirstPage":1913,"LastPage":1922,"PaperType":"J","Abstract":"Geologists usually deal with rocks that are up to several thousand million years old. They try to reconstruct the tectonic settings where these rocks were formed and the history of events that affected them through the geological time. The spinel group minerals provide useful information regarding the geological environment in which the host rocks were formed. They constitute excellent indicators of geological environments (tectonic settings) and are of invaluable help in the search for mineral deposits of economic interest. The current workflow requires the scientists to work with different applications to analyze spine data. They do use specific diagrams, but these are usually not interactive. The current workflow hinders domain experts to fully exploit the potentials of tediously and expensively collected data. In this paper, we introduce the Spinel Explorer-an interactive visual analysis application for spinel group minerals. The design of the Spinel Explorer and of the newly introduced interactions is a result of a careful study of geologists' tasks. The Spinel Explorer includes most of the diagrams commonly used for analyzing spinel group minerals, including 2D binary plots, ternary plots, and 3D Spinel prism plots. Besides specific plots, conventional information visualization views are also integrated in the Spinel Explorer. All views are interactive and linked. The Spinel Explorer supports conventional statistics commonly used in spinel minerals exploration. The statistics views and different data derivation techniques are fully integrated in the system. Besides the Spinel Explorer as newly proposed interactive exploration system, we also describe the identified analysis tasks, and propose a new workflow. We evaluate the Spinel Explorer using real-life data from two locations in Argentina: the Frontal Cordillera in Central Andes and Patagonia. We describe the new findings of the geologists which would have been much more difficult to achieve using the current workflow only. Very positive feedback from geologists confirms the usefulness of the Spinel Explorer.","AuthorNames-Deduped":"Maria Luján Ganuza;Gabriela Ferracutti;Maria Florencia Gargiulo;Silvia Mabel Castro;Ernesto A. Bjerg;M. Eduard Gröller;Kresimir Matkovic","AuthorNames":"María Luján Ganuza;Gabriela Ferracutti;María Florencia Gargiulo;Silvia Mabel Castro;Ernesto Bjerg;Eduard Gröller;Krešimir Matković","AuthorAffiliation":"VyGLab Research Laboratory at the Universidad Nacional del Sur, Bahía Blanca, Argentina;Departamento de GeologíaINGEOSUR CCT CONICET, Universidad Nacional del Sur, Bahía Blanca, Argentina;Departamento de Geología, Universidad Nacional del Sur, Bahía Blanca, Argentina;VyGLab Research Laboratory at the Universidad Nacional del Sur, Bahía Blanca, Argentina;Departamento de GeologíaINGEOSUR CCT CONICET, Universidad Nacional del Sur, Bahía Blanca, Argentina;Vienna University of Technology, Austria;VRVis Research Center, Vienna, Austria","InternalReferences":"10.1109/INFVIS.2000.885086;10.1109/TVCG.2009.155;10.1109/VISUAL.1995.485139","AuthorKeywords":"Interactive visual analysis, visualization in earth/space/ and environmental sciences, coordinated and multiple views, design studies","AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":6,"XploreCitationCount - 2020-01":5,"PubsCited":29,"Award":null,"image":"20tvcg12-ganuza-2346754-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Visual Reconciliation of Alternative Similarity Spaces in Climate Modeling","DOI":"10.1109/TVCG.2014.2346755","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346755","FirstPage":1923,"LastPage":1932,"PaperType":"J","Abstract":"Visual data analysis often requires grouping of data objects based on their similarity. In many application domains researchers use algorithms and techniques like clustering and multidimensional scaling to extract groupings from data. While extracting these groups using a single similarity criteria is relatively straightforward, comparing alternative criteria poses additional challenges. In this paper we define visual reconciliation as the problem of reconciling multiple alternative similarity spaces through visualization and interaction. We derive this problem from our work on model comparison in climate science where climate modelers are faced with the challenge of making sense of alternative ways to describe their models: one through the output they generate, another through the large set of properties that describe them. Ideally, they want to understand whether groups of models with similar spatio-temporal behaviors share similar sets of criteria or, conversely, whether similar criteria lead to similar behaviors. We propose a visual analytics solution based on linked views, that addresses this problem by allowing the user to dynamically create, modify and observe the interaction among groupings, thereby making the potential explanations apparent. We present case studies that demonstrate the usefulness of our technique in the area of climate science.","AuthorNames-Deduped":"Jorge Poco;Aritra Dasgupta;Yaxing Wei;William W. Hargrove;Christopher R. Schwalm;Deborah N. Huntzinger;Robert B. Cook;Enrico Bertini;Cláudio T. Silva","AuthorNames":"Jorge Poco;Aritra Dasgupta;Yaxing Wei;William Hargrove;Christopher R. Schwalm;Deborah N. Huntzinger;Robert Cook;Enrico Bertini;Claudio T. Silva","AuthorAffiliation":"New York University;New York University and DataONE;Oak Ridge National Laboratory;USDA Forest Service;Northern Arizona University;Northern Arizona University;Oak Ridge National Laboratory;New York University;New York University","InternalReferences":"10.1109/TVCG.2008.139;10.1109/TVCG.2012.256;10.1109/VISUAL.2005.1532821;10.1109/TVCG.2013.157;10.1109/VAST.2012.6400486;10.1109/TVCG.2013.188;10.1109/TVCG.2013.224;10.1109/VAST.2008.4677350;10.1109/TVCG.2013.120","AuthorKeywords":"Similarity, clustering, matrix, optimization, climate model","AminerCitationCount_02-2020":7,"AminerCitationCount_06-2020":13,"XploreCitationCount - 2020-01":12,"PubsCited":42,"Award":null,"image":"20tvcg12-poco-2346755-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Visualizing Mobility of Public Transportation System","DOI":"10.1109/TVCG.2014.2346893","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346893","FirstPage":1833,"LastPage":1842,"PaperType":"J","Abstract":"Public transportation systems (PTSs) play an important role in modern cities, providing shared/massive transportation services that are essential for the general public. However, due to their increasing complexity, designing effective methods to visualize and explore PTS is highly challenging. Most existing techniques employ network visualization methods and focus on showing the network topology across stops while ignoring various mobility-related factors such as riding time, transfer time, waiting time, and round-the-clock patterns. This work aims to visualize and explore passenger mobility in a PTS with a family of analytical tasks based on inputs from transportation researchers. After exploring different design alternatives, we come up with an integrated solution with three visualization modules: isochrone map view for geographical information, isotime flow map view for effective temporal information comparison and manipulation, and OD-pair journey view for detailed visual analysis of mobility factors along routes between specific origin-destination pairs. The isotime flow map linearizes a flow map into a parallel isoline representation, maximizing the visualization of mobility information along the horizontal time axis while presenting clear and smooth pathways from origin to destinations. Moreover, we devise several interactive visual query methods for users to easily explore the dynamics of PTS mobility over space and time. Lastly, we also construct a PTS mobility model from millions of real passenger trajectories, and evaluate our visualization techniques with assorted case studies with the transportation researchers.","AuthorNames-Deduped":"Wei Zeng;Chi-Wing Fu;Stefan Müller Arisona;Alexander Erath;Huamin Qu","AuthorNames":"Wei Zeng;Chi-Wing Fu;Stefan Müller Arisona;Alexander Erath;Huamin Qu","AuthorAffiliation":"Nanyang Technological University, Singapore;Nanyang Technological University, Singapore;University of Applied Sciences;ETH Zurich;Hong Kong University of Science and Technology","InternalReferences":"10.1109/INFVIS.2001.963273;10.1109/TVCG.2011.202;10.1109/TVCG.2011.205;10.1109/TVCG.2009.143;10.1109/TVCG.2012.265;10.1109/TVCG.2013.228;10.1109/TVCG.2013.226;10.1109/VAST.2011.6102455;10.1109/INFVIS.2005.1532150","AuthorKeywords":"Mobility, public transportation, visual analytics","AminerCitationCount_02-2020":37,"AminerCitationCount_06-2020":47,"XploreCitationCount - 2020-01":42,"PubsCited":42,"Award":null,"image":"20tvcg12-zeng-2346893-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Visual Analysis of Public Utility Service Problems in a Metropolis","DOI":"10.1109/TVCG.2014.2346898","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346898","FirstPage":1843,"LastPage":1852,"PaperType":"J","Abstract":"Issues about city utility services reported by citizens can provide unprecedented insights into the various aspects of such services. Analysis of these issues can improve living quality through evidence-based decision making. However, these issues are complex, because of the involvement of spatial and temporal components, in addition to having multi-dimensional and multivariate natures. Consequently, exploring utility service problems and creating visual representations are difficult. To analyze these issues, we propose a visual analytics process based on the main tasks of utility service management. We also propose an aggregate method that transforms numerous issues into legible events and provide visualizations for events. In addition, we provide a set of tools and interaction techniques to explore such issues. Our approach enables administrators to make more informed decisions.","AuthorNames-Deduped":"Jiawan Zhang;E. Yanli;Jing Ma;Yahui Zhao;Binghan Xu;Liting Sun;Jinyan Chen;Xiaoru Yuan","AuthorNames":"Jiawan Zhang;E Yanli;Jing Ma;Yahui Zhao;Binghan Xu;Liting Sun;Jinyan Chen;Xiaoru Yuan","AuthorAffiliation":"School of Computer Science and Technology;School of Computer Science and Technology;SCS, Tianjin University;SCS, Tianjin University;SCS, Tianjin University;SCS, Tianjin University;SCS, Tianjin University;School of EECS","InternalReferences":"10.1109/VAST.2012.6400557;10.1109/VAST.2012.6400556;10.1109/TVCG.2013.228;10.1109/TVCG.2009.111;10.1109/VAST.2008.4677356;10.1109/TVCG.2012.291;10.1109/TVCG.2013.132;10.1109/TVCG.2013.226;10.1109/VAST.2011.6102455;10.1109/VAST.2011.6102460;10.1109/TVCG.2009.122","AuthorKeywords":"utility services, evidence-based decision making, visual analytics, aggregate","AminerCitationCount_02-2020":13,"AminerCitationCount_06-2020":18,"XploreCitationCount - 2020-01":14,"PubsCited":43,"Award":null,"image":"20tvcg12-zhang-2346898-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"VASA: Interactive Computational Steering of Large Asynchronous Simulation Pipelines for Societal Infrastructure","DOI":"10.1109/TVCG.2014.2346911","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346911","FirstPage":1853,"LastPage":1862,"PaperType":"J","Abstract":"We present VASA, a visual analytics platform consisting of a desktop application, a component model, and a suite of distributed simulation components for modeling the impact of societal threats such as weather, food contamination, and traffic on critical infrastructure such as supply chains, road networks, and power grids. Each component encapsulates a high-fidelity simulation model that together form an asynchronous simulation pipeline: a system of systems of individual simulations with a common data and parameter exchange format. At the heart of VASA is the Workbench, a visual analytics application providing three distinct features: (1) low-fidelity approximations of the distributed simulation components using local simulation proxies to enable analysts to interactively configure a simulation run; (2) computational steering mechanisms to manage the execution of individual simulation components; and (3) spatiotemporal and interactive methods to explore the combined results of a simulation run. We showcase the utility of the platform using examples involving supply chains during a hurricane as well as food contamination in a fast food restaurant chain.","AuthorNames-Deduped":"Sungahn Ko;Jieqiong Zhao;Jing Xia;Shehzad Afzal;Xiaoyu Wang;Greg Abram;Niklas Elmqvist;Len Kne;David Van Riper;Kelly P. Gaither;Shaun Kennedy;William J. Tolone;William Ribarsky;David S. Ebert","AuthorNames":"Sungahn Ko;Shaun Kennedy;William Tolone;William Ribarsky;David S. Ebert;Jieqiong Zhao;Jing Xia;Shehzad Afzal;Xiaoyu Wang;Greg Abram;Niklas Elmqvist;Len Kne;David Van Riper;Kelly Gaither","AuthorAffiliation":"Purdue University in West Lafayette, IN, USA;University of Minnesota in Minneapolis, MN, USA;University of North Carolina at Charlotte in Charlotte, NC, USA;University of North Carolina at Charlotte in Charlotte, NC, USA;Purdue University in West Lafayette, IN, USA;Purdue University in West Lafayette, IN, USA;State Key Lab of CAD&CG, China;Purdue University in West Lafayette, IN, USA;University of North Carolina at Charlotte in Charlotte, NC, USA;University of Texas at Austin in Austin, TX, USA;Purdue University in West Lafayette, IN, USA;University of Minnesota in Minneapolis, MN, USA;University of Minnesota in Minneapolis, MN, USA;University of Texas at Austin in Austin, TX, USA","InternalReferences":"10.1109/INFVIS.2000.885098;10.1109/TVCG.2011.225;10.1109/TVCG.2012.260;10.1109/TVCG.2007.70541;10.1109/TVCG.2010.223;10.1109/TVCG.2013.146;10.1109/TVCG.2010.171;10.1109/VAST.2011.6102460;10.1109/VAST.2011.6102457","AuthorKeywords":"Computational steering, visual analytics, critical infrastructure, homeland security","AminerCitationCount_02-2020":6,"AminerCitationCount_06-2020":9,"XploreCitationCount - 2020-01":5,"PubsCited":42,"Award":null,"image":"20tvcg12-ko-2346911-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"LoyalTracker: Visualizing Loyalty Dynamics in Search Engines","DOI":"10.1109/TVCG.2014.2346912","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346912","FirstPage":1733,"LastPage":1742,"PaperType":"J","Abstract":"The huge amount of user log data collected by search engine providers creates new opportunities to understand user loyalty and defection behavior at an unprecedented scale. However, this also poses a great challenge to analyze the behavior and glean insights into the complex, large data. In this paper, we introduce LoyalTracker, a visual analytics system to track user loyalty and switching behavior towards multiple search engines from the vast amount of user log data. We propose a new interactive visualization technique (flow view) based on a flow metaphor, which conveys a proper visual summary of the dynamics of user loyalty of thousands of users over time. Two other visualization techniques, a density map and a word cloud, are integrated to enable analysts to gain further insights into the patterns identified by the flow view. Case studies and the interview with domain experts are conducted to demonstrate the usefulness of our technique in understanding user loyalty and switching behavior in search engines.","AuthorNames-Deduped":"Conglei Shi;Yingcai Wu;Shixia Liu;Hong Zhou;Huamin Qu","AuthorNames":"Conglei Shi;Yingcai Wu;Shixia Liu;Hong Zhou;Huamin Qu","AuthorAffiliation":"Hong Kong University of Science and Technology;Microsoft Research Asia;Microsoft Research Asia;Shenzhen University;Hong Kong University of Science and Technology","InternalReferences":"10.1109/VAST.2010.5652931;10.1109/TVCG.2009.171;10.1109/VAST.2007.4389008;10.1109/TVCG.2012.253;10.1109/INFVIS.1996.559227;10.1109/TVCG.2012.212;10.1109/TVCG.2010.129;10.1109/TVCG.2012.225;10.1109/TVCG.2011.239;10.1109/VAST.2012.6400494;10.1109/INFVIS.2005.1532150;10.1109/TVCG.2008.166","AuthorKeywords":"Time-series visualization, stacked graphs, log data visualization, text visualization","AminerCitationCount_02-2020":14,"AminerCitationCount_06-2020":16,"XploreCitationCount - 2020-01":14,"PubsCited":48,"Award":"HM","image":"20tvcg12-shi-2346912-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"VAET: A Visual Analytics Approach for E-Transactions Time-Series","DOI":"10.1109/TVCG.2014.2346913","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346913","FirstPage":1743,"LastPage":1752,"PaperType":"J","Abstract":"Previous studies on E-transaction time-series have mainly focused on finding temporal trends of transaction behavior. Interesting transactions that are time-stamped and situation-relevant may easily be obscured in a large amount of information. This paper proposes a visual analytics system, Visual Analysis of E-transaction Time-Series (VAET), that allows the analysts to interactively explore large transaction datasets for insights about time-varying transactions. With a set of analyst-determined training samples, VAET automatically estimates the saliency of each transaction in a large time-series using a probabilistic decision tree learner. It provides an effective time-of-saliency (TOS) map where the analysts can explore a large number of transactions at different time granularities. Interesting transactions are further encoded with KnotLines, a compact visual representation that captures both the temporal variations and the contextual connection of transactions. The analysts can thus explore, select, and investigate knotlines of interest. A case study and user study with a real E-transactions dataset (26 million records) demonstrate the effectiveness of VAET.","AuthorNames-Deduped":"Cong Xie;Wei Chen 0001;Xinxin Huang;Yueqi Hu;Scott Barlowe;Jing Yang","AuthorNames":"Cong Xie;Wei Chen;Xinxin Huang;Yueqi Hu;Scott Barlowe;Jing Yang","AuthorAffiliation":"State Key Lab of CAD&CG, Zhejiang University;State Key Lab of CAD&CG, Zhejiang University;State Key Lab of CAD&CG, Zhejiang University;Dept. of Computer Science, University of North Carolina, Charlotte;Western Carolina University;Dept. of Computer Science, University of North Carolina, Charlotte","InternalReferences":"10.1109/TVCG.2009.123;10.1109/VAST.2007.4389009;10.1109/TVCG.2012.212;10.1109/INFVIS.1995.528685;10.1109/VAST.2012.6400494;10.1109/TVCG.2010.162;10.1109/TVCG.2009.180","AuthorKeywords":"Time-Series, Visual Analytics, E-transaction","AminerCitationCount_02-2020":14,"AminerCitationCount_06-2020":23,"XploreCitationCount - 2020-01":28,"PubsCited":27,"Award":null,"image":"20tvcg12-xie-2346913-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2014,"Title":"EvoRiver: Visual Analysis of Topic Coopetition on Social Media","DOI":"10.1109/TVCG.2014.2346919","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346919","FirstPage":1753,"LastPage":1762,"PaperType":"J","Abstract":"Cooperation and competition (jointly called “coopetition”) are two modes of interactions among a set of concurrent topics on social media. How do topics cooperate or compete with each other to gain public attention? Which topics tend to cooperate or compete with one another? Who plays the key role in coopetition-related interactions? We answer these intricate questions by proposing a visual analytics system that facilitates the in-depth analysis of topic coopetition on social media. We model the complex interactions among topics as a combination of carry-over, coopetition recruitment, and coopetition distraction effects. This model provides a close functional approximation of the coopetition process by depicting how different groups of influential users (i.e., “topic leaders”) affect coopetition. We also design EvoRiver, a time-based visualization, that allows users to explore coopetition-related interactions and to detect dynamically evolving patterns, as well as their major causes. We test our model and demonstrate the usefulness of our system based on two Twitter data sets (social topics data and business topics data).","AuthorNames-Deduped":"Guodao Sun;Yingcai Wu;Shixia Liu;Tai-Quan Peng;Jonathan J. H. Zhu;Ronghua Liang","AuthorNames":"Guodao Sun;Yingcai Wu;Shixia Liu;Tai-Quan Peng;Jonathan J. H. Zhu;Ronghua Liang","AuthorAffiliation":"Zhejiang University of Technology;Microsoft Research;Microsoft Research;Nanyang Technological University;City University of Hong Kong;Zhejiang University of Technology","InternalReferences":"10.1109/VAST.2010.5652931;10.1109/TVCG.2012.291;10.1109/TVCG.2008.166;10.1109/TVCG.2011.239;10.1109/TVCG.2012.253;10.1109/TVCG.2014.2346920;10.1109/TVCG.2013.221;10.1109/TVCG.2013.196;10.1109/TVCG.2013.162","AuthorKeywords":"Topic coopetition, information diffusion, information propagation, time-based visualization","AminerCitationCount_02-2020":40,"AminerCitationCount_06-2020":56,"XploreCitationCount - 2020-01":55,"PubsCited":46,"Award":null,"image":"20tvcg12-sun-2346919-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2014,"Title":"OpinionFlow: Visual Analysis of Opinion Diffusion on Social Media","DOI":"10.1109/TVCG.2014.2346920","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346920","FirstPage":1763,"LastPage":1772,"PaperType":"J","Abstract":"It is important for many different applications such as government and business intelligence to analyze and explore the diffusion of public opinions on social media. However, the rapid propagation and great diversity of public opinions on social media pose great challenges to effective analysis of opinion diffusion. In this paper, we introduce a visual analysis system called OpinionFlow to empower analysts to detect opinion propagation patterns and glean insights. Inspired by the information diffusion model and the theory of selective exposure, we develop an opinion diffusion model to approximate opinion propagation among Twitter users. Accordingly, we design an opinion flow visualization that combines a Sankey graph with a tailored density map in one view to visually convey diffusion of opinions among many users. A stacked tree is used to allow analysts to select topics of interest at different levels. The stacked tree is synchronized with the opinion flow visualization to help users examine and compare diffusion patterns across topics. Experiments and case studies on Twitter data demonstrate the effectiveness and usability of OpinionFlow.","AuthorNames-Deduped":"Yingcai Wu;Shixia Liu;Kai Yan;Mengchen Liu;Fangzhao Wu","AuthorNames":"Yingcai Wu;Shixia Liu;Kai Yan;Mengchen Liu;Fangzhao Wu","AuthorAffiliation":"Microsoft Research;Microsoft Research;Harbin Institute of Technology;Tsinghua University;Tsinghua University","InternalReferences":"10.1109/TVCG.2011.239;10.1109/TVCG.2013.162;10.1109/TVCG.2013.221;10.1109/TVCG.2014.2346433;10.1109/INFVIS.2005.1532152;10.1109/TVCG.2012.291;10.1109/VAST.2006.261431;10.1109/TVCG.2010.129;10.1109/TVCG.2013.196;10.1109/TVCG.2014.2346919;10.1109/TVCG.2010.183;10.1109/VAST.2009.5333919","AuthorKeywords":"Opinion visualization, opinion diffusion, opinion flow, influence estimation, kernel density estimation, level-of-detail","AminerCitationCount_02-2020":58,"AminerCitationCount_06-2020":82,"XploreCitationCount - 2020-01":70,"PubsCited":48,"Award":null,"image":"20tvcg12-wu-2346920-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2014,"Title":"#FluxFlow: Visual Analysis of Anomalous Information Spreading on Social Media","DOI":"10.1109/TVCG.2014.2346922","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346922","FirstPage":1773,"LastPage":1782,"PaperType":"J","Abstract":"We present FluxFlow, an interactive visual analysis system for revealing and analyzing anomalous information spreading in social media. Everyday, millions of messages are created, commented, and shared by people on social media websites, such as Twitter and Facebook. This provides valuable data for researchers and practitioners in many application domains, such as marketing, to inform decision-making. Distilling valuable social signals from the huge crowd's messages, however, is challenging, due to the heterogeneous and dynamic crowd behaviors. The challenge is rooted in data analysts' capability of discerning the anomalous information behaviors, such as the spreading of rumors or misinformation, from the rest that are more conventional patterns, such as popular topics and newsworthy events, in a timely fashion. FluxFlow incorporates advanced machine learning algorithms to detect anomalies, and offers a set of novel visualization designs for presenting the detected threads for deeper analysis. We evaluated FluxFlow with real datasets containing the Twitter feeds captured during significant events such as Hurricane Sandy. Through quantitative measurements of the algorithmic performance and qualitative interviews with domain experts, the results show that the back-end anomaly detection model is effective in identifying anomalous retweeting threads, and its front-end interactive visualizations are intuitive and useful for analysts to discover insights in data and comprehend the underlying analytical model.","AuthorNames-Deduped":"Jian Zhao 0010;Nan Cao;Zhen Wen;Yale Song;Yu-Ru Lin;Christopher Collins 0001","AuthorNames":"Jian Zhao;Nan Cao;Zhen Wen;Yale Song;Yu-Ru Lin;Christopher Collins","AuthorAffiliation":"University of Toronto;MIT;MIT;IBM J. Watson Research Center;University of Pittsburgh;UOIT","InternalReferences":"10.1109/VAST.2011.6102456;10.1109/TVCG.2012.291;10.1109/VAST.2012.6400557;10.1109/TVCG.2011.179;10.1109/TVCG.2011.239;10.1109/TVCG.2012.226;10.1109/TVCG.2013.227;10.1109/VAST.2012.6400485;10.1109/VAST.2010.5652922;10.1109/TVCG.2010.129;10.1109/TVCG.2013.221;10.1109/TVCG.2013.162","AuthorKeywords":"Retweeting threads, anomaly detection, social media, visual analytics, machine learning, information visualization","AminerCitationCount_02-2020":49,"AminerCitationCount_06-2020":69,"XploreCitationCount - 2020-01":57,"PubsCited":48,"Award":"HM","image":"20tvcg12-zhao-2346922-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Proactive Spatiotemporal Resource Allocation and Predictive Visual Analytics for Community Policing and Law Enforcement","DOI":"10.1109/TVCG.2014.2346926","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346926","FirstPage":1863,"LastPage":1872,"PaperType":"J","Abstract":"In this paper, we present a visual analytics approach that provides decision makers with a proactive and predictive environment in order to assist them in making effective resource allocation and deployment decisions. The challenges involved with such predictive analytics processes include end-users' understanding, and the application of the underlying statistical algorithms at the right spatiotemporal granularity levels so that good prediction estimates can be established. In our approach, we provide analysts with a suite of natural scale templates and methods that enable them to focus and drill down to appropriate geospatial and temporal resolution levels. Our forecasting technique is based on the Seasonal Trend decomposition based on Loess (STL) method, which we apply in a spatiotemporal visual analytics context to provide analysts with predicted levels of future activity. We also present a novel kernel density estimation technique we have developed, in which the prediction process is influenced by the spatial correlation of recent incidents at nearby locations. We demonstrate our techniques by applying our methodology to Criminal, Traffic and Civil (CTC) incident datasets.","AuthorNames-Deduped":"Abish Malik;Ross Maciejewski;Sherry Towers;Sean McCullough;David S. Ebert","AuthorNames":"Abish Malik;Ross Maciejewski;Sherry Towers;Sean McCullough;David S. Ebert","AuthorAffiliation":"Purdue University;Arizona State University;Arizona State University;Purdue University;Purdue University","InternalReferences":"10.1109/TVCG.2013.125;10.1109/TVCG.2013.206;10.1109/VAST.2012.6400491;10.1109/VAST.2007.4389006;10.1109/TVCG.2013.200","AuthorKeywords":"Visual Analytics, Natural Scales, Seasonal Trend decomposition based on Loess (STL), Law Enforcement","AminerCitationCount_02-2020":28,"AminerCitationCount_06-2020":39,"XploreCitationCount - 2020-01":29,"PubsCited":45,"Award":null,"image":"20tvcg12-malik-2346926-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Run Watchers: Automatic Simulation-Based Decision Support in Flood Management","DOI":"10.1109/TVCG.2014.2346930","Link":"http://dx.doi.org/10.1109/TVCG.2014.2346930","FirstPage":1873,"LastPage":1882,"PaperType":"J","Abstract":"In this paper, we introduce a simulation-based approach to design protection plans for flood events. Existing solutions require a lot of computation time for an exhaustive search, or demand for a time-consuming expert supervision and steering. We present a faster alternative based on the automated control of multiple parallel simulation runs. Run Watchers are dedicated system components authorized to monitor simulation runs, terminate them, and start new runs originating from existing ones according to domain-specific rules. This approach allows for a more efficient traversal of the search space and overall performance improvements due to a re-use of simulated states and early termination of failed runs. In the course of search, Run Watchers generate large and complex decision trees. We visualize the entire set of decisions made by Run Watchers using interactive, clustered timelines. In addition, we present visualizations to explain the resulting response plans. Run Watchers automatically generate storyboards to convey plan details and to justify the underlying decisions, including those which leave particular buildings unprotected. We evaluate our solution with domain experts.","AuthorNames-Deduped":"Artem Konev;Jürgen Waser;Bernhard Sadransky;Daniel Cornel;Rui A. P. Perdigão;Zsolt Horváth;M. Eduard Gröller","AuthorNames":"Artem Konev;Jürgen Waser;Bernhard Sadransky;Daniel Cornel;Rui A.P. Perdigão;Zsolt Horváth;M. Eduard Gröller","AuthorAffiliation":"VRVis Vienna;VRVis Vienna;VRVis Vienna;VRVis Vienna;TU Vienna;TU Vienna;TU Vienna","InternalReferences":"10.1109/INFVIS.2002.1173149;10.1109/VISUAL.2000.885727;10.1109/TVCG.2010.190;10.1109/TVCG.2011.248;10.1109/TVCG.2010.223;10.1109/TVCG.2008.145","AuthorKeywords":"Disaster management, simulation control, decision making, visual evidence, storytelling","AminerCitationCount_02-2020":6,"AminerCitationCount_06-2020":7,"XploreCitationCount - 2020-01":5,"PubsCited":36,"Award":null,"image":"20tvcg12-konev-2346930-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Towards Interactive, Intelligent, and Integrated Multimedia Analytics","DOI":"10.1109/VAST.2014.7042476","Link":"http://dx.doi.org/10.1109/VAST.2014.7042476","FirstPage":3,"LastPage":12,"PaperType":"C","Abstract":"The size and importance of visual multimedia collections grew rapidly over the last years, creating a need for sophisticated multimedia analytics systems enabling large-scale, interactive, and insightful analysis. These systems need to integrate the human's natural expertise in analyzing multimedia with the machine's ability to process large-scale data. The paper starts off with a comprehensive overview of representation, learning, and interaction techniques from both the human's and the machine's point of view. To this end, hundreds of references from the related disciplines (visual analytics, information visualization, computer vision, multimedia information retrieval) have been surveyed. Based on the survey, a novel general multimedia analytics model is synthesized. In the model, the need for semantic navigation of the collection is emphasized and multimedia analytics tasks are placed on the exploration-search axis. The axis is composed of both exploration and search in a certain proportion which changes as the analyst progresses towards insight. Categorization is proposed as a suitable umbrella task realizing the exploration-search axis in the model. Finally, the pragmatic gap, defined as the difference between the tight machine categorization model and the flexible human categorization model is identified as a crucial multimedia analytics topic.","AuthorNames-Deduped":"Jan Zahálka;Marcel Worring","AuthorNames":"Jan Zahálka;Marcel Worring","AuthorAffiliation":"University of Amsterdam;University of Amsterdam","InternalReferences":"10.1109/VAST.2006.261425;10.1109/VAST.2007.4389003;10.1109/INFVIS.2005.1532136;10.1109/TVCG.2010.136;10.1109/TVCG.2007.70515;10.1109/TVCG.2007.70541;10.1109/TVCG.2013.168","AuthorKeywords":"Multimedia (image/video/music) visualization, machine learning","AminerCitationCount_02-2020":9,"AminerCitationCount_06-2020":16,"XploreCitationCount - 2020-01":8,"PubsCited":100,"Award":null,"image":"7042476-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Feature-Driven Visual Analytics of Soccer Data","DOI":"10.1109/VAST.2014.7042477","Link":"http://dx.doi.org/10.1109/VAST.2014.7042477","FirstPage":13,"LastPage":22,"PaperType":"C","Abstract":"Soccer is one the most popular sports today and also very interesting from an scientific point of view. We present a system for analyzing high-frequency position-based soccer data at various levels of detail, allowing to interactively explore and analyze for movement features and game events. Our Visual Analytics method covers single-player, multi-player and event-based analytical views. Depending on the task the most promising features are semi-automatically selected, processed, and visualized. Our aim is to help soccer analysts in finding the most important and interesting events in a match. We present a flexible, modular, and expandable layer-based system allowing in-depth analysis. The integration of Visual Analytics techniques into the analysis process enables the analyst to find interesting events based on classification and allows, by a set of custom views, to communicate the found results. The feedback loop in the Visual Analytics pipeline helps to further improve the classification results. We evaluate our approach by investigating real-world soccer matches and collecting additional expert feedback. Several use cases and findings illustrate the capabilities of our approach.","AuthorNames-Deduped":"Halldór Janetzko;Dominik Sacha;Manuel Stein;Tobias Schreck;Daniel A. Keim;Oliver Deussen","AuthorNames":"Halld'or Janetzko;Dominik Sacha;Manuel Stein;Tobias Schreck;Daniel A. Keim;Oliver Deussen","AuthorAffiliation":"University of Konstanz;University of Konstanz;University of Konstanz;University of Konstanz;University of Konstanz;University of Konstanz","InternalReferences":"10.1109/TVCG.2012.263;10.1109/VAST.2008.4677350;10.1109/TVCG.2007.70621;10.1109/TVCG.2013.228;10.1109/TVCG.2013.193;10.1109/TVCG.2013.207;10.1109/TVCG.2013.186","AuthorKeywords":"Visual Analytics, Sport Analytics, Soccer Analysis","AminerCitationCount_02-2020":24,"AminerCitationCount_06-2020":38,"XploreCitationCount - 2020-01":24,"PubsCited":43,"Award":null,"image":"7042477-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2014,"Title":"Baseball4D: A Tool for Baseball Game Reconstruction & Visualization","DOI":"10.1109/VAST.2014.7042478","Link":"http://dx.doi.org/10.1109/VAST.2014.7042478","FirstPage":23,"LastPage":32,"PaperType":"C","Abstract":"While many sports use statistics and video to analyze and improve game play, baseball has led the charge throughout its history. With the advent of new technologies that allow all players and the ball to be tracked across the entire field, it is now possible to bring this understanding to another level. From discrete positions across time, we present techniques to reconstruct entire baseball games and visually explore each play. This provides opportunities to not only derive new metrics for the game, but also allow us to investigate existing measures with targeted visualizations. In addition, our techniques allow users to filter on demand so specific situations can be analyzed both in general and according to those situations. We show that gameplay can be accurately reconstructed from the raw position data and discuss how visualization and statistical methods can combine to better inform baseball analyses.","AuthorNames-Deduped":"Carlos A. Dietrich;David Koop;Huy T. Vo;Cláudio T. Silva","AuthorNames":"Carlos Dietrich;David Koop;Huy T. Vo;Cláudio T. Silva","AuthorAffiliation":"NYU;NYU;NYU","InternalReferences":"10.1109/TVCG.2012.263;10.1109/TVCG.2013.192;10.1109/TVCG.2012.225;10.1109/VISUAL.2001.964496","AuthorKeywords":"sports visualization, sports analytics, baseball, game reconstruction, baseball metrics, event data","AminerCitationCount_02-2020":7,"AminerCitationCount_06-2020":12,"XploreCitationCount - 2020-01":12,"PubsCited":35,"Award":null,"image":"7042478-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2014,"Title":"A System for Visual Analysis of Radio Signal Data","DOI":"10.1109/VAST.2014.7042479","Link":"http://dx.doi.org/10.1109/VAST.2014.7042479","FirstPage":33,"LastPage":42,"PaperType":"C","Abstract":"Analysis of radio transmissions is vital for military defense as it provides valuable information about enemy communication and infrastructure. One challenge to the data analysis task is that there are far too many signals for analysts to go through by hand. Even typical signal meta data (such as frequency band, duration, and geographic location) can be overwhelming. In this paper, we present a system for exploring and analyzing such radio signal meta-data. Our system incorporates several visual representations for signal data, designed for readability and ease of comparison, as well as novel algorithms for extracting and classifying consistent signal patterns. We demonstrate the effectiveness of our system using data collected from real missions with an airborne sensor platform.","AuthorNames-Deduped":"Tarik Crnovrsanin;Chris Muelder;Kwan-Liu Ma","AuthorNames":"Tarik Crnovrsanin;Chris Muelder;Kwan-Liu Ma","AuthorAffiliation":"VIDi @ U. C. Davis;VIDi @ U. C. Davis;VIDi @ U. C. Davis","InternalReferences":"10.1109/TVCG.2012.286;10.1109/VAST.2009.5332596;10.1109/INFVIS.2005.1532138;10.1109/VISUAL.1998.745302;10.1109/VAST.2009.5332593","AuthorKeywords":"Intelligence Analysis, Coordinated and Multiple Views, Time-varying data, Geographic/Geospatial Visualization","AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":2,"PubsCited":29,"Award":null,"image":"7042479-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Feedback-Driven Interactive Exploration of Large Multidimensional Data Supported by Visual Classifier","DOI":"10.1109/VAST.2014.7042480","Link":"http://dx.doi.org/10.1109/VAST.2014.7042480","FirstPage":43,"LastPage":52,"PaperType":"C","Abstract":"The extraction of relevant and meaningful information from multivariate or high-dimensional data is a challenging problem. One reason for this is that the number of possible representations, which might contain relevant information, grows exponentially with the amount of data dimensions. Also, not all views from a possibly large view space, are potentially relevant to a given analysis task or user. Focus+Context or Semantic Zoom Interfaces can help to some extent to efficiently search for interesting views or data segments, yet they show scalability problems for very large data sets. Accordingly, users are confronted with the problem of identifying interesting views, yet the manual exploration of the entire view space becomes ineffective or even infeasible. While certain quality metrics have been proposed recently to identify potentially interesting views, these often are defined in a heuristic way and do not take into account the application or user context. We introduce a framework for a feedback-driven view exploration, inspired by relevance feedback approaches used in Information Retrieval. Our basic idea is that users iteratively express their notion of interestingness when presented with candidate views. From that expression, a model representing the user's preferences, is trained and used to recommend further interesting view candidates. A decision support system monitors the exploration process and assesses the relevance-driven search process for convergence and stability. We present an instantiation of our framework for exploration of Scatter Plot Spaces based on visual features. We demonstrate the effectiveness of this implementation by a case study on two real-world datasets. We also discuss our framework in light of design alternatives and point out its usefulness for development of user- and context-dependent visual exploration systems.","AuthorNames-Deduped":"Michael Behrisch 0001;Fatih Korkmaz;Lin Shao;Tobias Schreck","AuthorNames":"Michael Behrisch;Fatih Korkmaz;Lin Shao;Tobias Schreck","AuthorAffiliation":"Universit&#x00E4;t Konstanz, Germany;Universit&#x00E4;t Konstanz, Germany;Universit&#x00E4;t Konstanz, Germany;Universit&#x00E4;t Konstanz, Germany","InternalReferences":"10.1109/INFVIS.2005.1532142;10.1109/TVCG.2012.277;10.1109/TVCG.2010.184;10.1109/VAST.2012.6400486;10.1109/VAST.2007.4389001;10.1109/TVCG.2013.160;10.1109/VAST.2012.6400488","AuthorKeywords":"View Space Exploration Framework, Interesting View Problem, Relevance Feedback, User Preference Model","AminerCitationCount_02-2020":21,"AminerCitationCount_06-2020":31,"XploreCitationCount - 2020-01":21,"PubsCited":37,"Award":null,"image":"7042480-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"An Integrated Visual Analysis System for Fusing MR Spectroscopy and Multi-Modal Radiology Imaging","DOI":"10.1109/VAST.2014.7042481","Link":"http://dx.doi.org/10.1109/VAST.2014.7042481","FirstPage":53,"LastPage":62,"PaperType":"C","Abstract":"For cancers such as glioblastoma multiforme, there is an increasing interest in defining \"biological target volumes\" (BTV), high tumour-burden regions which may be targeted with dose boosts in radiotherapy. The definition of a BTV requires insight into tumour characteristics going beyond conventionally defined radiological abnormalities and anatomical features. Molecular and biochemical imaging techniques, like positron emission tomography, the use of Magnetic Resonance (MR) Imaging contrast agents or MR Spectroscopy deliver this information and support BTV delineation. MR Spectroscopy Imaging (MRSI) is the only non-invasive technique in this list. Studies with MRSI have shown that voxels with certain metabolic signatures are more susceptible to predict the site of relapse. Nevertheless, the discovery of complex relationships between a high number of different metabolites, anatomical, molecular and functional features is an ongoing topic of research - still lacking appropriate tools supporting a smooth workflow by providing data integration and fusion of MRSI data with other imaging modalities. We present a solution bridging this gap which gives fast and flexible access to all data at once. By integrating a customized visualization of the multi-modal and multi-variate image data with a highly flexible visual analytics (VA) framework, it is for the first time possible to interactively fuse, visualize and explore user defined metabolite relations derived from MRSI in combination with markers delivered by other imaging modalities. Real-world medical cases demonstrate the utility of our solution. By making MRSI data available both in a VA tool and in a multi-modal visualization renderer we can combine insights from each side to arrive at a superior BTV delineation. We also report feedback from domain experts indicating significant positive impact in how this work can improve the understanding of MRSI data and its integration into radiotherapy planning.","AuthorNames-Deduped":"Miguel Nunes;Benjamin Rowland;Matthias Schlachter;Soléakhéna Ken;Kresimir Matkovic;Anne Laprie;Katja Bühler","AuthorNames":"Miguel Nunes;Benjamin Rowland;Matthias Schlachter;Soléakhéna Ken;Kresimir Matkovic;Anne Laprie;Katja Bühler","AuthorAffiliation":"VRVis Research Center, Vienna, Austria;Institut Claudius Regaud, Toulouse, France;VRVis Research Center, Vienna, Austria;Institut Claudius Regaud, Toulouse, France;VRVis Research Center, Vienna, Austria;Institut Claudius Regaud, Toulouse, France;VRVis Research Center, Vienna, Austria","InternalReferences":"10.1109/TVCG.2007.70569;10.1109/TVCG.2013.180;10.1109/TVCG.2010.176","AuthorKeywords":"MR spectroscopy, cancer, brain, visualization, multi-modality data, radiotherapy planning, medical decision support systems","AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":3,"PubsCited":29,"Award":null,"image":"7042481-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"An Insight- and Task-based Methodology for Evaluating Spatiotemporal Visual Analytics","DOI":"10.1109/VAST.2014.7042482","Link":"http://dx.doi.org/10.1109/VAST.2014.7042482","FirstPage":63,"LastPage":72,"PaperType":"C","Abstract":"We present a method for evaluating visualizations using both tasks and exploration, and demonstrate this method in a study of spatiotemporal network designs for a visual analytics system. The method is well suited for studying visual analytics applications in which users perform both targeted data searches and analyses of broader patterns. In such applications, an effective visualization design is one that helps users complete tasks accurately and efficiently, and supports hypothesis generation during open-ended exploration. To evaluate both of these aims in a single study, we developed an approach called layered insight- and task-based evaluation (LITE) that interposes several prompts for observations about the data model between sequences of predefined search tasks. We demonstrate the evaluation method in a user study of four network visualizations for spatiotemporal data in a visual analytics application. Results include findings that might have been difficult to obtain in a single experiment using a different methodology. For example, with one dataset we studied, we found that on average participants were faster on search tasks using a force-directed layout than using our other designs; at the same time, participants found this design least helpful in understanding the data. Our contributions include a novel evaluation method that combines well-defined tasks with exploration and observation, an evaluation of network visualization designs for spatiotemporal visual analytics, and guidelines for using this evaluation method.","AuthorNames-Deduped":"Steven R. Gomez;Hua Guo;Caroline Ziemkiewicz;David H. Laidlaw","AuthorNames":"Steven R. Gomez;Hua Guo;Caroline Ziemkiewicz;David H. Laidlaw","AuthorAffiliation":"Brown University;Brown University;Aptima, Inc;Brown University","InternalReferences":"10.1109/TVCG.2012.233;10.1109/TVCG.2007.70617;10.1109/TVCG.2013.124;10.1109/TVCG.2010.154;10.1109/TVCG.2013.126;10.1109/INFVIS.2005.1532136;10.1109/TVCG.2009.128;10.1109/TVCG.2011.185;10.1109/TVCG.2010.163;10.1109/TVCG.2013.120","AuthorKeywords":"Evaluation methodology, insight-based evaluation, visual analytics, network visualization, information visualization","AminerCitationCount_02-2020":8,"AminerCitationCount_06-2020":10,"XploreCitationCount - 2020-01":7,"PubsCited":31,"Award":null,"image":"7042482-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Weaving a Carpet from Log Entries: A Network Security Visualization Built with Co-Creation","DOI":"10.1109/VAST.2014.7042483","Link":"http://dx.doi.org/10.1109/VAST.2014.7042483","FirstPage":73,"LastPage":82,"PaperType":"C","Abstract":"We created a pixel map for multivariate data based on an analysis of the needs of network security engineers. Parameters of a log record are shown as pixels and these pixels are stacked to represent a record. This allows a broad view of a data set on one screen while staying very close to the raw data and to expose common and rare patterns of user behavior through the visualization itself (the \"Carpet\"). Visualizations that immediately point to areas of suspicious activity without requiring extensive filtering, help network engineers investigating unknown computer security incidents. Most of them, however, have limited knowledge of advanced visualization techniques, while many designers and data scientists are unfamiliar with computer security topics. To bridge this gap, we developed visualizations together with engineers, following a co-creative process. We will show how we explored the scope of the engineers' tasks and how we jointly developed ideas and designs. Our expert evaluation indicates that this visualization helps to scan large parts of log files quickly and to define areas of interest for closer inspection.","AuthorNames-Deduped":"Johannes Landstorfer;Ivo Herrmann;Jan-Erik Stange;Marian Dörk;Reto Wettach","AuthorNames":"Johannes Landstorfer;Ivo Herrmann;Jan-Erik Stange;Marian Dörk;Reto Wettach","AuthorAffiliation":"Department of Design at the University of Applied Sciences Potsdam, Germany;Department of Design at the University of Applied Sciences Potsdam, Germany;Department of Design at the University of Applied Sciences Potsdam, Germany;Department of Design at the University of Applied Sciences Potsdam, Germany;Department of Design at the University of Applied Sciences Potsdam, Germany","InternalReferences":"10.1109/TVCG.2006.160;10.1109/INFVIS.2005.1532134;10.1109/VISUAL.1991.175795;10.1109/VAST.2006.261436;10.1109/TVCG.2009.111;10.1109/INFVIS.1995.528685","AuthorKeywords":"Pixel-oriented techniques, task and requirements analysis, multidimensional data, network security and intrusion","AminerCitationCount_02-2020":6,"AminerCitationCount_06-2020":9,"XploreCitationCount - 2020-01":8,"PubsCited":35,"Award":null,"image":"7042483-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Analyzing High-dimensional Multivariate Network Links with Integrated Anomaly Detection, Highlighting and Exploration","DOI":"10.1109/VAST.2014.7042484","Link":"http://dx.doi.org/10.1109/VAST.2014.7042484","FirstPage":83,"LastPage":92,"PaperType":"C","Abstract":"This paper focuses on the integration of a family of visual analytics techniques for analyzing high-dimensional, multivariate network data that features spatial and temporal information, network connections, and a variety of other categorical and numerical data types. Such data types are commonly encountered in transportation, shipping, and logistics industries. Due to the scale and complexity of the data, it is essential to integrate techniques for data analysis, visualization, and exploration. We present new visual representations, Petal and Thread, to effectively present many-to-many network data including multi-attribute vectors. In addition, we deploy an information-theoretic model for anomaly detection across varying dimensions, displaying highlighted anomalies in a visually consistent manner, as well as supporting a managed process of exploration. Lastly, we evaluate the proposed methodology through data exploration and an empirical study.","AuthorNames-Deduped":"Sungahn Ko;Shehzad Afzal;Simon J. Walton;Yang Yang;Junghoon Chae;Abish Malik;Yun Jang;Min Chen 0001;David S. Ebert","AuthorNames":"Sungahnn Ko;Shehzad Afzal;Simon Walton;Yang Yang;Junghoon Chae;Abish Malik;Yun Jang;Min Chen;David Ebert","AuthorAffiliation":"Purdue University;Purdue University;Oxford University;Purdue University;Purdue University;Purdue University;Sejong University;Oxford University;Purdue University","InternalReferences":"10.1109/VAST.2012.6400554;10.1109/TVCG.2010.150;10.1109/TVCG.2007.70582;10.1109/TVCG.2011.190;10.1109/VAST.2011.6102440;10.1109/TVCG.2009.143;10.1109/INFVIS.1999.801851;10.1109/TVCG.2006.166;10.1109/VAST.2007.4389013","AuthorKeywords":null,"AminerCitationCount_02-2020":7,"AminerCitationCount_06-2020":null,"XploreCitationCount - 2020-01":8,"PubsCited":47,"Award":null,"image":"7042484-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2014,"Title":"Visual Analysis of Patterns in Multiple Amino Acid Mutation Graphs","DOI":"10.1109/VAST.2014.7042485","Link":"http://dx.doi.org/10.1109/VAST.2014.7042485","FirstPage":93,"LastPage":102,"PaperType":"C","Abstract":"Proteins are essential parts in all living organisms. They consist of sequences of amino acids. An interaction with reactive agent can stimulate a mutation at a specific position in the sequence. This mutation may set off a chain reaction, which effects other amino acids in the protein. Chain reactions need to be analyzed, as they may invoke unwanted side effects in drug treatment. A mutation chain is represented by a directed acyclic graph, where amino acids are connected by their mutation dependencies. As each amino acid may mutate individually, many mutation graphs exist. To determine important impacts of mutations, experts need to analyze and compare common patterns in these mutations graphs. Experts, however, lack suitable tools for this purpose. We present a new system for the search and the exploration of frequent patterns (i.e., motifs) in mutation graphs. We present a fast pattern search algorithm specifically developed for finding biologically relevant patterns in many mutation graphs (i.e., many labeled acyclic directed graphs). Our visualization system allows an interactive exploration and comparison of the found patterns. It enables locating the found patterns in the mutation graphs and in the 3D protein structures. In this way, potentially interesting patterns can be discovered. These patterns serve as starting point for a further biological analysis. In cooperation with biologists, we use our approach for analyzing a real world data set based on multiple HIV protease sequences.","AuthorNames-Deduped":"Olav Lenz;Frank Keul;Sebastian Bremm;Kay Hamacher;Tatiana von Landesberger","AuthorNames":"Olav Lenz;Frank Keul;Sebastian Bremm;Kay Hamacher;Tatiana von Landesberger","AuthorAffiliation":"GRIS, TU Darmstadt;Computational Biology, TU Darmstadt;GRIS, TU Darmstadt;Computational Biology, TU Darmstadt;GRIS, TU Darmstadt","InternalReferences":"10.1109/TVCG.2013.225;10.1109/VAST.2011.6102439;10.1109/VAST.2009.5333893;10.1109/TVCG.2009.167;10.1109/TVCG.2007.70521;10.1109/TVCG.2009.122;10.1109/TVCG.2007.70529","AuthorKeywords":"Biologic Visualization, Graph Visualization, Motif Search, Motif Visualization, Biology, Mutations, Pattern Visualization","AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":8,"XploreCitationCount - 2020-01":6,"PubsCited":51,"Award":null,"image":"7042485-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2014,"Title":"A Visual Reasoning Approach for Data-driven Transport Assessment on Urban Roads","DOI":"10.1109/VAST.2014.7042486","Link":"http://dx.doi.org/10.1109/VAST.2014.7042486","FirstPage":103,"LastPage":112,"PaperType":"C","Abstract":"Transport assessment plays a vital role in urban planning and traffic control, which are influenced by multi-faceted traffic factors involving road infrastructure and traffic flow. Conventional solutions can hardly meet the requirements and expectations of domain experts. In this paper we present a data-driven solution by leveraging a visual analysis system to evaluate the real traffic situations based on taxi trajectory data. A sketch-based visual interface is designed to support dynamic query and visual reasoning of traffic situations within multiple coordinated views. In particular, we propose a novel road-based query model for analysts to interactively conduct evaluation tasks. This model is supported by a bi-directional hash structure, TripHash, which enables real-time responses to the data queries over a huge amount of trajectory data. Case studies with a real taxi GPS trajectory dataset (&amp;gt; 30GB) show that our system performs well for on-demand transport assessment and reasoning.","AuthorNames-Deduped":"Fei Wang 0016;Wei Chen 0001;Feiran Wu;Ye Zhao;Han Hong;Tianyu Gu;Long Wang;Ronghua Liang;Hujun Bao","AuthorNames":"Fei Wang;Wei Chen;Feiran Wu;Ye Zhao;Han Hong;Tianyu Gu;Long Wang;Ronghua Liang;Hujun Bao","AuthorAffiliation":"State Key Lab of CAD&amp;CG, Zhejiang University;State Key Lab of CAD&amp;CG, Zhejiang University;State Key Lab of CAD&amp;CG, Zhejiang University;Kent State University;State Key Lab of CAD&amp;CG, Zhejiang University;State Key Lab of CAD&amp;CG, Zhejiang University;State Key Lab of CAD&amp;CG, Zhejiang University;Zhejiang University of Technology;State Key Lab of CAD&amp;CG, Zhejiang University","InternalReferences":"10.1109/VAST.2011.6102458;10.1109/TVCG.2013.226;10.1109/TVCG.2013.228;10.1109/TVCG.2013.179;10.1109/VAST.2011.6102455;10.1109/TVCG.2013.133","AuthorKeywords":"Road-based Query, Taxi Trajectory, Hash Index, Visual Analysis","AminerCitationCount_02-2020":13,"AminerCitationCount_06-2020":20,"XploreCitationCount - 2020-01":20,"PubsCited":48,"Award":null,"image":"7042486-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Using Visualizations to Monitor Changes and Harvest Insights from a Global-Scale Logging Infrastructure at Twitter","DOI":"10.1109/VAST.2014.7042487","Link":"http://dx.doi.org/10.1109/VAST.2014.7042487","FirstPage":113,"LastPage":122,"PaperType":"C","Abstract":"Logging user activities is essential to data analysis for internet products and services. Twitter has built a unified logging infrastructure that captures user activities across all clients it owns, making it one of the largest datasets in the organization. This paper describes challenges and opportunities in applying information visualization to log analysis at this massive scale, and shows how various visualization techniques can be adapted to help data scientists extract insights. In particular, we focus on two scenarios: (1) monitoring and exploring a large collection of log events, and (2) performing visual funnel analysis on log data with tens of thousands of event types. Two interactive visualizations were developed for these purposes: we discuss design choices and the implementation of these systems, along with case studies of how they are being used in day-to-day operations at Twitter.","AuthorNames-Deduped":"Krist Wongsuphasawat;Jimmy Lin","AuthorNames":"Krist Wongsuphasawat;Jimmy Lin","AuthorAffiliation":"Twitter, Inc.;Twitter, Inc.","InternalReferences":"10.1109/INFVIS.2000.885091;10.1109/TVCG.2009.117;10.1109/INFVIS.1997.636718;10.1109/VAST.2007.4389008;10.1109/INFVIS.1996.559227;10.1109/TVCG.2012.225;10.1109/TVCG.2007.70529;10.1109/VAST.2012.6400494;10.1109/TVCG.2013.231;10.1109/INFVIS.2004.64;10.1109/VISUAL.1991.175815;10.1109/TVCG.2013.200;10.1109/VAST.2006.261421;10.1109/TVCG.2011.185","AuthorKeywords":"Information Visualization, Visual Analytics, Log Analysis, Log Visualization, Session Analysis, Funnel Analysis","AminerCitationCount_02-2020":11,"AminerCitationCount_06-2020":20,"XploreCitationCount - 2020-01":13,"PubsCited":45,"Award":null,"image":"7042487-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2014,"Title":"HydroQual: Visual Analysis of River Water Quality","DOI":"10.1109/VAST.2014.7042488","Link":"http://dx.doi.org/10.1109/VAST.2014.7042488","FirstPage":123,"LastPage":132,"PaperType":"C","Abstract":"Economic development based on industrialization, intensive agriculture expansion and population growth places greater pressure on water resources through increased water abstraction and water quality degradation [40], River pollution is now a visible issue, with emblematic ecological disasters following industrial accidents such as the pollution of the Rhine river in 1986 [31]. River water quality is a pivotal public health and environmental issue that has prompted governments to plan initiatives for preserving or restoring aquatic ecosystems and water resources [56], Water managers require operational tools to help interpret the complex range of information available on river water quality functioning. Tools based on statistical approaches often fail to resolve some tasks due to the sparse nature of the data. Here we describe HydroQual, a tool to facilitate visual analysis of river water quality. This tool combines spatiotemporal data mining and visualization techniques to perform tasks defined by water experts. We illustrate the approach with a case study that illustrates how the tool helps experts analyze water quality. We also perform a qualitative evaluation with these experts.","AuthorNames-Deduped":"Pierre Accorsi;Nathalie Lalande;Mickaël Fabrègue;Agnès Braud;Pascal Poncelet;Arnaud Sallaberry;Sandra Bringay;Maguelonne Teisseire;Flavie Cernesson;Florence Le Ber","AuthorNames":"Pierre Accorsi;Nathalie Lalande;Mickäel Fabrègue;Agnès Braud;Pascal Poncelet;Arnaud Sallaberry;Sandra Bringay;Maguelonne Teisseire;Flavie Cernesson;Florence Le Ber","AuthorAffiliation":"LIRMM Univ. Montpellier 2;IRSTEA Montpellier;IRSTEA Montpellier Univ. Strasbourg/ENGEES;ICube Univ. Strasbourg;LIRMM Univ. Montpellier 2;LIRMM Univ. Montpellier 3;LIRMM Univ. Montpellier 3;IRSTEA Montpellier;AgroParisTech Montpellier;ICube Univ. Strasbourg/ENGEES","InternalReferences":"10.1109/VISUAL.1996.568146;10.1109/INFVIS.2000.885097","AuthorKeywords":"Visual Analytics, Spatiotemporal Data Mining and Visualization, Water Quality","AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":6,"XploreCitationCount - 2020-01":4,"PubsCited":60,"Award":null,"image":"7042488-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2014,"Title":"Vismate: Interactive Visual Analysis of Station-Based Observation Data on Climate Changes","DOI":"10.1109/VAST.2014.7042489","Link":"http://dx.doi.org/10.1109/VAST.2014.7042489","FirstPage":133,"LastPage":142,"PaperType":"C","Abstract":"We present a new approach to visualizing the climate data of multi-dimensional, time-series, and geo-related characteristics. Our approach integrates three new highly interrelated visualization techniques, and uses the same input data types as in the traditional model-based analysis methods. As the main visualization view, Global Radial Map is used to identify the overall state of climate changes and provide users with a compact and intuitive view for analyzing spatial and temporal patterns at the same time. Other two visualization techniques, providing complementary views, are specialized in analysing time trend and detecting abnormal cases, which are two important analysis tasks in any climate change study. Case studies and expert reviews have been conducted, through which the effectiveness and scalability of the proposed approach has been confirmed.","AuthorNames-Deduped":"Jie Li 0006;Kang Zhang;Zhaopeng Meng","AuthorNames":"Jie Li;Kang Zhang;Zhao-Peng Meng","AuthorAffiliation":"School of Computer Science and Technology, Tianjin University, and National Ocean Technology Center, Tianjin, China;Department of Computer Science, The University of Texas at Dallas, USA;School of Computer Software, Tianjin University, China","InternalReferences":"10.1109/VAST.2012.6400491;10.1109/TVCG.2010.194;10.1109/INFVIS.2000.885098;10.1109/TVCG.2007.70523;10.1109/TVCG.2009.199;10.1109/TVCG.2010.183;10.1109/VAST.2012.6400553;10.1109/TVCG.2010.180;10.1109/TVCG.2009.197","AuthorKeywords":"climate changes, spatiotemporal visualization, station-based observation data, radial layout, visual analytics","AminerCitationCount_02-2020":10,"AminerCitationCount_06-2020":14,"XploreCitationCount - 2020-01":9,"PubsCited":54,"Award":null,"image":"7042489-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"BoundarySeer: Visual Analysis of 2D Boundary Changes","DOI":"10.1109/VAST.2014.7042490","Link":"http://dx.doi.org/10.1109/VAST.2014.7042490","FirstPage":143,"LastPage":152,"PaperType":"C","Abstract":"Boundary changes exist ubiquitously in our daily life. From the Antarctic ozone hole to the land desertification, and from the territory of a country to the area within one-hour reach from a downtown location, boundaries change over time. With a large number of time-varying boundaries recorded, people often need to analyze the changes, detect their similarities or differences, and find out spatial and temporal patterns of the evolution for various applications. In this paper, we present a comprehensive visual analytics system, BoundarySeer, to help users gain insight into the changes of boundaries. Our system consists of four major viewers: 1) a global viewer to show boundary groups based on their similarity and the distribution of boundary attributes such as smoothness and perimeter; 2) a region viewer to display the regions encircled by the boundaries and how they are affected by boundary changes; 3) a trend viewer to reveal the temporal patterns in the boundary evolution and potential spatio-temporal correlations; 4) a directional change viewer to encode movements of boundary segments in different directions. Quantitative analyses of boundaries (e.g., similarity measurement and adaptive clustering) and intuitive visualizations (e.g., density map and ThemeRiver) are integrated into these viewers, which enable users to explore boundary changes from different aspects and at different scales. Case studies with two real-world datasets have been carried out to demonstrate the effectiveness of our system.","AuthorNames-Deduped":"Wenchao Wu;Yixian Zheng;Huamin Qu;Wei Chen 0001;M. Eduard Gröller;Lionel M. Ni","AuthorNames":"Wenchao Wu;Yixian Zheng;Huamin Qu;Wei Chen;Eduard Gröller;Lionel M. Ni","AuthorAffiliation":"Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;State Key Lab of CAD&CG, Zhejiang University;Institute of Computer Graphics and Algorithms, Vienna University of Technology and VRVis Research Center, Austria;Hong Kong University of Science and Technology","InternalReferences":"10.1109/TVCG.2013.230;10.1109/INFVIS.2004.27;10.1109/INFVIS.2001.963273;10.1109/TVCG.2011.239;10.1109/TVCG.2008.166;10.1109/INFVIS.2005.1532149;10.1109/TVCG.2013.213;10.1109/TVCG.2012.265;10.1109/TVCG.2007.70535;10.1109/TVCG.2008.125;10.1109/TVCG.2007.70561","AuthorKeywords":"Boundary change, visual analytics, scatter plot, ThemeRiver, contour map, radial visualization","AminerCitationCount_02-2020":10,"AminerCitationCount_06-2020":14,"XploreCitationCount - 2020-01":7,"PubsCited":40,"Award":null,"image":"7042490-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2014,"Title":"YMCA - Your Mesh Comparison Application","DOI":"10.1109/VAST.2014.7042491","Link":"http://dx.doi.org/10.1109/VAST.2014.7042491","FirstPage":153,"LastPage":162,"PaperType":"C","Abstract":"Polygonal meshes can be created in several different ways. In this paper we focus on the reconstruction of meshes from point clouds, which are sets of points in 3D. Several algorithms that tackle this task already exist, but they have different benefits and drawbacks, which leads to a large number of possible reconstruction results (i.e., meshes). The evaluation of those techniques requires extensive comparisons between different meshes which is up to now done by either placing images of rendered meshes side-by-side, or by encoding differences by heat maps. A major drawback of both approaches is that they do not scale well with the number of meshes. This paper introduces a new comparative visual analysis technique for 3D meshes which enables the simultaneous comparison of several meshes and allows for the interactive exploration of their differences. Our approach gives an overview of the differences of the input meshes in a 2D view. By selecting certain areas of interest, the user can switch to a 3D representation and explore the spatial differences in detail. To inspect local variations, we provide a magic lens tool in 3D. The location and size of the lens provide further information on the variations of the reconstructions in the selected area. With our comparative visualization approach, differences between several mesh reconstruction algorithms can be easily localized and inspected.","AuthorNames-Deduped":"Johanna Schmidt;Reinhold Preiner;Thomas Auzinger;Michael Wimmer;M. Eduard Gröller;Stefan Bruckner","AuthorNames":"Johanna Schmidt;Reinhold Preiner;Thomas Auzinger;Michael Wimmer;M. Eduard Gröller;Stefan Bruckner","AuthorAffiliation":"Vienna University of Technology, Austria;Vienna University of Technology, Austria;Vienna University of Technology, Austria;Vienna University of Technology, Austria;Vienna University of Technology, Vienna, Austria;University of Bergen, Norway","InternalReferences":"10.1109/INFVIS.2002.1173157;10.1109/VISUAL.1990.146402;10.1109/TVCG.2013.213;10.1109/VISUAL.2002.1183790","AuthorKeywords":"Visual analysis, comparative visualization, 3D data exploration, focus+context, mesh comparison","AminerCitationCount_02-2020":11,"AminerCitationCount_06-2020":14,"XploreCitationCount - 2020-01":10,"PubsCited":33,"Award":null,"image":"7042491-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Multi-Model Semantic Interaction for Text Analytics","DOI":"10.1109/VAST.2014.7042492","Link":"http://dx.doi.org/10.1109/VAST.2014.7042492","FirstPage":163,"LastPage":172,"PaperType":"C","Abstract":"Semantic interaction offers an intuitive communication mechanism between human users and complex statistical models. By shielding the users from manipulating model parameters, they focus instead on directly manipulating the spatialization, thus remaining in their cognitive zone. However, this technique is not inherently scalable past hundreds of text documents. To remedy this, we present the concept of multi-model semantic interaction, where semantic interactions can be used to steer multiple models at multiple levels of data scale, enabling users to tackle larger data problems. We also present an updated visualization pipeline model for generalized multi-model semantic interaction. To demonstrate multi-model semantic interaction, we introduce StarSPIRE, a visual text analytics prototype that transforms user interactions on documents into both small-scale display layout updates as well as large-scale relevancy-based document selection.","AuthorNames-Deduped":"Lauren Bradel;Chris North;Leanna House;Scotland Leman","AuthorNames":"Lauren Bradel;Chris North;Leanna House;Scotland Leman","AuthorAffiliation":"Virginia Tech.;Virginia Tech.;Virginia Tech.;Virginia Tech.","InternalReferences":"10.1109/VAST.2011.6102449;10.1109/TVCG.2013.188;10.1109/VAST.2012.6400559;10.1109/VAST.2012.6400486;10.1109/INFVIS.1995.528686;10.1109/VAST.2007.4389006","AuthorKeywords":"Visual analytics, Semantic Interaction, Sensemaking, Text Analytics","AminerCitationCount_02-2020":17,"AminerCitationCount_06-2020":24,"XploreCitationCount - 2020-01":18,"PubsCited":35,"Award":null,"image":"7042492-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Serendip: Topic Model-Driven Visual Exploration of Text Corpora","DOI":"10.1109/VAST.2014.7042493","Link":"http://dx.doi.org/10.1109/VAST.2014.7042493","FirstPage":173,"LastPage":182,"PaperType":"C","Abstract":"Exploration and discovery in a large text corpus requires investigation at multiple levels of abstraction, from a zoomed-out view of the entire corpus down to close-ups of individual passages and words. At each of these levels, there is a wealth of information that can inform inquiry - from statistical models, to metadata, to the researcher's own knowledge and expertise. Joining all this information together can be a challenge, and there are issues of scale to be combatted along the way. In this paper, we describe an approach to text analysis that addresses these challenges of scale and multiple information sources, using probabilistic topic models to structure exploration through multiple levels of inquiry in a way that fosters serendipitous discovery. In implementing this approach into a tool called Serendip, we incorporate topic model data and metadata into a highly reorderable matrix to expose corpus level trends; extend encodings of tagged text to illustrate probabilistic information at a passage level; and introduce a technique for visualizing individual word rankings, along with interaction techniques and new statistical methods to create links between different levels and information types. We describe example uses from both the humanities and visualization research that illustrate the benefits of our approach.","AuthorNames-Deduped":"Eric C. Alexander;Joe Kohlmann;Robin Valenza;Michael Witmore;Michael Gleicher","AuthorNames":"Eric Alexander;Joe Kohlmann;Robin Valenza;Michael Witmore;Michael Gleicher","AuthorAffiliation":"Department of Computer Sciences at the University of Wisconsin-Madison;Department of Computer Sciences at the University of Wisconsin-Madison;Department of English at the University of Wisconsin-Madison;Folger Shakespeare Library in Washington, D. C.;Department of Computer Sciences at the University of Wisconsin-Madison.","InternalReferences":"10.1109/VAST.2011.6102449;10.1109/INFVIS.2000.885098;10.1109/INFVIS.1998.729568;10.1109/TVCG.2011.239;10.1109/TVCG.2011.220;10.1109/VAST.2012.6400486;10.1109/VAST.2011.6102461;10.1109/TVCG.2013.157;10.1109/TVCG.2013.162;10.1109/VAST.2007.4389006;10.1109/VAST.2007.4389004","AuthorKeywords":"Text visualization, topic modeling","AminerCitationCount_02-2020":39,"AminerCitationCount_06-2020":51,"XploreCitationCount - 2020-01":36,"PubsCited":38,"Award":null,"image":"7042493-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2014,"Title":"TopicPanorama: A Full Picture of Relevant Topics","DOI":"10.1109/VAST.2014.7042494","Link":"http://dx.doi.org/10.1109/VAST.2014.7042494","FirstPage":183,"LastPage":192,"PaperType":"C","Abstract":"We present a visual analytics approach to developing a full picture of relevant topics discussed in multiple sources such as news, blogs, or micro-blogs. The full picture consists of a number of common topics among multiple sources as well as distinctive topics. The key idea behind our approach is to jointly match the topics extracted from each source together in order to interactively and effectively analyze common and distinctive topics. We start by modeling each textual corpus as a topic graph. These graphs are then matched together with a consistent graph matching method. Next, we develop an LOD-based visualization for better understanding and analysis of the matched graph. The major feature of this visualization is that it combines a radially stacked tree visualization with a density-based graph visualization to facilitate the examination of the matched topic graph from multiple perspectives. To compensate for the deficiency of the graph matching algorithm and meet different users' needs, we allow users to interactively modify the graph matching result. We have applied our approach to various data including news, tweets, and blog data. Qualitative evaluation and a real-world case study with domain experts demonstrate the promise of our approach, especially in support of analyzing a topic-graph-based full picture at different levels of detail.","AuthorNames-Deduped":"Shixia Liu;Xiting Wang;Jianfei Chen;Jim Zhu;Baining Guo","AuthorNames":"Shixia Liu;Xiting Wang;Jianfei Chen;Jim Zhu;Baining Guo","AuthorAffiliation":"Microsoft Research Asia;Tsinghua University and Microsoft Research Asia;Dept. of Comp. Sei. &amp; Tech., TNList Lab, State Key Lab of lntell. Tech. &amp; Sys., Tsingua University;Dept. of Comp. Sei. &amp; Tech., TNList Lab, State Key Lab of Intell. Tech. &amp; Sys., Tsinghua University;Microsoft Research Asia","InternalReferences":"10.1109/VAST.2011.6102461;10.1109/TVCG.2011.239;10.1109/TVCG.2009.111;10.1109/TVCG.2010.154;10.1109/VAST.2011.6102439;10.1109/TVCG.2006.193;10.1109/TVCG.2012.285;10.1109/TVCG.2013.221;10.1109/TVCG.2013.162;10.1109/TVCG.2012.279;10.1109/TVCG.2010.183;10.1109/TVCG.2014.2346433;10.1109/TVCG.2007.70521;10.1109/TVCG.2013.233;10.1109/TVCG.2013.212;10.1109/TVCG.2007.70582;10.1109/TVCG.2013.232;10.1109/TVCG.2010.129;10.1109/TVCG.2014.2346919","AuthorKeywords":"Topic graph, graph matching, graph visualization, user interactions, level-of-detail","AminerCitationCount_02-2020":10,"AminerCitationCount_06-2020":22,"XploreCitationCount - 2020-01":29,"PubsCited":61,"Award":null,"image":"7042494-fig-1-source-large.gif"},{"Conference":"VAST","Year":2014,"Title":"Integrating Predictive Analytics and Social Media","DOI":"10.1109/VAST.2014.7042495","Link":"http://dx.doi.org/10.1109/VAST.2014.7042495","FirstPage":193,"LastPage":202,"PaperType":"C","Abstract":"A key analytical task across many domains is model building and exploration for predictive analysis. Data is collected, parsed and analyzed for relationships, and features are selected and mapped to estimate the response of a system under exploration. As social media data has grown more abundant, data can be captured that may potentially represent behavioral patterns in society. In turn, this unstructured social media data can be parsed and integrated as a key factor for predictive intelligence. In this paper, we present a framework for the development of predictive models utilizing social media data. We combine feature selection mechanisms, similarity comparisons and model cross-validation through a variety of interactive visualizations to support analysts in model building and prediction. In order to explore how predictions might be performed in such a framework, we present results from a user study focusing on social media data as a predictor for movie box-office success.","AuthorNames-Deduped":"Yafeng Lu;Robert Krüger;Dennis Thom;Feng Wang 0012;Steffen Koch;Thomas Ertl;Ross Maciejewski","AuthorNames":"Yafeng Lu;Robert Krüger;Dennis Thom;Feng Wang;Steffen Koch;Thomas Ertl;Ross Maciejewski","AuthorAffiliation":"Arizona State University;University of Stuttgart, Germany;University of Stuttgart, Germany;Arizona State University;University of Stuttgart, Germany;University of Stuttgart, Germany;Arizona State University","InternalReferences":"10.1109/VAST.2012.6400557;10.1109/TVCG.2011.185;10.1109/VAST.2012.6400486;10.1109/TVCG.2013.162;10.1109/TVCG.2013.221;10.1109/VAST.2011.6102456;10.1109/TVCG.2012.291;10.1109/TVCG.2013.125;10.1109/INFVIS.2004.10;10.1109/VAST.2011.6102448;10.1109/VAST.2010.5652443;10.1109/INFVIS.2004.3","AuthorKeywords":"Social Media, Predictive Analytics, Feature Selection","AminerCitationCount_02-2020":17,"AminerCitationCount_06-2020":24,"XploreCitationCount - 2020-01":20,"PubsCited":49,"Award":null,"image":"7042495-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2014,"Title":"PEARL: An Interactive Visual Analytic Tool for Understanding Personal Emotion Style Derived from Social Media","DOI":"10.1109/VAST.2014.7042496","Link":"http://dx.doi.org/10.1109/VAST.2014.7042496","FirstPage":203,"LastPage":212,"PaperType":"C","Abstract":"Hundreds of millions of people leave digital footprints on social media (e.g., Twitter and Facebook). Such data not only disclose a person's demographics and opinions, but also reveal one's emotional style. Emotional style captures a person's patterns of emotions over time, including his overall emotional volatility and resilience. Understanding one's emotional style can provide great benefits for both individuals and businesses alike, including the support of self-reflection and delivery of individualized customer care. We present PEARL, a timeline-based visual analytic tool that allows users to interactively discover and examine a person's emotional style derived from this person's social media text. Compared to other visual text analytic systems, our work offers three unique contributions. First, it supports multi-dimensional emotion analysis from social media text to automatically detect a person's expressed emotions at different time points and summarize those emotions to reveal the person's emotional style. Second, it effectively visualizes complex, multi-dimensional emotion analysis results to create a visual emotional profile of an individual, which helps users browse and interpret one's emotional style. Third, it supports rich visual interactions that allow users to interactively explore and validate emotion analysis results. We have evaluated our work extensively through a series of studies. The results demonstrate the effectiveness of our tool both in emotion analysis from social media and in support of interactive visualization of the emotion analysis results.","AuthorNames-Deduped":"Jian Zhao 0010;Liang Gou;Fei Wang;Michelle X. Zhou","AuthorNames":"Jian Zhao;Liang Gou;Fei Wang;Michelle Zhou","AuthorAffiliation":"University of Toronto;IBM Research Almad&#x00E9;n;IBM Research Almad&#x00E9;n;IBM Research Almad&#x00E9;n","InternalReferences":"10.1109/TVCG.2011.239;10.1109/VAST.2012.6400485;10.1109/TVCG.2010.129;10.1109/TVCG.2011.185;10.1109/TVCG.2010.183","AuthorKeywords":"Personal emotion analytics, affective and mood modeling, social media text, Twitter, information visualization","AminerCitationCount_02-2020":16,"AminerCitationCount_06-2020":29,"XploreCitationCount - 2020-01":19,"PubsCited":34,"Award":null,"image":"7042496-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"TargetVue: Visual Analysis of Anomalous User Behaviors in Online Communication Systems","DOI":"10.1109/TVCG.2015.2467196","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467196","FirstPage":280,"LastPage":289,"PaperType":"J","Abstract":"Users with anomalous behaviors in online communication systems (e.g. email and social medial platforms) are potential threats to society. Automated anomaly detection based on advanced machine learning techniques has been developed to combat this issue; challenges remain, though, due to the difficulty of obtaining proper ground truth for model training and evaluation. Therefore, substantial human judgment on the automated analysis results is often required to better adjust the performance of anomaly detection. Unfortunately, techniques that allow users to understand the analysis results more efficiently, to make a confident judgment about anomalies, and to explore data in their context, are still lacking. In this paper, we propose a novel visual analysis system, TargetVue, which detects anomalous users via an unsupervised learning model and visualizes the behaviors of suspicious users in behavior-rich context through novel visualization designs and multiple coordinated contextual views. Particularly, TargetVue incorporates three new ego-centric glyphs to visually summarize a user's behaviors which effectively present the user's communication activities, features, and social interactions. An efficient layout method is proposed to place these glyphs on a triangle grid, which captures similarities among users and facilitates comparisons of behaviors of different users. We demonstrate the power of TargetVue through its application in a social bot detection challenge using Twitter data, a case study based on email records, and an interview with expert users. Our evaluation shows that TargetVue is beneficial to the detection of users with anomalous communication behaviors.","AuthorNames-Deduped":"Nan Cao;Conglei Shi;Wan-Yi Sabrina Lin;Jie Lu;Yu-Ru Lin;Ching-Yung Lin","AuthorNames":"Nan Cao;Conglei Shi;Sabrina Lin;Jie Lu;Yu-Ru Lin;Ching-Yung Lin","AuthorAffiliation":"IBM T. J. Watson Research Center;IBM T. J. Watson Research Center;IBM T. J. Watson Research Center;IBM T. J. Watson Research Center;University of Pissburg;IBM T. J. Watson Research Center","InternalReferences":"10.1109/TVCG.2012.291;10.1109/TVCG.2006.170;10.1109/VISUAL.2002.1183816;10.1109/TVCG.2014.2346922","AuthorKeywords":"Anomaly Detection, Social Media, Visual Analysis","AminerCitationCount_02-2020":28,"AminerCitationCount_06-2020":47,"XploreCitationCount - 2020-01":39,"PubsCited":45,"Award":null,"image":"22tvcg01-cao-2467196-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"TimeLineCurator: Interactive Authoring of Visual Timelines from Unstructured Text","DOI":"10.1109/TVCG.2015.2467531","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467531","FirstPage":300,"LastPage":309,"PaperType":"J","Abstract":"We present TimeLineCurator, a browser-based authoring tool that automatically extracts event data from temporal references in unstructured text documents using natural language processing and encodes them along a visual timeline. Our goal is to facilitate the timeline creation process for journalists and others who tell temporal stories online. Current solutions involve manually extracting and formatting event data from source documents, a process that tends to be tedious and error prone. With TimeLineCurator, a prospective timeline author can quickly identify the extent of time encompassed by a document, as well as the distribution of events occurring along this timeline. Authors can speculatively browse possible documents to quickly determine whether they are appropriate sources of timeline material. TimeLineCurator provides controls for curating and editing events on a timeline, the ability to combine timelines from multiple source documents, and export curated timelines for online deployment. We evaluate TimeLineCurator through a benchmark comparison of entity extraction error against a manual timeline curation process, a preliminary evaluation of the user experience of timeline authoring, a brief qualitative analysis of its visual output, and a discussion of prospective use cases suggested by members of the target author communities following its deployment.","AuthorNames-Deduped":"Johanna Fulda;Matthew Brehmer;Tamara Munzner","AuthorNames":"Johanna Fulda;Matthew Brehmel;Tamara Munzner","AuthorAffiliation":"University of Munich (LMU);University of British Columbia;University of British Columbia","InternalReferences":"10.1109/VAST.2014.7042493;10.1109/TVCG.2011.185;10.1109/TVCG.2014.2346431;10.1109/TVCG.2013.124;10.1109/VAST.2012.6400557;10.1109/VAST.2011.6102461;10.1109/VAST.2012.6400485;10.1109/TVCG.2013.162;10.1109/TVCG.2013.214;10.1109/TVCG.2012.224;10.1109/TVCG.2014.2346291;10.1109/TVCG.2012.213;10.1109/VAST.2007.4389006;10.1109/TVCG.2012.212;10.1109/VAST.2012.6400530;10.1109/TVCG.2007.70577","AuthorKeywords":"System, timelines, authoring environment, time-oriented data, journalism","AminerCitationCount_02-2020":14,"AminerCitationCount_06-2020":27,"XploreCitationCount - 2020-01":22,"PubsCited":76,"Award":null,"image":"22tvcg01-fulda-2467531-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2015,"Title":"Characterizing Provenance in Visualization and Data Analysis: An Organizational Framework of Provenance Types and Purposes","DOI":"10.1109/TVCG.2015.2467551","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467551","FirstPage":31,"LastPage":40,"PaperType":"J","Abstract":"While the primary goal of visual analytics research is to improve the quality of insights and findings, a substantial amount of research in provenance has focused on the history of changes and advances throughout the analysis process. The term, provenance, has been used in a variety of ways to describe different types of records and histories related to visualization. The existing body of provenance research has grown to a point where the consolidation of design knowledge requires cross-referencing a variety of projects and studies spanning multiple domain areas. We present an organizational framework of the different types of provenance information and purposes for why they are desired in the field of visual analytics. Our organization is intended to serve as a framework to help researchers specify types of provenance and coordinate design knowledge across projects. We also discuss the relationships between these factors and the methods used to capture provenance information. In addition, our organization can be used to guide the selection of evaluation methodology and the comparison of study outcomes in provenance research.","AuthorNames-Deduped":"Eric D. Ragan;Alex Endert;Jibonananda Sanyal;Jian Chen","AuthorNames":"Eric D. Ragan;Alex Endert;Jibonananda Sanyal;Jian Chen","AuthorAffiliation":"Texas A&M University;Georgia Tech;Oak Ridge National Laboratory;University of Maryland, Baltimore County","InternalReferences":"10.1109/INFVIS.2005.1532136;10.1109/VISUAL.2005.1532788;10.1109/TVCG.2013.155;10.1109/VISUAL.1993.398857;10.1109/VAST.2012.6400486;10.1109/TVCG.2014.2346575;10.1109/VAST.2010.5652932;10.1109/VAST.2008.4677365;10.1109/TVCG.2008.137;10.1109/TVCG.2013.126;10.1109/VAST.2009.5333020;10.1109/VAST.2010.5653598;10.1109/TVCG.2012.271;10.1109/TVCG.2014.2346573;10.1109/VAST.2008.4677366;10.1109/TVCG.2013.130;10.1109/TVCG.2010.181;10.1109/TVCG.2010.179;10.1109/VISUAL.1990.146375","AuthorKeywords":"Provenance, Analytic provenance, Visual analytics, Framework, Visualization, Conceptual model","AminerCitationCount_02-2020":34,"AminerCitationCount_06-2020":69,"XploreCitationCount - 2020-01":52,"PubsCited":97,"Award":null,"image":""},{"Conference":"VAST","Year":2015,"Title":"The Data Context Map: Fusing Data and Attributes into a Unified Display","DOI":"10.1109/TVCG.2015.2467552","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467552","FirstPage":121,"LastPage":130,"PaperType":"J","Abstract":"Numerous methods have been described that allow the visualization of the data matrix. But all suffer from a common problem - observing the data points in the context of the attributes is either impossible or inaccurate. We describe a method that allows these types of comprehensive layouts. We achieve it by combining two similarity matrices typically used in isolation - the matrix encoding the similarity of the attributes and the matrix encoding the similarity of the data points. This combined matrix yields two of the four submatrices needed for a full multi-dimensional scaling type layout. The remaining two submatrices are obtained by creating a fused similarity matrix - one that measures the similarity of the data points with respect to the attributes, and vice versa. The resulting layout places the data objects in direct context of the attributes and hence we call it the data context map. It allows users to simultaneously appreciate (1) the similarity of data objects, (2) the similarity of attributes in the specific scope of the collection of data objects, and (3) the relationships of data objects with attributes and vice versa. The contextual layout also allows data regions to be segmented and labeled based on the locations of the attributes. This enables, for example, the map's application in selection tasks where users seek to identify one or more data objects that best fit a certain configuration of factors, using the map to visually balance the tradeoffs.","AuthorNames-Deduped":"Shenghui Cheng;Klaus Mueller","AuthorNames":"Shenghui Cheng;Klaus Mueller","AuthorAffiliation":"Visual Analytics and Imaging Laboratory, Computer Science Department, Stony Brook University and SUNY, Korea;Visual Analytics and Imaging Laboratory, Computer Science Department, Stony Brook University and SUNY, Korea","InternalReferences":"10.1109/TVCG.2013.146;10.1109/VAST.2009.5332629;10.1109/VISUAL.1997.663916;10.1109/VISUAL.1990.146402;10.1109/TVCG.2011.220;10.1109/INFVIS.1997.636793;10.1109/TVCG.2010.207","AuthorKeywords":"High Dimensional Data, Low-Dimensional Embedding, Visual Analytics, Decision Make, Tradeoffs","AminerCitationCount_02-2020":14,"AminerCitationCount_06-2020":28,"XploreCitationCount - 2020-01":28,"PubsCited":37,"Award":null,"image":"22tvcg01-cheng-2467552-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Temporal MDS Plots for Analysis of Multivariate Data","DOI":"10.1109/TVCG.2015.2467553","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467553","FirstPage":141,"LastPage":150,"PaperType":"J","Abstract":"Multivariate time series data can be found in many application domains. Examples include data from computer networks, healthcare, social networks, or financial markets. Often, patterns in such data evolve over time among multiple dimensions and are hard to detect. Dimensionality reduction methods such as PCA and MDS allow analysis and visualization of multivariate data, but per se do not provide means to explore multivariate patterns over time. We propose Temporal Multidimensional Scaling (TMDS), a novel visualization technique that computes temporal one-dimensional MDS plots for multivariate data which evolve over time. Using a sliding window approach, MDS is computed for each data window separately, and the results are plotted sequentially along the time axis, taking care of plot alignment. Our TMDS plots enable visual identification of patterns based on multidimensional similarity of the data evolving over time. We demonstrate the usefulness of our approach in the field of network security and show in two case studies how users can iteratively explore the data to identify previously unknown, temporally evolving patterns.","AuthorNames-Deduped":"Dominik Jäckle;Fabian Fischer 0001;Tobias Schreck;Daniel A. Keim","AuthorNames":"Dominik Jäckle;Fabian Fischer;Tobias Schreck;Daniel A. Keim","AuthorAffiliation":"University of Konstanz, Germany;University of Konstanz, Germany;Graz University of Technology, Austria;University of Konstanz, Germany","InternalReferences":"10.1109/VAST.2009.5332593;10.1109/VISUAL.1990.146402;10.1109/VISUAL.1995.485140;10.1109/VISUAL.1990.146386;10.1109/TVCG.2007.70592;10.1109/VAST.2009.5332628","AuthorKeywords":"Multivariate Data, Time Series, Data Reduction, Multidimensional Scaling","AminerCitationCount_02-2020":22,"AminerCitationCount_06-2020":32,"XploreCitationCount - 2020-01":26,"PubsCited":41,"Award":null,"image":"22tvcg01-jackle-2467553-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"An Uncertainty-Aware Approach for Exploratory Microblog Retrieval","DOI":"10.1109/TVCG.2015.2467554","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467554","FirstPage":250,"LastPage":259,"PaperType":"J","Abstract":"Although there has been a great deal of interest in analyzing customer opinions and breaking news in microblogs, progress has been hampered by the lack of an effective mechanism to discover and retrieve data of interest from microblogs. To address this problem, we have developed an uncertainty-aware visual analytics approach to retrieve salient posts, users, and hashtags. We extend an existing ranking technique to compute a multifaceted retrieval result: the mutual reinforcement rank of a graph node, the uncertainty of each rank, and the propagation of uncertainty among different graph nodes. To illustrate the three facets, we have also designed a composite visualization with three visual components: a graph visualization, an uncertainty glyph, and a flow map. The graph visualization with glyphs, the flow map, and the uncertainty analysis together enable analysts to effectively find the most uncertain results and interactively refine them. We have applied our approach to several Twitter datasets. Qualitative evaluation and two real-world case studies demonstrate the promise of our approach for retrieving high-quality microblog data.","AuthorNames-Deduped":"Mengchen Liu;Shixia Liu;Xizhou Zhu;Qinying Liao;Furu Wei;Shimei Pan","AuthorNames":"Mengchen Liu;Shixia Liu;Xizhou Zhu;Qinying Liao;Furu Wei;Shimei Pan","AuthorAffiliation":"Tsinghua University;Tsinghua University;USTC;Microsoft;Microsoft;University of Maryland, Baltimore County","InternalReferences":"10.1109/TVCG.2013.186;10.1109/TVCG.2012.291;10.1109/VAST.2009.5332611;10.1109/TVCG.2013.223;10.1109/TVCG.2011.233;10.1109/VAST.2014.7042494;10.1109/VISUAL.1996.568116;10.1109/INFVIS.2005.1532150;10.1109/VAST.2010.5652931;10.1109/TVCG.2011.197;10.1109/TVCG.2014.2346919;10.1109/TVCG.2013.232;10.1109/TVCG.2011.202;10.1109/TVCG.2014.2346920;10.1109/TVCG.2010.183;10.1109/TVCG.2012.285;10.1109/TVCG.2013.221;10.1109/TVCG.2014.2346922","AuthorKeywords":"microblog data, mutual reinforcement model, uncertainty modeling, uncertainty visualization, uncertainty propagation","AminerCitationCount_02-2020":11,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":18,"PubsCited":55,"Award":null,"image":"22tvcg01-liu-2467554-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"VisOHC: Designing Visual Analytics for Online Health Communities","DOI":"10.1109/TVCG.2015.2467555","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467555","FirstPage":71,"LastPage":80,"PaperType":"J","Abstract":"Through online health communities (OHCs), patients and caregivers exchange their illness experiences and strategies for overcoming the illness, and provide emotional support. To facilitate healthy and lively conversations in these communities, their members should be continuously monitored and nurtured by OHC administrators. The main challenge of OHC administrators' tasks lies in understanding the diverse dimensions of conversation threads that lead to productive discussions in their communities. In this paper, we present a design study in which three domain expert groups participated, an OHC researcher and two OHC administrators of online health communities, which was conducted to find with a visual analytic solution. Through our design study, we characterized the domain goals of OHC administrators and derived tasks to achieve these goals. As a result of this study, we propose a system called VisOHC, which visualizes individual OHC conversation threads as collapsed boxes-a visual metaphor of conversation threads. In addition, we augmented the posters' reply authorship network with marks and/or beams to show conversation dynamics within threads. We also developed unique measures tailored to the characteristics of OHCs, which can be encoded for thread visualizations at the users' requests. Our observation of the two administrators while using VisOHC showed that it supports their tasks and reveals interesting insights into online health communities. Finally, we share our methodological lessons on probing visual designs together with domain experts by allowing them to freely encode measurements into visual variables.","AuthorNames-Deduped":"Bum Chul Kwon;Sung-Hee Kim;Sukwon Lee;Jaegul Choo;Jina Huh;Ji Soo Yi","AuthorNames":"Bum Chul Kwon;Sung-Hee Kim;Sukwon Lee;Jaegul Choo;Jina Huh;Ji Soo Yi","AuthorAffiliation":"University of Konstanz;University of British Columbia;Purdue University;Korea University;Michigan State University;Purdue University","InternalReferences":"10.1109/TVCG.2014.2346433;10.1109/VAST.2011.6102441;10.1109/TVCG.2014.2346292;10.1109/INFVIS.2003.1249028;10.1109/TVCG.2010.175;10.1109/VAST.2014.7042494;10.1109/TVCG.2014.2346331;10.1109/VAST.2009.5333919;10.1109/TVCG.2012.213;10.1109/TVCG.2009.171;10.1109/TVCG.2009.187;10.1109/TVCG.2013.221;10.1109/VAST.2012.6400554;10.1109/VAST.2014.7042496;10.1109/TVCG.2008.171","AuthorKeywords":"Online health communities, visual analytics, conversation analysis, thread visualization, healthcare, design study","AminerCitationCount_02-2020":19,"AminerCitationCount_06-2020":22,"XploreCitationCount - 2020-01":15,"PubsCited":44,"Award":null,"image":"22tvcg01-kwon-2467555-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2015,"Title":"The Role of Uncertainty, Awareness, and Trust in Visual Analytics","DOI":"10.1109/TVCG.2015.2467591","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467591","FirstPage":240,"LastPage":249,"PaperType":"J","Abstract":"Visual analytics supports humans in generating knowledge from large and often complex datasets. Evidence is collected, collated and cross-linked with our existing knowledge. In the process, a myriad of analytical and visualisation techniques are employed to generate a visual representation of the data. These often introduce their own uncertainties, in addition to the ones inherent in the data, and these propagated and compounded uncertainties can result in impaired decision making. The user's confidence or trust in the results depends on the extent of user's awareness of the underlying uncertainties generated on the system side. This paper unpacks the uncertainties that propagate through visual analytics systems, illustrates how human's perceptual and cognitive biases influence the user's awareness of such uncertainties, and how this affects the user's trust building. The knowledge generation model for visual analytics is used to provide a terminology and framework to discuss the consequences of these aspects in knowledge construction and though examples, machine uncertainty is compared to human trust measures with provenance. Furthermore, guidelines for the design of uncertainty-aware systems are presented that can aid the user in better decision making.","AuthorNames-Deduped":"Dominik Sacha;Hansi Senaratne;Bum Chul Kwon;Geoffrey P. Ellis;Daniel A. Keim","AuthorNames":"Dominik Sacha;Hansi Senaratne;Bum Chul Kwon;Geoffrey Ellis;Daniel A. Keim","AuthorAffiliation":"Data Analysis and Visualisation Group;Data Analysis and Visualisation Group;Data Analysis and Visualisation Group;Data Analysis and Visualisation Group;Data Analysis and Visualisation Group","InternalReferences":"10.1109/TVCG.2014.2346575;10.1109/VISUAL.2000.885679;10.1109/VAST.2008.4677385;10.1109/VAST.2009.5332611;10.1109/TVCG.2012.260;10.1109/VAST.2011.6102473;10.1109/VAST.2009.5333020;10.1109/VAST.2011.6102435;10.1109/TVCG.2012.279;10.1109/TVCG.2014.2346481;10.1109/VAST.2006.261416","AuthorKeywords":"Visual Analytics, Knowledge Generation, Uncertainty Measures and Propagation, Trust Building, Human Factors","AminerCitationCount_02-2020":36,"AminerCitationCount_06-2020":68,"XploreCitationCount - 2020-01":50,"PubsCited":83,"Award":null,"image":"22tvcg01-sacha-2467591-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Visually Exploring Transportation Schedules","DOI":"10.1109/TVCG.2015.2467592","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467592","FirstPage":170,"LastPage":179,"PaperType":"J","Abstract":"Public transportation schedules are designed by agencies to optimize service quality under multiple constraints. However, real service usually deviates from the plan. Therefore, transportation analysts need to identify, compare and explain both eventual and systemic performance issues that must be addressed so that better timetables can be created. The purely statistical tools commonly used by analysts pose many difficulties due to the large number of attributes at tripand station-level for planned and real service. Also challenging is the need for models at multiple scales to search for patterns at different times and stations, since analysts do not know exactly where or when relevant patterns might emerge and need to compute statistical summaries for multiple attributes at different granularities. To aid in this analysis, we worked in close collaboration with a transportation expert to design TR-EX, a visual exploration tool developed to identify, inspect and compare spatio-temporal patterns for planned and real transportation service. TR-EX combines two new visual encodings inspired by Marey's Train Schedule: Trips Explorer for trip-level analysis of frequency, deviation and speed; and Stops Explorer for station-level study of delay, wait time, reliability and performance deficiencies such as bunching. To tackle overplotting and to provide a robust representation for a large numbers of trips and stops at multiple scales, the system supports variable kernel bandwidths to achieve the level of detail required by users for different tasks. We justify our design decisions based on specific analysis needs of transportation analysts. We provide anecdotal evidence of the efficacy of TR-EX through a series of case studies that explore NYC subway service, which illustrate how TR-EX can be used to confirm hypotheses and derive new insights through visual exploration.","AuthorNames-Deduped":"Cesar Palomo;Zhan Guo;Cláudio T. Silva;Juliana Freire","AuthorNames":"Cesar Palomo;Zhan Guo;Cláudio T. Silva;Juliana Freire","AuthorAffiliation":"New York University;New York University;New York University;New York University","InternalReferences":"10.1109/INFVIS.2004.68;10.1109/TVCG.2014.2346449;10.1109/TVCG.2007.70535;10.1109/TVCG.2011.176;10.1109/TVCG.2013.226;10.1109/VISUAL.1999.809866;10.1109/TVCG.2008.137;10.1109/TVCG.2009.131;10.1109/INFVIS.2005.1532138;10.1109/TVCG.2011.179;10.1109/TVCG.2006.170;10.1109/INFVIS.2003.1249005","AuthorKeywords":"Transportation, schedules, kernel density estimation, visual exploration","AminerCitationCount_02-2020":10,"AminerCitationCount_06-2020":13,"XploreCitationCount - 2020-01":14,"PubsCited":44,"Award":null,"image":"22tvcg01-palomo-2467592-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"SensePath: Understanding the Sensemaking Process Through Analytic Provenance","DOI":"10.1109/TVCG.2015.2467611","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467611","FirstPage":41,"LastPage":50,"PaperType":"J","Abstract":"Sensemaking is described as the process of comprehension, finding meaning and gaining insight from information, producing new knowledge and informing further action. Understanding the sensemaking process allows building effective visual analytics tools to make sense of large and complex datasets. Currently, it is often a manual and time-consuming undertaking to comprehend this: researchers collect observation data, transcribe screen capture videos and think-aloud recordings, identify recurring patterns, and eventually abstract the sensemaking process into a general model. In this paper, we propose a general approach to facilitate such a qualitative analysis process, and introduce a prototype, SensePath, to demonstrate the application of this approach with a focus on browser-based online sensemaking. The approach is based on a study of a number of qualitative research sessions including observations of users performing sensemaking tasks and post hoc analyses to uncover their sensemaking processes. Based on the study results and a follow-up participatory design session with HCI researchers, we decided to focus on the transcription and coding stages of thematic analysis. SensePath automatically captures user's sensemaking actions, i.e., analytic provenance, and provides multi-linked views to support their further analysis. A number of other requirements elicited from the design session are also implemented in SensePath, such as easy integration with existing qualitative analysis workflow and non-intrusive for participants. The tool was used by an experienced HCI researcher to analyze two sensemaking sessions. The researcher found the tool intuitive and considerably reduced analysis time, allowing better understanding of the sensemaking process.","AuthorNames-Deduped":"Phong H. Nguyen;Kai Xu 0003;Ashley Wheat;B. L. William Wong;Simon Attfield;Bob Fields","AuthorNames":"Phong H. Nguyen;Kai Xu;Ashley Wheat;B.L. William Wong;Simon Attfield;Bob Fields","AuthorAffiliation":"Middlesex University;Middlesex University;Middlesex University;Middlesex University;Middlesex University;Middlesex University","InternalReferences":"10.1109/VISUAL.2005.1532788;10.1109/TVCG.2011.185;10.1109/TVCG.2014.2346575;10.1109/VAST.2008.4677365;10.1109/TVCG.2008.137;10.1109/VAST.2009.5333020;10.1109/TVCG.2013.132","AuthorKeywords":"Sensemaking, analytic provenance, transcription, coding, qualitative research, timeline visualization","AminerCitationCount_02-2020":11,"AminerCitationCount_06-2020":18,"XploreCitationCount - 2020-01":13,"PubsCited":42,"Award":null,"image":"22tvcg01-nguyen-2467611-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2015,"Title":"Visual Analytics for Development and Evaluation of Order Selection Criteria for Autoregressive Processes","DOI":"10.1109/TVCG.2015.2467612","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467612","FirstPage":151,"LastPage":159,"PaperType":"J","Abstract":"Order selection of autoregressive processes is an active research topic in time series analysis, and the development and evaluation of automatic order selection criteria remains a challenging task for domain experts. We propose a visual analytics approach, to guide the analysis and development of such criteria. A flexible synthetic model generator-combined with specialized responsive visualizations-allows comprehensive interactive evaluation. Our fast framework allows feedback-driven development and fine-tuning of new order selection criteria in real-time. We demonstrate the applicability of our approach in three use-cases for two general as well as a real-world example.","AuthorNames-Deduped":"Thomas Löwe;Emmy-Charlotte Förster;Georgia Albuquerque;Jens-Peter Kreiss;Marcus A. Magnor","AuthorNames":"Thomas Löwe;Emmy-Charlotte Förster;Georgia Albuquerque;Jens-Peter Kreiss;Marcus Magnor","AuthorAffiliation":"Computer Graphics Lab, Germany;Computer Graphics Lab, Germany;Computer Graphics Lab, Germany;Institut für Mathematische Stochastik, Germany;Computer Graphics Lab, Germany","InternalReferences":"10.1109/TVCG.2013.222","AuthorKeywords":"Visual analytics, time series analysis, order selection","AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":5,"XploreCitationCount - 2020-01":5,"PubsCited":44,"Award":null,"image":"22tvcg01-lowe-2467612-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2015,"Title":"A Case Study Using Visualization Interaction Logs and Insight Metrics to Understand How Analysts Arrive at Insights","DOI":"10.1109/TVCG.2015.2467613","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467613","FirstPage":51,"LastPage":60,"PaperType":"J","Abstract":"We present results from an experiment aimed at using logs of interactions with a visual analytics application to better understand how interactions lead to insight generation. We performed an insight-based user study of a visual analytics application and ran post hoc quantitative analyses of participants' measured insight metrics and interaction logs. The quantitative analyses identified features of interaction that were correlated with insight characteristics, and we confirmed these findings using a qualitative analysis of video captured during the user study. Results of the experiment include design guidelines for the visual analytics application aimed at supporting insight generation. Furthermore, we demonstrated an analysis method using interaction logs that identified which interaction patterns led to insights, going beyond insight-based evaluations that only quantify insight characteristics. We also discuss choices and pitfalls encountered when applying this analysis method, such as the benefits and costs of applying an abstraction framework to application-specific actions before further analysis. Our method can be applied to evaluations of other visualization tools to inform the design of insight-promoting interactions and to better understand analyst behaviors.","AuthorNames-Deduped":"Hua Guo;Steven R. Gomez;Caroline Ziemkiewicz;David H. Laidlaw","AuthorNames":"Hua Guo;Steven R. Gomez;Caroline Ziemkiewicz;David H. Laidlaw","AuthorAffiliation":"Brown University;Brown University;Aptima Inc.;Brown University","InternalReferences":"10.1109/INFVIS.2005.1532136;10.1109/TVCG.2014.2346575;10.1109/VAST.2014.7042482;10.1109/VAST.2008.4677365;10.1109/TVCG.2008.137;10.1109/VAST.2009.5333878;10.1109/TVCG.2014.2346452;10.1109/TVCG.2012.221;10.1109/TVCG.2007.70515","AuthorKeywords":"Evaluation, visual analytics, interaction, intelligence analysis, insight-based evaluation","AminerCitationCount_02-2020":19,"AminerCitationCount_06-2020":31,"XploreCitationCount - 2020-01":21,"PubsCited":34,"Award":"HM","image":"22tvcg01-guo-2467613-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"InterAxis: Steering Scatterplot Axes via Observation-Level Interaction","DOI":"10.1109/TVCG.2015.2467615","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467615","FirstPage":131,"LastPage":140,"PaperType":"J","Abstract":"Scatterplots are effective visualization techniques for multidimensional data that use two (or three) axes to visualize data items as a point at its corresponding x and y Cartesian coordinates. Typically, each axis is bound to a single data attribute. Interactive exploration occurs by changing the data attributes bound to each of these axes. In the case of using scatterplots to visualize the outputs of dimension reduction techniques, the x and y axes are combinations of the true, high-dimensional data. For these spatializations, the axes present usability challenges in terms of interpretability and interactivity. That is, understanding the axes and interacting with them to make adjustments can be challenging. In this paper, we present InterAxis, a visual analytics technique to properly interpret, define, and change an axis in a user-driven manner. Users are given the ability to define and modify axes by dragging data items to either side of the x or y axes, from which the system computes a linear combination of data attributes and binds it to the axis. Further, users can directly tune the positive and negative contribution to these complex axes by using the visualization of data attributes that correspond to each axis. We describe the details of our technique and demonstrate the intended usage through two scenarios.","AuthorNames-Deduped":"Hannah Kim;Jaegul Choo;Haesun Park;Alex Endert","AuthorNames":"Hannah Kim;Jaegul Choo;Haesun Park;Alex Endert","AuthorAffiliation":"Georgia Institute of Technology;Korea University;Georgia Institute of Technology;Georgia Institute of Technology","InternalReferences":"10.1109/TVCG.2011.185;10.1109/VAST.2012.6400486;10.1109/TVCG.2013.212;10.1109/VAST.2010.5652443;10.1109/TVCG.2011.201;10.1109/TVCG.2008.153;10.1109/VAST.2011.6102449;10.1109/TVCG.2013.157;10.1109/TVCG.2014.2346250;10.1109/VISUAL.1990.146386;10.1109/TVCG.2011.178;10.1109/TVCG.2013.167","AuthorKeywords":"Scatterplots, user interaction, model steering","AminerCitationCount_02-2020":27,"AminerCitationCount_06-2020":44,"XploreCitationCount - 2020-01":35,"PubsCited":50,"Award":null,"image":"22tvcg01-kim-2467615-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2015,"Title":"Task-Driven Comparison of Topic Models","DOI":"10.1109/TVCG.2015.2467618","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467618","FirstPage":320,"LastPage":329,"PaperType":"J","Abstract":"Topic modeling, a method of statistically extracting thematic content from a large collection of texts, is used for a wide variety of tasks within text analysis. Though there are a growing number of tools and techniques for exploring single models, comparisons between models are generally reduced to a small set of numerical metrics. These metrics may or may not reflect a model's performance on the analyst's intended task, and can therefore be insufficient to diagnose what causes differences between models. In this paper, we explore task-centric topic model comparison, considering how we can both provide detail for a more nuanced understanding of differences and address the wealth of tasks for which topic models are used. We derive comparison tasks from single-model uses of topic models, which predominantly fall into the categories of understanding topics, understanding similarity, and understanding change. Finally, we provide several visualization techniques that facilitate these tasks, including buddy plots, which combine color and position encodings to allow analysts to readily view changes in document similarity.","AuthorNames-Deduped":"Eric C. Alexander;Michael Gleicher","AuthorNames":"Eric Alexander;Michael Gleicher","AuthorAffiliation":"University of Wisconsin-Madison;University of Wisconsin-Madison","InternalReferences":"10.1109/TVCG.2011.232;10.1109/VAST.2014.7042493;10.1109/TVCG.2013.212;10.1109/TVCG.2011.239;10.1109/TVCG.2012.260;10.1109/INFVIS.2000.885098;10.1109/TVCG.2014.2346578;10.1109/TVCG.2013.221","AuthorKeywords":"Text visualization, topic modeling","AminerCitationCount_02-2020":12,"AminerCitationCount_06-2020":25,"XploreCitationCount - 2020-01":19,"PubsCited":36,"Award":null,"image":"22tvcg01-alexander4-2467618-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Interactive Visual Discovering of Movement Patterns from Sparsely Sampled Geo-tagged Social Media Data","DOI":"10.1109/TVCG.2015.2467619","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467619","FirstPage":270,"LastPage":279,"PaperType":"J","Abstract":"Social media data with geotags can be used to track people's movements in their daily lives. By providing both rich text and movement information, visual analysis on social media data can be both interesting and challenging. In contrast to traditional movement data, the sparseness and irregularity of social media data increase the difficulty of extracting movement patterns. To facilitate the understanding of people's movements, we present an interactive visual analytics system to support the exploration of sparsely sampled trajectory data from social media. We propose a heuristic model to reduce the uncertainty caused by the nature of social media data. In the proposed system, users can filter and select reliable data from each derived movement category, based on the guidance of uncertainty model and interactive selection tools. By iteratively analyzing filtered movements, users can explore the semantics of movements, including the transportation methods, frequent visiting sequences and keyword descriptions. We provide two cases to demonstrate how our system can help users to explore the movement patterns.","AuthorNames-Deduped":"Siming Chen;Xiaoru Yuan;Zhenhuang Wang;Cong Guo;Jie Liang 0004;Zuchao Wang;Xiaolong Zhang 0001;Jiawan Zhang","AuthorNames":"Siming Chen;Xiaoru Yuan;Zhenhuang Wang;Cong Guo;Jie Liang;Zuchao Wang;Xiaolong Luke Zhang;Jiawan Zhang","AuthorAffiliation":"Key Laboratory of Machine Perception (Ministry of Education), School of EECS, Peking University;Key Laboratory of Machine Perception (Ministry of Education), School of EECS, Peking University;Key Laboratory of Machine Perception (Ministry of Education), School of EECS, Peking University;Key Laboratory of Machine Perception (Ministry of Education), School of EECS, Peking University;Key Laboratory of Machine Perception (Ministry of Education), School of EECS, Peking University;Key Laboratory of Machine Perception (Ministry of Education), School of EECS, Peking University;College of Information Sciences and Technology, Pennsylvania State University;School of Computer Science and Technology, and School of Computer Software, Tianjin University","InternalReferences":"10.1109/VAST.2009.5332584;10.1109/VAST.2008.4677356;10.1109/TVCG.2009.182;10.1109/TVCG.2011.185;10.1109/TVCG.2012.291;10.1109/TVCG.2009.143;10.1109/INFVIS.2004.27;10.1109/INFVIS.2005.1532150;10.1109/TVCG.2012.265;10.1109/TVCG.2014.2346746;10.1109/TVCG.2014.2346922","AuthorKeywords":"Spatial temporal visual analytics, Geo-tagged social media, Sparsely sampling, Uncertainty, Movement","AminerCitationCount_02-2020":33,"AminerCitationCount_06-2020":53,"XploreCitationCount - 2020-01":52,"PubsCited":47,"Award":null,"image":"22tvcg01-chen-2467619-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2015,"Title":"Interactive Visual Profiling of Musicians","DOI":"10.1109/TVCG.2015.2467620","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467620","FirstPage":200,"LastPage":209,"PaperType":"J","Abstract":"Determining similar objects based upon the features of an object of interest is a common task for visual analytics systems. This process is called profiling, if the object of interest is a person with individual attributes. The profiling of musicians similar to a musician of interest with the aid of visual means became an interesting research question for musicologists working with the Bavarian Musicians Encyclopedia Online. This paper illustrates the development of a visual analytics profiling system that is used to address such research questions. Taking musicological knowledge into account, we outline various steps of our collaborative digital humanities project, priority (1) the definition of various measures to determine the similarity of musicians' attributes, and (2) the design of an interactive profiling system that supports musicologists in iteratively determining similar musicians. The utility of the profiling system is emphasized by various usage scenarios illustrating current research questions in musicology.","AuthorNames-Deduped":"Stefan Jänicke;Josef Focht;Gerik Scheuermann","AuthorNames":"Stefan Jänicke;Josef Focht;Gerik Scheuermann","AuthorAffiliation":"Image and Signal Processing Group, Germany;Museum of Musical Instruments, Germany;Image and Signal Processing Group, Germany","InternalReferences":"10.1109/VAST.2011.6102454;10.1109/TVCG.2010.159;10.1109/TVCG.2014.2346431;10.1109/TVCG.2007.70617;10.1109/VAST.2009.5333443;10.1109/TVCG.2014.2346433;10.1109/TVCG.2008.175;10.1109/TVCG.2012.252;10.1109/VAST.2012.6400485;10.1109/INFVIS.2005.1532126;10.1109/TVCG.2012.277;10.1109/VAST.2012.6400491;10.1109/VAST.2007.4389004;10.1109/TVCG.2014.2346677;10.1109/TVCG.2009.111;10.1109/TVCG.2006.122;10.1109/VAST.2010.5652931;10.1109/VAST.2009.5333023;10.1109/VAST.2007.4389006;10.1109/VAST.2009.5333248;10.1109/VAST.2008.4677370;10.1109/VAST.2010.5652520","AuthorKeywords":"visual analytics, profiling system, musicians database visualization, digital humanities, musicology","AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":10,"XploreCitationCount - 2020-01":10,"PubsCited":54,"Award":null,"image":"22tvcg01-janicke-2467620-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"CiteRivers: Visual Analytics of Citation Patterns","DOI":"10.1109/TVCG.2015.2467621","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467621","FirstPage":190,"LastPage":199,"PaperType":"J","Abstract":"The exploration and analysis of scientific literature collections is an important task for effective knowledge management. Past interest in such document sets has spurred the development of numerous visualization approaches for their interactive analysis. They either focus on the textual content of publications, or on document metadata including authors and citations. Previously presented approaches for citation analysis aim primarily at the visualization of the structure of citation networks and their exploration. We extend the state-of-the-art by presenting an approach for the interactive visual analysis of the contents of scientific documents, and combine it with a new and flexible technique to analyze their citations. This technique facilitates user-steered aggregation of citations which are linked to the content of the citing publications using a highly interactive visualization approach. Through enriching the approach with additional interactive views of other important aspects of the data, we support the exploration of the dataset over time and enable users to analyze citation patterns, spot trends, and track long-term developments. We demonstrate the strengths of our approach through a use case and discuss it based on expert user feedback.","AuthorNames-Deduped":"Florian Heimerl;Qi Han;Steffen Koch;Thomas Ertl","AuthorNames":"Florian Heimerl;Qi Han;Steffen Koch;Thomas Ertl","AuthorAffiliation":"Institute for Visualization and Interactive Systems (VIS);Institute for Visualization and Interactive Systems (VIS);Institute for Visualization and Interactive Systems (VIS);Institute for Visualization and Interactive Systems (VIS)","InternalReferences":"10.1109/INFVIS.2004.77;10.1109/TVCG.2015.2467757;10.1109/TVCG.2008.166;10.1109/TVCG.2013.212;10.1109/VAST.2009.5333443;10.1109/TVCG.2011.239;10.1109/TVCG.2012.252;10.1109/TVCG.2013.162;10.1109/TVCG.2012.277;10.1109/INFVIS.2004.45;10.1109/INFVIS.2005.1532150;10.1109/TVCG.2009.162;10.1109/TVCG.2009.171;10.1109/INFVIS.2005.1532122;10.1109/INFVIS.1995.528686;10.1109/TVCG.2014.2346920;10.1109/TVCG.2009.202","AuthorKeywords":"scientific literature, visual document analysis, visual citation analysis, streamgraph, clustering","AminerCitationCount_02-2020":21,"AminerCitationCount_06-2020":38,"XploreCitationCount - 2020-01":34,"PubsCited":53,"Award":null,"image":"22tvcg01-heimerl-2467621-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Supporting Iterative Cohort Construction with Visual Temporal Queries","DOI":"10.1109/TVCG.2015.2467622","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467622","FirstPage":91,"LastPage":100,"PaperType":"J","Abstract":"Many researchers across diverse disciplines aim to analyze the behavior of cohorts whose behaviors are recorded in large event databases. However, extracting cohorts from databases is a difficult yet important step, often overlooked in many analytical solutions. This is especially true when researchers wish to restrict their cohorts to exhibit a particular temporal pattern of interest. In order to fill this gap, we designed COQUITO, a visual interface that assists users defining cohorts with temporal constraints. COQUITO was designed to be comprehensible to domain experts with no preknowledge of database queries and also to encourage exploration. We then demonstrate the utility of COQUITO via two case studies, involving medical and social media researchers.","AuthorNames-Deduped":"Josua Krause;Adam Perer;Harry Stavropoulos","AuthorNames":"Josua Krause;Adam Perer;Harry Stavropoulos","AuthorAffiliation":"NYU;IBM T.J. Watson Research Center;IBM T.J. Watson Research Center","InternalReferences":"10.1109/TVCG.2011.185;10.1109/VAST.2007.4389013;10.1109/VAST.2006.261421;10.1109/TVCG.2014.2346682;10.1109/VAST.2010.5652890;10.1109/TVCG.2014.2346482;10.1109/TVCG.2013.200;10.1109/TVCG.2013.206;10.1109/TVCG.2009.117;10.1109/INFVIS.2001.963273;10.1109/TVCG.2012.225;10.1109/TVCG.2013.167","AuthorKeywords":"Visual temporal queries, cohort definition, electronic medical records, information visualization","AminerCitationCount_02-2020":24,"AminerCitationCount_06-2020":43,"XploreCitationCount - 2020-01":31,"PubsCited":44,"Award":null,"image":"22tvcg01-krause-2467622-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2015,"Title":"PhenoBlocks: Phenotype Comparison Visualizations","DOI":"10.1109/TVCG.2015.2467733","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467733","FirstPage":101,"LastPage":110,"PaperType":"J","Abstract":"The differential diagnosis of hereditary disorders is a challenging task for clinicians due to the heterogeneity of phenotypes that can be observed in patients. Existing clinical tools are often text-based and do not emphasize consistency, completeness, or granularity of phenotype reporting. This can impede clinical diagnosis and limit their utility to genetics researchers. Herein, we present PhenoBlocks, a novel visual analytics tool that supports the comparison of phenotypes between patients, or between a patient and the hallmark features of a disorder. An informal evaluation of PhenoBlocks with expert clinicians suggested that the visualization effectively guides the process of differential diagnosis and could reinforce the importance of complete, granular phenotypic reporting.","AuthorNames-Deduped":"Michael Glueck;Peter Hamilton;Fanny Chevalier;Simon Breslav;Azam Khan;Daniel J. Wigdor;Michael Brudno","AuthorNames":"Michael Glueck;Peter Hamilton;Fanny Chevalier;Simon Breslav;Azam Khan;Daniel Wigdor;Michael Brudno","AuthorAffiliation":"Autodesk Research;University of Toronto;INRIA;Autodesk Research;Autodesk Research;University of Toronto;University of Toronto","InternalReferences":"10.1109/VAST.2011.6102439;10.1109/TVCG.2013.214;10.1109/TVCG.2013.231;10.1109/VAST.2011.6102438;10.1109/TVCG.2008.121;10.1109/TVCG.2009.167;10.1109/TVCG.2009.116;10.1109/INFVIS.2000.885091;10.1109/TVCG.2007.70529;10.1109/INFVIS.2003.1249030;10.1109/TVCG.2012.226","AuthorKeywords":"Clinical diagnosis, differential hierarchy comparison, ontology, genomics, phenomics, phenotype","AminerCitationCount_02-2020":9,"AminerCitationCount_06-2020":13,"XploreCitationCount - 2020-01":9,"PubsCited":60,"Award":null,"image":"22tvcg01-glueck-2467733-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Visual Analysis and Dissemination of Scientific Literature Collections with SurVis","DOI":"10.1109/TVCG.2015.2467757","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467757","FirstPage":180,"LastPage":189,"PaperType":"J","Abstract":"Bibliographic data such as collections of scientific articles and citation networks have been studied extensively in information visualization and visual analytics research. Powerful systems have been built to support various types of bibliographic analysis, but they require some training and cannot be used to disseminate the insights gained. In contrast, we focused on developing a more accessible visual analytics system, called SurVis, that is ready to disseminate a carefully surveyed literature collection. The authors of a survey may use our Web-based system to structure and analyze their literature database. Later, readers of the survey can obtain an overview, quickly retrieve specific publications, and reproduce or extend the original bibliographic analysis. Our system employs a set of selectors that enable users to filter and browse the literature collection as well as to control interactive visualizations. The versatile selector concept includes selectors for textual search, filtering by keywords and meta-information, selection and clustering of similar publications, and following citation links. Agreement to the selector is represented by word-sized sparkline visualizations seamlessly integrated into the user interface. Based on an analysis of the analytical reasoning process, we derived requirements for the system. We developed the system in a formative way involving other researchers writing literature surveys. A questionnaire study with 14 visual analytics experts confirms that SurVis meets the initially formulated requirements.","AuthorNames-Deduped":"Fabian Beck 0001;Sebastian Koch;Daniel Weiskopf","AuthorNames":"Fabian Beck;Sebastian Koch;Daniel Weiskopf","AuthorAffiliation":"VISUS, University of Stuttgart, Germany;VISUS, University of Stuttgart, Germany;VISUS, University of Stuttgart, Germany","InternalReferences":"10.1109/TVCG.2011.169;10.1109/TVCG.2012.252;10.1109/TVCG.2015.2467621;10.1109/VAST.2009.5333564;10.1109/TVCG.2010.194;10.1109/VAST.2007.4389006;10.1109/TVCG.2013.167","AuthorKeywords":"Visual analytics of documents, bibliographic data, dissemination, literature browser","AminerCitationCount_02-2020":19,"AminerCitationCount_06-2020":34,"XploreCitationCount - 2020-01":33,"PubsCited":37,"Award":null,"image":"22tvcg01-beck-2467757-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2015,"Title":"TrajGraph: A Graph-Based Visual Analytics Approach to Studying Urban Network Centralities Using Taxi Trajectory Data","DOI":"10.1109/TVCG.2015.2467771","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467771","FirstPage":160,"LastPage":169,"PaperType":"J","Abstract":"We propose TrajGraph, a new visual analytics method, for studying urban mobility patterns by integrating graph modeling and visual analysis with taxi trajectory data. A special graph is created to store and manifest real traffic information recorded by taxi trajectories over city streets. It conveys urban transportation dynamics which can be discovered by applying graph analysis algorithms. To support interactive, multiscale visual analytics, a graph partitioning algorithm is applied to create region-level graphs which have smaller size than the original street-level graph. Graph centralities, including Pagerank and betweenness, are computed to characterize the time-varying importance of different urban regions. The centralities are visualized by three coordinated views including a node-link graph view, a map view and a temporal information view. Users can interactively examine the importance of streets to discover and assess city traffic patterns. We have implemented a fully working prototype of this approach and evaluated it using massive taxi trajectories of Shenzhen, China. TrajGraph's capability in revealing the importance of city streets was evaluated by comparing the calculated centralities with the subjective evaluations from a group of drivers in Shenzhen. Feedback from a domain expert was collected. The effectiveness of the visual interface was evaluated through a formal user study. We also present several examples and a case study to demonstrate the usefulness of TrajGraph in urban transportation analysis.","AuthorNames-Deduped":"Xiaoke Huang;Ye Zhao;Chao Ma;Jing Yang;Xinyue Ye;Chong Zhang","AuthorNames":"Xiaoke Huang;Ye Zhao;Chao Ma;Jing Yang;Xinyue Ye;Chong Zhang","AuthorAffiliation":"Department of Computer Science, Kent State University;Department of Computer Science, Kent State University;Department of Computer Science, Kent State University;Department of Computer Science, University of North Carolina at Charlotte;Department of Geography, Kent State University;Department of Computer Science, University of North Carolina at Charlotte","InternalReferences":"10.1109/VAST.2009.5332593;10.1109/TVCG.2013.226;10.1109/TVCG.2009.145;10.1109/VAST.2011.6102455;10.1109/TVCG.2006.122;10.1109/TVCG.2012.265;10.1109/TVCG.2013.228;10.1109/TVCG.2014.2346746","AuthorKeywords":"Graph based visual analytics, Centrality, Taxi trajectories, Urban network, Transportation assessment","AminerCitationCount_02-2020":33,"AminerCitationCount_06-2020":63,"XploreCitationCount - 2020-01":65,"PubsCited":39,"Award":null,"image":"22tvcg01-huang-2467771-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"BiSet: Semantic Edge Bundling with Biclusters for Sensemaking","DOI":"10.1109/TVCG.2015.2467813","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467813","FirstPage":310,"LastPage":319,"PaperType":"J","Abstract":"Identifying coordinated relationships is an important task in data analytics. For example, an intelligence analyst might want to discover three suspicious people who all visited the same four cities. Existing techniques that display individual relationships, such as between lists of entities, require repetitious manual selection and significant mental aggregation in cluttered visualizations to find coordinated relationships. In this paper, we present BiSet, a visual analytics technique to support interactive exploration of coordinated relationships. In BiSet, we model coordinated relationships as biclusters and algorithmically mine them from a dataset. Then, we visualize the biclusters in context as bundled edges between sets of related entities. Thus, bundles enable analysts to infer task-oriented semantic insights about potentially coordinated activities. We make bundles as first class objects and add a new layer, “in-between”, to contain these bundle objects. Based on this, bundles serve to organize entities represented in lists and visually reveal their membership. Users can interact with edge bundles to organize related entities, and vice versa, for sensemaking purposes. With a usage scenario, we demonstrate how BiSet supports the exploration of coordinated relationships in text analytics.","AuthorNames-Deduped":"Maoyuan Sun;Peng Mi;Chris North;Naren Ramakrishnan","AuthorNames":"Maoyuan Sun;Peng Mi;Chris North;Naren Ramakrishnan","AuthorAffiliation":"Department of Computer Science, Discovery Analytics Center;Department of Computer Science, Discovery Analytics Center;Department of Computer Science, Discovery Analytics Center;Department of Computer Science, Discovery Analytics Center","InternalReferences":"10.1109/TVCG.2007.70521;10.1109/TVCG.2009.122;10.1109/TVCG.2008.135;10.1109/TVCG.2012.252;10.1109/TVCG.2012.260;10.1109/INFVIS.2004.1;10.1109/TVCG.2014.2346260;10.1109/TVCG.2007.70582;10.1109/TVCG.2006.147;10.1109/TVCG.2011.233;10.1109/VAST.2009.5333878;10.1109/TVCG.2011.250;10.1109/TVCG.2010.138;10.1109/TVCG.2014.2346752;10.1109/TVCG.2010.210;10.1109/TVCG.2011.183;10.1109/TVCG.2014.2346665","AuthorKeywords":"Bicluster, coordinated relationship, semantic edge bundling","AminerCitationCount_02-2020":16,"AminerCitationCount_06-2020":24,"XploreCitationCount - 2020-01":23,"PubsCited":58,"Award":null,"image":"22tvcg01-sun-2467813-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"VA2: A Visual Analytics Approach for Evaluating Visual Analytics Applications","DOI":"10.1109/TVCG.2015.2467871","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467871","FirstPage":61,"LastPage":70,"PaperType":"J","Abstract":"Evaluation has become a fundamental part of visualization research and researchers have employed many approaches from the field of human-computer interaction like measures of task performance, thinking aloud protocols, and analysis of interaction logs. Recently, eye tracking has also become popular to analyze visual strategies of users in this context. This has added another modality and more data, which requires special visualization techniques to analyze this data. However, only few approaches exist that aim at an integrated analysis of multiple concurrent evaluation procedures. The variety, complexity, and sheer amount of such coupled multi-source data streams require a visual analytics approach. Our approach provides a highly interactive visualization environment to display and analyze thinking aloud, interaction, and eye movement data in close relation. Automatic pattern finding algorithms allow an efficient exploratory search and support the reasoning process to derive common eye-interaction-thinking patterns between participants. In addition, our tool equips researchers with mechanisms for searching and verifying expected usage patterns. We apply our approach to a user study involving a visual analytics application and we discuss insights gained from this joint analysis. We anticipate our approach to be applicable to other combinations of evaluation techniques and a broad class of visualization applications.","AuthorNames-Deduped":"Tanja Blascheck;Markus John;Kuno Kurzhals;Steffen Koch;Thomas Ertl","AuthorNames":"Tanja Blascheck;Markus John;Kuno Kurzhals;Steffen Koch;Thomas Ertl","AuthorAffiliation":"Institute for Visualization and Interactive Systems (VIS), Germany;Institute for Visualization and Interactive Systems (VIS), Germany;Visualization Research Center, Germany;Institute for Visualization and Interactive Systems (VIS), Germany;Institute for Visualization and Interactive Systems (VIS), Germany","InternalReferences":"10.1109/TVCG.2012.276;10.1109/TVCG.2013.124;10.1109/VAST.2008.4677361;10.1109/VAST.2009.5333878;10.1109/TVCG.2014.2346677;10.1109/VAST.2010.5653598;10.1109/TVCG.2012.273;10.1109/VISUAL.2005.1532837","AuthorKeywords":"visual analytics, qualitative evaluation, thinking aloud, interaction logs, eye tracking, time series data","AminerCitationCount_02-2020":24,"AminerCitationCount_06-2020":36,"XploreCitationCount - 2020-01":29,"PubsCited":53,"Award":"HM","image":"22tvcg01-blascheck-2467871-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2015,"Title":"The Visual Causality Analyst: An Interactive Interface for Causal Reasoning","DOI":"10.1109/TVCG.2015.2467931","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467931","FirstPage":230,"LastPage":239,"PaperType":"J","Abstract":"Uncovering the causal relations that exist among variables in multivariate datasets is one of the ultimate goals in data analytics. Causation is related to correlation but correlation does not imply causation. While a number of casual discovery algorithms have been devised that eliminate spurious correlations from a network, there are no guarantees that all of the inferred causations are indeed true. Hence, bringing a domain expert into the casual reasoning loop can be of great benefit in identifying erroneous casual relationships suggested by the discovery algorithm. To address this need we present the Visual Causal Analyst - a novel visual causal reasoning framework that allows users to apply their expertise, verify and edit causal links, and collaborate with the causal discovery algorithm to identify a valid causal network. Its interface consists of both an interactive 2D graph view and a numerical presentation of salient statistical parameters, such as regression coefficients, p-values, and others. Both help users in gaining a good understanding of the landscape of causal structures particularly when the number of variables is large. Our framework is also novel in that it can handle both numerical and categorical variables within one unified model and return plausible results. We demonstrate its use via a set of case studies using multiple practical datasets.","AuthorNames-Deduped":"Jun Wang;Klaus Mueller","AuthorNames":"Jun Wang;Klaus Mueller","AuthorAffiliation":"Computer Science Department, Visual Analytics and Imaging Lab, Stony Brook, NY;Computer Science Department, Visual Analytics and Imaging Lab, Stony Brook, NY","InternalReferences":"10.1109/INFVIS.2003.1249025;10.1109/TVCG.2007.70528;10.1109/TVCG.2012.225;10.1109/VAST.2007.4388999","AuthorKeywords":"Visual knowledge discovery, Causality, Hypothesis testing, Visual evidence, High-dimensional data","AminerCitationCount_02-2020":8,"AminerCitationCount_06-2020":21,"XploreCitationCount - 2020-01":18,"PubsCited":31,"Award":null,"image":"22tvcg01-wang-2467931-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2015,"Title":"VEEVVIE: Visual Explorer for Empirical Visualization, VR and Interaction Experiments","DOI":"10.1109/TVCG.2015.2467954","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467954","FirstPage":111,"LastPage":120,"PaperType":"J","Abstract":"Empirical, hypothesis-driven, experimentation is at the heart of the scientific discovery process and has become commonplace in human-factors related fields. To enable the integration of visual analytics in such experiments, we introduce VEEVVIE, the Visual Explorer for Empirical Visualization, VR and Interaction Experiments. VEEVVIE is comprised of a back-end ontology which can model several experimental designs encountered in these fields. This formalization allows VEEVVIE to capture experimental data in a query-able form and makes it accessible through a front-end interface. This front-end offers several multi-dimensional visualization widgets with built-in filtering and highlighting functionality. VEEVVIE is also expandable to support custom experimental measurements and data types through a plug-in visualization widget architecture. We demonstrate VEEVVIE through several case studies of visual analysis, performed on the design and data collected during an experiment on the scalability of high-resolution, immersive, tiled-display walls.","AuthorNames-Deduped":"Charilaos Papadopoulos;Ievgeniia Gutenko;Arie E. Kaufman","AuthorNames":"C. Papadopoulos;I. Gutenko;A. E. Kaufman","AuthorAffiliation":"Dept. of Computer Science, Stony Brook University, Stony Brook, NY;Dept. of Computer Science, Stony Brook University, Stony Brook, NY;Dept. of Computer Science, Stony Brook University, Stony Brook, NY","InternalReferences":"10.1109/TVCG.2012.276;10.1109/TVCG.2012.251;10.1109/TVCG.2014.2346591;10.1109/TVCG.2010.157;10.1109/TVCG.2014.2346311;10.1109/INFVIS.2000.885086;10.1109/INFVIS.2004.12","AuthorKeywords":"Visual Analytics, Evaluation, User Studies, Ontology, Experiments, Interaction, Virtual Reality, Visualization","AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":6,"XploreCitationCount - 2020-01":3,"PubsCited":32,"Award":null,"image":"22tvcg01-papadopoulos-2467954-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"VAiRoma: A Visual Analytics System for Making Sense of Places, Times, and Events in Roman History","DOI":"10.1109/TVCG.2015.2467971","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467971","FirstPage":210,"LastPage":219,"PaperType":"J","Abstract":"Learning and gaining knowledge of Roman history is an area of interest for students and citizens at large. This is an example of a subject with great sweep (with many interrelated sub-topics over, in this case, a 3,000 year history) that is hard to grasp by any individual and, in its full detail, is not available as a coherent story. In this paper, we propose a visual analytics approach to construct a data driven view of Roman history based on a large collection of Wikipedia articles. Extracting and enabling the discovery of useful knowledge on events, places, times, and their connections from large amounts of textual data has always been a challenging task. To this aim, we introduce VAiRoma, a visual analytics system that couples state-of-the-art text analysis methods with an intuitive visual interface to help users make sense of events, places, times, and more importantly, the relationships between them. VAiRoma goes beyond textual content exploration, as it permits users to compare, make connections, and externalize the findings all within the visual interface. As a result, VAiRoma allows users to learn and create new knowledge regarding Roman history in an informed way. We evaluated VAiRoma with 16 participants through a user study, with the task being to learn about roman piazzas through finding relevant articles and new relationships. Our study results showed that the VAiRoma system enables the participants to find more relevant articles and connections compared to Web searches and literature search conducted in a roman library. Subjective feedback on VAiRoma was also very positive. In addition, we ran two case studies that demonstrate how VAiRoma can be used for deeper analysis, permitting the rapid discovery and analysis of a small number of key documents even when the original collection contains hundreds of thousands of documents.","AuthorNames-Deduped":"Isaac Cho;Wenwen Dou;Xiaoyu Wang;Eric Sauda;William Ribarsky","AuthorNames":"Isaac Cho;Wewnen Dou;Derek Xiaoyu Wang;Eric Sauda;William Ribarsky","AuthorAffiliation":"UNC Charlotte;UNC Charlotte","InternalReferences":"10.1109/VAST.2014.7042493;10.1109/VAST.2007.4389012;10.1109/TVCG.2014.2346431;10.1109/TVCG.2007.70617;10.1109/TVCG.2008.178;10.1109/VAST.2010.5652885;10.1109/TVCG.2011.239;10.1109/VAST.2012.6400485;10.1109/TVCG.2013.162;10.1109/INFVIS.2000.885098;10.1109/TVCG.2011.179;10.1109/TVCG.2014.2346481;10.1109/INFVIS.2000.885091","AuthorKeywords":"Visual Analytics, Text Analytics, Wikipedia","AminerCitationCount_02-2020":15,"AminerCitationCount_06-2020":26,"XploreCitationCount - 2020-01":19,"PubsCited":40,"Award":null,"image":"22tvcg01-cho-2467971-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2015,"Title":"Exploring Evolving Media Discourse Through Event Cueing","DOI":"10.1109/TVCG.2015.2467991","Link":"http://dx.doi.org/10.1109/TVCG.2015.2467991","FirstPage":220,"LastPage":229,"PaperType":"J","Abstract":"Online news, microblogs and other media documents all contain valuable insight regarding events and responses to events. Underlying these documents is the concept of framing, a process in which communicators act (consciously or unconsciously) to construct a point of view that encourages facts to be interpreted by others in a particular manner. As media discourse evolves, how topics and documents are framed can undergo change, shifting the discussion to different viewpoints or rhetoric. What causes these shifts can be difficult to determine directly; however, by linking secondary datasets and enabling visual exploration, we can enhance the hypothesis generation process. In this paper, we present a visual analytics framework for event cueing using media data. As discourse develops over time, our framework applies a time series intervention model which tests to see if the level of framing is different before or after a given date. If the model indicates that the times before and after are statistically significantly different, this cues an analyst to explore related datasets to help enhance their understanding of what (if any) events may have triggered these changes in discourse. Our framework consists of entity extraction and sentiment analysis as lenses for data exploration and uses two different models for intervention analysis. To demonstrate the usage of our framework, we present a case study on exploring potential relationships between climate change framing and conflicts in Africa.","AuthorNames-Deduped":"Yafeng Lu;Michael Steptoe;Sarah Burke;Hong Wang;Jiun-Yi Tsai;Hasan Davulcu;Douglas C. Montgomery;Steven R. Corman;Ross Maciejewski","AuthorNames":"Yafeng Lu;Michael Steptoe;Sarah Burke;Hong Wang;Jiun-Yi Tsai;Hasan Davulcu;Douglas Montgomery;Steven R. Corman;Ross Maciejewski","AuthorAffiliation":"Arizona State University;Arizona State University;Arizona State University;Arizona State University;Arizona State University;Arizona State University;Arizona State University;Arizona State University;Arizona State University","InternalReferences":"10.1109/TVCG.2013.222;10.1109/VAST.2011.6102488;10.1109/VAST.2012.6400557;10.1109/VAST.2012.6400485;10.1109/TVCG.2013.162;10.1109/VAST.2008.4677364;10.1109/TVCG.2014.2346682;10.1109/VAST.2014.7042484;10.1109/TVCG.2011.179;10.1109/VAST.2014.7042494;10.1109/VAST.2012.6400491;10.1109/VAST.2009.5333919;10.1109/INFVIS.1999.801851;10.1109/TVCG.2012.225;10.1109/TVCG.2014.2346913","AuthorKeywords":"Media Analysis, Time Series Analysis, Event Detection","AminerCitationCount_02-2020":12,"AminerCitationCount_06-2020":15,"XploreCitationCount - 2020-01":16,"PubsCited":43,"Award":null,"image":"22tvcg01-lu-2467991-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2015,"Title":"LiteVis: Integrated Visualization for Simulation-Based Decision Support in Lighting Design","DOI":"10.1109/TVCG.2015.2468011","Link":"http://dx.doi.org/10.1109/TVCG.2015.2468011","FirstPage":290,"LastPage":299,"PaperType":"J","Abstract":"State-of-the-art lighting design is based on physically accurate lighting simulations of scenes such as offices. The simulation results support lighting designers in the creation of lighting configurations, which must meet contradicting customer objectives regarding quality and price while conforming to industry standards. However, current tools for lighting design impede rapid feedback cycles. On the one side, they decouple analysis and simulation specification. On the other side, they lack capabilities for a detailed comparison of multiple configurations. The primary contribution of this paper is a design study of LiteVis, a system for efficient decision support in lighting design. LiteVis tightly integrates global illumination-based lighting simulation, a spatial representation of the scene, and non-spatial visualizations of parameters and result indicators. This enables an efficient iterative cycle of simulation parametrization and analysis. Specifically, a novel visualization supports decision making by ranking simulated lighting configurations with regard to a weight-based prioritization of objectives that considers both spatial and non-spatial characteristics. In the spatial domain, novel concepts support a detailed comparison of illumination scenarios. We demonstrate LiteVis using a real-world use case and report qualitative feedback of lighting designers. This feedback indicates that LiteVis successfully supports lighting designers to achieve key tasks more efficiently and with greater certainty.","AuthorNames-Deduped":"Johannes Sorger;Thomas Ortner;Christian Luksch;Michael Schwärzler;M. Eduard Gröller;Harald Piringer","AuthorNames":"Johannes Sorger;Thomas Ortner;Christian Luksch;Michael Schwärzler;Eduard Gröller;Harald Piringer","AuthorAffiliation":"VRVis Research Center;VRVis Research Center;VRVis Research Center;VRVis Research Center;TU Wien;VRVis Research Center","InternalReferences":"10.1109/TVCG.2014.2346626;10.1109/TVCG.2011.185;10.1109/TVCG.2010.190;10.1109/TVCG.2013.147;10.1109/INFVIS.2003.1249032;10.1109/TVCG.2013.173;10.1109/TVCG.2009.110;10.1109/TVCG.2014.2346321","AuthorKeywords":"Integrating Spatial and Non-Spatial Data Visualization, Visualization in Physical Sciences and Engineering, Coordinated and Multiple Views, Visual Knowledge Discovery","AminerCitationCount_02-2020":8,"AminerCitationCount_06-2020":15,"XploreCitationCount - 2020-01":11,"PubsCited":34,"Award":null,"image":"22tvcg01-sorger-2468011-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Reducing Snapshots to Points: A Visual Analytics Approach to Dynamic Network Exploration","DOI":"10.1109/TVCG.2015.2468078","Link":"http://dx.doi.org/10.1109/TVCG.2015.2468078","FirstPage":1,"LastPage":10,"PaperType":"J","Abstract":"We propose a visual analytics approach for the exploration and analysis of dynamic networks. We consider snapshots of the network as points in high-dimensional space and project these to two dimensions for visualization and interaction using two juxtaposed views: one for showing a snapshot and one for showing the evolution of the network. With this approach users are enabled to detect stable states, recurring states, outlier topologies, and gain knowledge about the transitions between states and the network evolution in general. The components of our approach are discretization, vectorization and normalization, dimensionality reduction, and visualization and interaction, which are discussed in detail. The effectiveness of the approach is shown by applying it to artificial and real-world dynamic networks.","AuthorNames-Deduped":"Stef van den Elzen;Danny Holten;Jorik Blaas;Jarke J. van Wijk","AuthorNames":"Stef van den Elzen;Danny Holten;Jorik Blaas;Jarke J. van Wijk","AuthorAffiliation":"Eindhoven University of Technology;SynerScope B. V.;SynerScope B. V.;Eindhoven University of Technology","InternalReferences":"10.1109/TVCG.2011.226;10.1109/INFVIS.2004.18;10.1109/TVCG.2013.198;10.1109/TVCG.2006.147;10.1109/TVCG.2006.193;10.1109/TVCG.2008.125;10.1109/TVCG.2011.178;10.1109/INFVIS.1999.801851","AuthorKeywords":"Dynamic Networks, Exploration, Dimensionality Reduction","AminerCitationCount_02-2020":33,"AminerCitationCount_06-2020":65,"XploreCitationCount - 2020-01":52,"PubsCited":63,"Award":"BP","image":"22tvcg01-vandenelzen-2468078-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"MobilityGraphs: Visual Analysis of Mass Mobility Dynamics via Spatio-Temporal Graphs and Clustering","DOI":"10.1109/TVCG.2015.2468111","Link":"http://dx.doi.org/10.1109/TVCG.2015.2468111","FirstPage":11,"LastPage":20,"PaperType":"J","Abstract":"Learning more about people mobility is an important task for official decision makers and urban planners. Mobility data sets characterize the variation of the presence of people in different places over time as well as movements (or flows) of people between the places. The analysis of mobility data is challenging due to the need to analyze and compare spatial situations (i.e., presence and flows of people at certain time moments) and to gain an understanding of the spatio-temporal changes (variations of situations over time). Traditional flow visualizations usually fail due to massive clutter. Modern approaches offer limited support for investigating the complex variation of the movements over longer time periods. We propose a visual analytics methodology that solves these issues by combined spatial and temporal simplifications. We have developed a graph-based method, called MobilityGraphs, which reveals movement patterns that were occluded in flow maps. Our method enables the visual representation of the spatio-temporal variation of movements for long time series of spatial situations originally containing a large number of intersecting flows. The interactive system supports data exploration from various perspectives and at various levels of detail by interactive setting of clustering parameters. The feasibility our approach was tested on aggregated mobility data derived from a set of geolocated Twitter posts within the Greater London city area and mobile phone call data records in Abidjan, Ivory Coast. We could show that MobilityGraphs support the identification of regular daily and weekly movement patterns of resident population.","AuthorNames-Deduped":"Tatiana von Landesberger;Felix Brodkorb;Philipp Roskosch;Natalia V. Andrienko;Gennady L. Andrienko;Andreas Kerren","AuthorNames":"Tatiana von Landesberger;Felix Brodkorb;Philipp Roskosch;Natalia Andrienko;Gennady Andrienko;Andreas Kerren","AuthorAffiliation":"Technical University of Darmstadt, Germany;Technical University of Darmstadt, Germany;Technical University of Darmstadt, Germany;Fraunhofer IAIS, Bonn, Germany;Fraunhofer IAIS, Bonn, Germany;Fraunhofer IAIS, Bonn, Germany","InternalReferences":"10.1109/TVCG.2011.202;10.1109/TVCG.2011.226;10.1109/TVCG.2011.233;10.1109/INFVIS.2004.18;10.1109/TVCG.2009.143;10.1109/TVCG.2014.2346271;10.1109/TVCG.2008.125;10.1109/TVCG.2014.2346441;10.1109/INFVIS.1999.801851;10.1109/VAST.2012.6400553;10.1109/VAST.2009.5333893;10.1109/INFVIS.2005.1532150","AuthorKeywords":"Visual analytics, movement data, networks, graphs, temporal aggregation, spatial aggregation, flows, clustering","AminerCitationCount_02-2020":40,"AminerCitationCount_06-2020":68,"XploreCitationCount - 2020-01":70,"PubsCited":56,"Award":null,"image":"22tvcg01-vonlandesberger-2468111-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"egoSlider: Visual Analysis of Egocentric Network Evolution","DOI":"10.1109/TVCG.2015.2468151","Link":"http://dx.doi.org/10.1109/TVCG.2015.2468151","FirstPage":260,"LastPage":269,"PaperType":"J","Abstract":"Ego-network, which represents relationships between a specific individual, i.e., the ego, and people connected to it, i.e., alters, is a critical target to study in social network analysis. Evolutionary patterns of ego-networks along time provide huge insights to many domains such as sociology, anthropology, and psychology. However, the analysis of dynamic ego-networks remains challenging due to its complicated time-varying graph structures, for example: alters come and leave, ties grow stronger and fade away, and alter communities merge and split. Most of the existing dynamic graph visualization techniques mainly focus on topological changes of the entire network, which is not adequate for egocentric analytical tasks. In this paper, we present egoSlider, a visual analysis system for exploring and comparing dynamic ego-networks. egoSlider provides a holistic picture of the data through multiple interactively coordinated views, revealing ego-network evolutionary patterns at three different layers: a macroscopic level for summarizing the entire ego-network data, a mesoscopic level for overviewing specific individuals' ego-network evolutions, and a microscopic level for displaying detailed temporal information of egos and their alters. We demonstrate the effectiveness of egoSlider with a usage scenario with the DBLP publication records. Also, a controlled user study indicates that in general egoSlider outperforms a baseline visualization of dynamic networks for completing egocentric analytical tasks.","AuthorNames-Deduped":"Yanhong Wu;Naveen Pitipornvivat;Jian Zhao 0010;Sixiao Yang;Guowei Huang;Huamin Qu","AuthorNames":"Yanhong Wu;Naveen Pitipornvivat;Jian Zhao;Sixiao Yang;Guowei Huang;Huamin Qu","AuthorAffiliation":"Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Autodesk Research;Huawei Technologies Co. Ltd.;Huawei Technologies Co. Ltd.;Hong Kong University of Science and Technology","InternalReferences":"10.1109/TVCG.2011.169;10.1109/TVCG.2011.226;10.1109/TVCG.2006.147;10.1109/TVCG.2013.149","AuthorKeywords":"Egocentric network, dynamic graph, network visualization, glyph-based design, visual analytics","AminerCitationCount_02-2020":24,"AminerCitationCount_06-2020":43,"XploreCitationCount - 2020-01":34,"PubsCited":53,"Award":null,"image":"22tvcg01-wu-2468151-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2015,"Title":"3D Regression Heat Map Analysis of Population Study Data","DOI":"10.1109/TVCG.2015.2468291","Link":"http://dx.doi.org/10.1109/TVCG.2015.2468291","FirstPage":81,"LastPage":90,"PaperType":"J","Abstract":"Epidemiological studies comprise heterogeneous data about a subject group to define disease-specific risk factors. These data contain information (features) about a subject's lifestyle, medical status as well as medical image data. Statistical regression analysis is used to evaluate these features and to identify feature combinations indicating a disease (the target feature). We propose an analysis approach of epidemiological data sets by incorporating all features in an exhaustive regression-based analysis. This approach combines all independent features w.r.t. a target feature. It provides a visualization that reveals insights into the data by highlighting relationships. The 3D Regression Heat Map, a novel 3D visual encoding, acts as an overview of the whole data set. It shows all combinations of two to three independent features with a specific target disease. Slicing through the 3D Regression Heat Map allows for the detailed analysis of the underlying relationships. Expert knowledge about disease-specific hypotheses can be included into the analysis by adjusting the regression model formulas. Furthermore, the influences of features can be assessed using a difference view comparing different calculation results. We applied our 3D Regression Heat Map method to a hepatic steatosis data set to reproduce results from a data mining-driven analysis. A qualitative analysis was conducted on a breast density data set. We were able to derive new hypotheses about relations between breast density and breast lesions with breast cancer. With the 3D Regression Heat Map, we present a visual overview of epidemiological data that allows for the first time an interactive regression-based analysis of large feature sets with respect to a disease.","AuthorNames-Deduped":"Paul Klemm;Kai Lawonn;Sylvia Saalfeld;Uli Niemann;Katrin Hegenscheid;Henry Völzke;Bernhard Preim","AuthorNames":"Paul Klemm;Kai Lawonn;Sylvia Glaßer;Uli Niemann;Katrin Hegenscheid;Henry Völzke;Bernhard Preim","AuthorAffiliation":"Otto-von-Guericke University Magdeburg, Germany;Otto-von-Guericke University Magdeburg, Germany;Otto-von-Guericke University Magdeburg, Germany;Otto-von-Guericke University Magdeburg, Germany;Ernst-Moritz-Arndt University Greifswald, Germany;Ernst-Moritz-Arndt University Greifswald, Germany;Otto-von-Guericke University Magdeburg, Germany","InternalReferences":"10.1109/TVCG.2011.229;10.1109/TVCG.2011.185;10.1109/VAST.2009.5333431;10.1109/TVCG.2013.160;10.1109/TVCG.2014.2346591;10.1109/TVCG.2013.161;10.1109/TVCG.2013.125;10.1109/TVCG.2014.2346321","AuthorKeywords":"Interactive Visual Analysis, Regression Analysis, Heat Map, Epidemiology, Breast Cancer, Hepatic Steatosis","AminerCitationCount_02-2020":7,"AminerCitationCount_06-2020":14,"XploreCitationCount - 2020-01":16,"PubsCited":43,"Award":null,"image":"22tvcg01-klemm-2468291-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"MotionFlow: Visual Abstraction and Aggregation of Sequential Patterns in Human Motion Tracking Data","DOI":"10.1109/TVCG.2015.2468292","Link":"http://dx.doi.org/10.1109/TVCG.2015.2468292","FirstPage":21,"LastPage":30,"PaperType":"J","Abstract":"Pattern analysis of human motions, which is useful in many research areas, requires understanding and comparison of different styles of motion patterns. However, working with human motion tracking data to support such analysis poses great challenges. In this paper, we propose MotionFlow, a visual analytics system that provides an effective overview of various motion patterns based on an interactive flow visualization. This visualization formulates a motion sequence as transitions between static poses, and aggregates these sequences into a tree diagram to construct a set of motion patterns. The system also allows the users to directly reflect the context of data and their perception of pose similarities in generating representative pose states. We provide local and global controls over the partition-based clustering process. To support the users in organizing unstructured motion data into pattern groups, we designed a set of interactions that enables searching for similar motion sequences from the data, detailed exploration of data subsets, and creating and modifying the group of motion patterns. To evaluate the usability of MotionFlow, we conducted a user study with six researchers with expertise in gesture-based interaction design. They used MotionFlow to explore and organize unstructured motion tracking data. Results show that the researchers were able to easily learn how to use MotionFlow, and the system effectively supported their pattern analysis activities, including leveraging their perception and domain knowledge.","AuthorNames-Deduped":"Sujin Jang;Niklas Elmqvist;Karthik Ramani","AuthorNames":"Sujin Jang;Niklas Elmqvist;Karthik Ramani","AuthorAffiliation":"Purdue University, West Lafayette, IN, USA;University of Maryland, College Park, MD, USA;Purdue University, West Lafayette, IN, USA","InternalReferences":"10.1109/TVCG.2013.178;10.1109/TVCG.2009.181;10.1109/TVCG.2011.239;10.1109/TVCG.2014.2346682;10.1109/TVCG.2012.258;10.1109/TVCG.2013.196;10.1109/TVCG.2013.200;10.1109/TVCG.2006.192;10.1109/INFVIS.2005.1532152;10.1109/TVCG.2013.181;10.1109/TVCG.2010.149;10.1109/VISUAL.2002.1183778;10.1109/TVCG.2008.172;10.1109/TVCG.2012.225;10.1109/TVCG.2014.2346920","AuthorKeywords":"Human motion visualization, interactive clustering, motion tracking data, expert reviews, user study","AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":7,"XploreCitationCount - 2020-01":7,"PubsCited":46,"Award":null,"image":"22tvcg01-jang-2468292-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2015,"Title":"Wavelet-based visualization of time-varying data on graphs","DOI":"10.1109/VAST.2015.7347624","Link":"http://dx.doi.org/10.1109/VAST.2015.7347624","FirstPage":1,"LastPage":8,"PaperType":"C","Abstract":"Visualizing time-varying data defined on the nodes of a graph is a challenging problem that has been faced with different approaches. Although techniques based on aggregation, topology, and topic modeling have proven their usefulness, the visual analysis of smooth and/or abrupt data variations as well as the evolution of such variations over time are aspects not properly tackled by existing methods. In this work we propose a novel visualization methodology that relies on graph wavelet theory and stacked graph metaphor to enable the visual analysis of time-varying data defined on the nodes of a graph. The proposed method is able to identify regions where data presents abrupt and mild spacial and/or temporal variation while still been able to show how such changes evolve over time, making the identification of events an easier task. The usefulness of our approach is shown through a set of results using synthetic as well as a real data set involving taxi trips in downtown Manhattan. The methodology was able to reveal interesting phenomena and events such as the identification of specific locations with abrupt variation in the number of taxi pickups.","AuthorNames-Deduped":"Paola Valdivia;Fabio Dias;Fabiano Petronetto;Cláudio T. Silva;Luis Gustavo Nonato","AuthorNames":"Paola Valdivia;Fabio Dias;Fabiano Petronetto;Cláudio T. Silva;L. G. Nonato","AuthorAffiliation":"University of São Paulo, Brazil;University of São Paulo, Brazil;UFES, Brazil;New York University, USA;University of São Paulo, Brazil","InternalReferences":"10.1109/VAST.2008.4677356;10.1109/TVCG.2014.2346449;10.1109/TVCG.2013.226;10.1109/INFVIS.2000.885098;10.1109/TVCG.2013.228","AuthorKeywords":"Time-varying data, graph wavelets, stacked graph visualization","AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":11,"XploreCitationCount - 2020-01":10,"PubsCited":22,"Award":null,"image":"7347624-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Mixed-initiative visual analytics using task-driven recommendations","DOI":"10.1109/VAST.2015.7347625","Link":"http://dx.doi.org/10.1109/VAST.2015.7347625","FirstPage":9,"LastPage":16,"PaperType":"C","Abstract":"Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.","AuthorNames-Deduped":"Kristin A. Cook;Nick Cramer;David J. Israel;Michael Wolverton;Joe Bruce;Russ Burtner;Alex Endert","AuthorNames":"Kristin Cook;Nick Cramer;David Israel;Michael Wolverton;Joe Bruce;Russ Burtner;Alex Endert","AuthorAffiliation":"Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA;SRI International, USA;SRI International, USA;Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA;Georgia Institute of Technology, USA","InternalReferences":"10.1109/VAST.2012.6400486;10.1109/VAST.2011.6102438;10.1109/VAST.2012.6400559;10.1109/TVCG.2014.2346573;10.1109/VAST.2014.7042492;10.1109/TVCG.2008.174;10.1109/TVCG.2013.225","AuthorKeywords":"mixed-initiative visual analytics, task modeling, recommender systems, sensemaking","AminerCitationCount_02-2020":8,"AminerCitationCount_06-2020":12,"XploreCitationCount - 2020-01":8,"PubsCited":36,"Award":null,"image":"7347625-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Integrating predictive analytics into a spatiotemporal epidemic simulation","DOI":"10.1109/VAST.2015.7347626","Link":"http://dx.doi.org/10.1109/VAST.2015.7347626","FirstPage":17,"LastPage":24,"PaperType":"C","Abstract":"The Epidemic Simulation System (EpiSimS) is a scalable, complex modeling tool for analyzing disease within the United States. Due to its high input dimensionality, time requirements, and resource constraints, simulating over the entire parameter space is unfeasible. One solution is to take a granular sampling of the input space and use simpler predictive models (emulators) in between. The quality of the implemented emulator depends on many factors: robustness, sophistication, configuration, and suitability to the input data. Visual analytics can be leveraged to provide guidance and understanding of these things to the user. In this paper, we have implemented a novel interface and workflow for emulator building and use. We introduce a workflow to build emulators, make predictions, and then analyze the results. Our prediction process first predicts temporal time series, and uses these to derive predicted spatial densities. Integrated into the EpiSimS framework, we target users who are non-experts at statistical modeling. This approach allows for a high level of analysis into the state of the built emulators and their resultant predictions. We present our workflow, models, the associated system, and evaluate the overall utility with feedback from EpiSimS scientists.","AuthorNames-Deduped":"Chris Bryan;Xue Wu;Susan M. Mniszewski;Kwan-Liu Ma","AuthorNames":"Chris Bryan;Xue Wu;Susan Mniszewski;Kwan-Liu Ma","AuthorAffiliation":"VIDi @ U.C. Davis, USA;VIDi @ U.C. Davis, USA;Los Alamos National Lab, USA;VIDi @ U.C. Davis, USA","InternalReferences":"10.1109/VAST.2011.6102457;10.1109/INFVIS.1998.729563;10.1109/TVCG.2014.2346926;10.1109/TVCG.2013.125;10.1109/TVCG.2010.181;10.1109/TVCG.2014.2346321;10.1109/TVCG.2011.248;10.1109/TVCG.2012.190","AuthorKeywords":"Predictive Modeling, Visual Analytics, Epidemic Visualization, Spatial-Temporal Systems","AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":7,"XploreCitationCount - 2020-01":4,"PubsCited":36,"Award":null,"image":"7347626-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Collaborative visual analysis with RCloud","DOI":"10.1109/VAST.2015.7347627","Link":"http://dx.doi.org/10.1109/VAST.2015.7347627","FirstPage":25,"LastPage":32,"PaperType":"C","Abstract":"Consider the emerging role of data science teams embedded in larger organizations. Individual analysts work on loosely related problems, and must share their findings with each other and the organization at large, moving results from exploratory data analyses (EDA) into automated visualizations, diagnostics and reports deployed for wider consumption. There are two problems with the current practice. First, there are gaps in this workflow: EDA is performed with one set of tools, and automated reports and deployments with another. Second, these environments often assume a single-developer perspective, while data scientist teams could get much benefit from easier sharing of scripts and data feeds, experiments, annotations, and automated recommendations, which are well beyond what traditional version control systems provide. We contribute and justify the following three requirements for systems built to support current data science teams and users: discoverability, technology transfer, and coexistence. In addition, we contribute the design and implementation of RCloud, a system that supports the requirements of collaborative data analysis, visualization and web deployment. About 100 people used RCloud for two years. We report on interviews with some of these users, and discuss design decisions, tradeoffs and limitations in comparison to other approaches.","AuthorNames-Deduped":"Stephen C. North;Carlos Scheidegger;Simon Urbanek;Gordon Woodhull","AuthorNames":"Stephen North;Carlos Scheidegger;Simon Urbanek;Gordon Woodhull","AuthorAffiliation":"Infovisible, USA;University of Arizona, USA;AT&T Labs, USA;AT&T Labs, USA","InternalReferences":"10.1109/TVCG.2011.185;10.1109/VAST.2007.4389011;10.1109/TVCG.2012.219;10.1109/TVCG.2009.195;10.1109/TVCG.2007.70577","AuthorKeywords":"visual analytics process, provenance, collaboration, visualization, computer-supported cooperative work","AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":1,"PubsCited":40,"Award":null,"image":"7347627-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Four considerations for supporting visual analysis in display ecologies","DOI":"10.1109/VAST.2015.7347628","Link":"http://dx.doi.org/10.1109/VAST.2015.7347628","FirstPage":33,"LastPage":40,"PaperType":"C","Abstract":"The current proliferation of large displays and mobile devices presents a number of exciting opportunities for visual analytics and information visualization. The display ecology enables multiple displays to function in concert within a broader technological environment to accomplish visual analysis tasks. Based on a comprehensive survey of multi-display systems from a variety of fields, we propose four key considerations for visual analysis in display ecologies: 1) Display Composition, 2) Information Coordination/Transfer, 3) Information Connection, and 4) Display Membership. Different aspects of display ecologies stemming from these design considerations will enable users to transform and empower multiple displays as a display ecology for visual analysis.","AuthorNames-Deduped":"Haeyong Chung;Chris North;Sarang Joshi;Jian Chen","AuthorNames":"Haeyong Chung;Chris North;Sarang Joshi;Jian Chen","AuthorAffiliation":"University of Alabama Huntsville, USA;Virginia Tech, USA;Virginia Tech, USA;University of Maryland Baltimore County, USA","InternalReferences":"10.1109/VAST.2008.4677358","AuthorKeywords":null,"AminerCitationCount_02-2020":6,"AminerCitationCount_06-2020":7,"XploreCitationCount - 2020-01":5,"PubsCited":51,"Award":null,"image":"7347628-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Supporting activity recognition by visual analytics","DOI":"10.1109/VAST.2015.7347629","Link":"http://dx.doi.org/10.1109/VAST.2015.7347629","FirstPage":41,"LastPage":48,"PaperType":"C","Abstract":"Recognizing activities has become increasingly relevant in many application domains, such as security or ambient assisted living. To handle different scenarios, the underlying automated algorithms are configured using multiple input parameters. However, the influence and interplay of these parameters is often not clear, making exhaustive evaluations necessary. On this account, we propose a visual analytics approach to supporting users in understanding the complex relationships among parameters, recognized activities, and associated accuracies. First, representative parameter settings are determined. Then, the respective output is computed and statistically analyzed to assess parameters' influence in general. Finally, visualizing the parameter settings along with the activities provides overview and allows to investigate the computed results in detail. Coordinated interaction helps to explore dependencies, compare different settings, and examine individual activities. By integrating automated, visual, and interactive means users can select parameter values that meet desired quality criteria. We demonstrate the application of our solution in a use case with realistic complexity, involving a study of human protagonists in daily living with respect to hundreds of parameter settings.","AuthorNames-Deduped":"Martin Röhlig;Martin Luboschik;Frank Krüger 0001;Thomas Kirste;Heidrun Schumann;Markus Bögl;Bilal Alsallakh;Silvia Miksch","AuthorNames":"Martin Röhlig;Martin Luboschik;Frank Krüger;Thomas Kirste;Heidrun Schumann;Markus Bögl;Bilal Alsallakh;Silvia Miksch","AuthorAffiliation":"University of Rostock, Germany;University of Rostock, Germany;University of Rostock, Germany;University of Rostock, Germany;University of Rostock, Germany;Vienna University of Technology, Austria;Vienna University of Technology, Austria;Vienna University of Technology, Austria","InternalReferences":"10.1109/TVCG.2014.2346454;10.1109/TVCG.2011.253;10.1109/TVCG.2014.2346321;10.1109/TVCG.2011.248;10.1109/TVCG.2009.187;10.1109/VAST.2009.5332595","AuthorKeywords":null,"AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":6,"XploreCitationCount - 2020-01":4,"PubsCited":23,"Award":null,"image":"7347629-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"iVizTRANS: Interactive visual learning for home and work place detection from massive public transportation data","DOI":"10.1109/VAST.2015.7347630","Link":"http://dx.doi.org/10.1109/VAST.2015.7347630","FirstPage":49,"LastPage":56,"PaperType":"C","Abstract":"Using transport smart card transaction data to understand the homework dynamics of a city for urban planning is emerging as an alternative to traditional surveys which may be conducted every few years are no longer effective and efficient for the rapidly transforming modern cities. As commuters travel patterns are highly diverse, existing rule-based methods are not fully adequate. In this paper, we present iVizTRANS - a tool which combines an interactive visual analytics (VA) component to aid urban planners to analyse complex travel patterns and decipher activity locations for single public transport commuters. It is coupled with a machine learning component that iteratively learns from the planners classifications to train a classifier. The classifier is then applied to the city-wide smart card data to derive the dynamics for all public transport commuters. Our evaluation shows it outperforms the rule-based methods in previous work.","AuthorNames-Deduped":"Liang Yu;Wei Wu;Xiaohui Li;Guangxia Li;Wee Siong Ng;See-Kiong Ng;Zhongwen Huang;Anushiya Arunan;Hui Min Watt","AuthorNames":"Liang Yu;Wei Wu;Xiaohui Li;Guangxia Li;Wee Siong Ng;See-Kiong Ng;Zhongwen Huang;Anushiya Arunan;Hui Min Watt","AuthorAffiliation":"Institute for Infocomm Research, Singapore;Institute for Infocomm Research, Singapore;Institute for Infocomm Research, Singapore;Institute for Infocomm Research, Singapore;Institute for Infocomm Research, Singapore;Institute for Infocomm Research, Singapore;Urban Redevelopment Authority, Singapore;Urban Redevelopment Authority, Singapore;Urban Redevelopment Authority, Singapore","InternalReferences":"10.1109/INFVIS.2004.27;10.1109/INFVIS.2002.1173155","AuthorKeywords":"Smart card data, origin-destination (OD), spatiotemporal visualization, clustering, machine learning","AminerCitationCount_02-2020":6,"AminerCitationCount_06-2020":12,"XploreCitationCount - 2020-01":0,"PubsCited":21,"Award":null,"image":"7347630-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"DemographicVis: Analyzing demographic information based on user generated content","DOI":"10.1109/VAST.2015.7347631","Link":"http://dx.doi.org/10.1109/VAST.2015.7347631","FirstPage":57,"LastPage":64,"PaperType":"C","Abstract":"The wide-spread of social media provides unprecedented sources of written language that can be used to model and infer online demographics. In this paper, we introduce a novel visual text analytics system, DemographicVis, to aid interactive analysis of such demographic information based on user-generated content. Our approach connects categorical data (demographic information) with textual data, allowing users to understand the characteristics of different demographic groups in a transparent and exploratory manner. The modeling and visualization are based on ground truth demographic information collected via a survey conducted on Reddit.com. Detailed user information is taken into our modeling process that connects the demographic groups with features that best describe the distinguishing characteristics of each group. Features including topical and linguistic are generated from the user-generated contents. Such features are then analyzed and ranked based on their ability to predict the users' demographic information. To enable interactive demographic analysis, we introduce a web-based visual interface that presents the relationship of the demographic groups, their topic interests, as well as the predictive power of various features. We present multiple case studies to showcase the utility of our visual analytics approach in exploring and understanding the interests of different demographic groups. We also report results from a comparative evaluation, showing that the DemographicVis is quantitatively superior or competitive and subjectively preferred when compared to a commercial text analysis tool.","AuthorNames-Deduped":"Wenwen Dou;Isaac Cho;Omar ElTayeby;Jaegul Choo;Xiaoyu Wang;William Ribarsky","AuthorNames":"Wenwen Dou;Isaac Cho;Omar ElTayeby;Jaegul Choo;Xiaoyu Wang;William Ribarsky","AuthorAffiliation":"UNC Charlotte, USA;UNC Charlotte, USA;UNC Charlotte, USA;Korea University, South Korea;Taste Analytics, USA;UNC Charlotte, USA","InternalReferences":"10.1109/VAST.2014.7042493;10.1109/TVCG.2013.212;10.1109/TVCG.2014.2346433;10.1109/VAST.2011.6102461;10.1109/TVCG.2014.2346920","AuthorKeywords":"Visual Text Analysis, User Interface, Social Media, Demographic Analysis","AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":10,"XploreCitationCount - 2020-01":7,"PubsCited":24,"Award":null,"image":"7347631-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2015,"Title":"EgoNetCloud: Event-based egocentric dynamic network visualization","DOI":"10.1109/VAST.2015.7347632","Link":"http://dx.doi.org/10.1109/VAST.2015.7347632","FirstPage":65,"LastPage":72,"PaperType":"C","Abstract":"Event-based egocentric dynamic networks are an important class of networks widely seen in many domains. In this paper, we present a visual analytics approach for these networks by combining data-driven network simplifications with a novel visualization design - EgoNetCloud. In particular, an integrated data processing pipeline is proposed to prune, compress and filter the networks into smaller but salient abstractions. To accommodate the simplified network into the visual design, we introduce a constrained graph layout algorithm on the dynamic network. Through a real-life case study as well as conversations with the domain expert, we demonstrate the effectiveness of the EgoNetCloud design and system in completing analysis tasks on event-based dynamic networks. The user study comparing EgoNetCloud with a working system on academic search confirms the effectiveness and convenience of our visual analytics based approach.","AuthorNames-Deduped":"Qingsong Liu;Yifan Hu;Lei Shi;Xinzhu Mu;Yutao Zhang;Jie Tang 0001","AuthorNames":"Qingsong Liu;Yifan Hu;Lei Shi;Xinzhu Mu;Yutao Zhang;Jie Tang","AuthorAffiliation":"SKLCS, Institute of Software, Chinese Academy of Sciences, China;Yahoo Labs, New York, USA;SKLCS, Institute of Software, Chinese Academy of Sciences, China;Academy of Art and Design, Tsinghua University, China;Department of Computer Science and Technology, Tsinghua University, China;Department of Computer Science and Technology, Tsinghua University, China","InternalReferences":"10.1109/TVCG.2010.159;10.1109/TVCG.2011.226;10.1109/TVCG.2011.213","AuthorKeywords":null,"AminerCitationCount_02-2020":6,"AminerCitationCount_06-2020":13,"XploreCitationCount - 2020-01":1,"PubsCited":31,"Award":null,"image":"7347632-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"FPSSeer: Visual analysis of game frame rate data","DOI":"10.1109/VAST.2015.7347633","Link":"http://dx.doi.org/10.1109/VAST.2015.7347633","FirstPage":73,"LastPage":80,"PaperType":"C","Abstract":"The rate at which frames are rendered in a computer game directly influences both game playability and enjoyability. Players frequently have to deal with the trade-off between high frame rates and good resolution. Analyzing patterns in frame rate data and their correlation with the overall game performance is important in designing games (e.g., graphic card/display setting suggestion and game performance measurement). However, this task is challenging because game frame rates vary both temporally and spatially. In addition, players may adjust their display settings based on their gaming experience and hardware conditions, which further contributes to the unpredictability of frame rates. In this paper, we present a comprehensive visual analytics system FPSSeer, to help game designers gain insight into frame rate data. Our system consists of four major views: 1) a frame rate view to show the overall distribution in a geographic scale, 2) a grid view to show the frame rate distribution and grid element clusters based on their similarity, 3) a FootRiver view to reveal the temporal patterns in game condition changes and potential spatiotemporal correlations, and 4) a comparison view to evaluate game performance discrepancy under different game tests. The real-world case studies demonstrate the effectiveness of our system. The system has been applied to an online commercial game to monitor its performance and to provide feedbacks to designers and developers.","AuthorNames-Deduped":"Quan Li;Peng Xu;Huamin Qu","AuthorNames":"Quan Li;Peng Xu;Huamin Qu","AuthorAffiliation":"NetEase Games, NetEase, Inc., Hong Kong University of Science and Technology, China;NetEase Games, NetEase, Inc., China;Hong Kong University of Science and Technology, China","InternalReferences":"10.1109/TVCG.2008.166;10.1109/INFVIS.2000.885098;10.1109/TVCG.2011.202;10.1109/TVCG.2014.2346445;10.1109/INFVIS.2001.963273","AuthorKeywords":"frame rate data, game performance evaluation, visual analytics","AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":0,"PubsCited":24,"Award":null,"image":"7347633-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2015,"Title":"Comparative visual analysis of vector field ensembles","DOI":"10.1109/VAST.2015.7347634","Link":"http://dx.doi.org/10.1109/VAST.2015.7347634","FirstPage":81,"LastPage":88,"PaperType":"C","Abstract":"We present a new visual analysis approach to support the comparative exploration of 2D vector-valued ensemble fields. Our approach enables the user to quickly identify the most similar groups of ensemble members, as well as the locations where the variation among the members is high. We further provide means to visualize the main features of the potentially multimodal directional distributions at user-selected locations. For this purpose, directional data is modelled using mixtures of probability density functions (pdfs), which allows us to characterize and classify complex distributions with relatively few parameters. The resulting mixture models are used to determine the degree of similarity between ensemble members, and to construct glyphs showing the direction, spread, and strength of the principal modes of the directional distributions. We also propose several similarity measures, based on which we compute pairwise member similarities in the spatial domain and form clusters of similar members. The hierarchical clustering is shown using dendrograms and similarity matrices, which can be used to select particular members and visualize their variations. A user interface providing multiple linked views enables the simultaneous visualization of aggregated global and detailed local variations, as well as the selection of members for a detailed comparison.","AuthorNames-Deduped":"Mihaela Jarema;Ismail Demir;Johannes Kehrer;Rüdiger Westermann","AuthorNames":"Mihaela Jarema;Ismail Demir;Johannes Kehrer;Rüdiger Westermann","AuthorAffiliation":"Technische Universität München, Germany;Technische Universität München, Germany;Technische Universität München, Germany;Technische Universität München, Germany","InternalReferences":"10.1109/TVCG.2014.2346626;10.1109/TVCG.2010.190;10.1109/VAST.2009.5332611;10.1109/TVCG.2006.160;10.1109/TVCG.2013.141;10.1109/TVCG.2013.177;10.1109/TVCG.2010.199;10.1109/TVCG.2014.2346321","AuthorKeywords":"Uncertainty Visualization, Vector Field Data, Coordinated and Multiple Views, Glyph-based Techniques","AminerCitationCount_02-2020":10,"AminerCitationCount_06-2020":20,"XploreCitationCount - 2020-01":14,"PubsCited":43,"Award":null,"image":"7347634-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Interactive visual steering of hierarchical simulation ensembles","DOI":"10.1109/VAST.2015.7347635","Link":"http://dx.doi.org/10.1109/VAST.2015.7347635","FirstPage":89,"LastPage":96,"PaperType":"C","Abstract":"Multi-level simulation models, i.e., models where different components are simulated using sub-models of varying levels of complexity, belong to the current state-of-the-art in simulation. The existing analysis practice for multi-level simulation results is to manually compare results from different levels of complexity, amounting to a very tedious and error-prone, trial-and-error exploration process. In this paper, we introduce hierarchical visual steering, a new approach to the exploration and design of complex systems. Hierarchical visual steering makes it possible to explore and analyze hierarchical simulation ensembles at different levels of complexity. At each level, we deal with a dynamic simulation ensemble - the ensemble grows during the exploration process. There is at least one such ensemble per simulation level, resulting in a collection of dynamic ensembles, analyzed simultaneously. The key challenge is to map the multi-dimensional parameter space of one ensemble to the multi-dimensional parameter space of another ensemble (from another level). In order to support the interactive visual analysis of such complex data we propose a novel approach to interactive and semi-automatic parameter space segmentation and comparison. The approach combines a novel interaction technique and automatic, computational methods - clustering, concave hull computation, and concave polygon overlapping - to support the analysts in the cross-ensemble parameter space mapping. In addition to the novel parameter space segmentation we also deploy coordinated multiple views with standard plots. We describe the abstract analysis tasks, identified during a case study, i.e., the design of a variable valve actuation system of a car engine. The study is conducted in cooperation with experts from the automotive industry. Very positive feedback indicates the usefulness and efficiency of the newly proposed approach.","AuthorNames-Deduped":"Rainer Splechtna;Kresimir Matkovic;Denis Gracanin;Mario Jelovic;Helwig Hauser","AuthorNames":"Rainer Splechtna;Krešimir Matković;Denis Gračanin;Mario Jelović;Helwig Hauser","AuthorAffiliation":"VRVis Research Center in Vienna, Austria;VRVis Research Center in Vienna, Austria;Virginia Tech, Blacksburg, VA, USA;AVL-AST Zagreb, Croatia;University of Bergen, Norway","InternalReferences":"10.1109/TVCG.2008.145;10.1109/TVCG.2014.2346744;10.1109/TVCG.2014.2346321;10.1109/VAST.2009.5333081;10.1109/TVCG.2010.223","AuthorKeywords":"Interactive Visual Analysis, Simulation-Ensemble Steering, Multi-resolution simulation","AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":6,"XploreCitationCount - 2020-01":7,"PubsCited":20,"Award":null,"image":"7347635-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Urbane: A 3D framework to support data driven decision making in urban development","DOI":"10.1109/VAST.2015.7347636","Link":"http://dx.doi.org/10.1109/VAST.2015.7347636","FirstPage":97,"LastPage":104,"PaperType":"C","Abstract":"Architects working with developers and city planners typically rely on experience, precedent and data analyzed in isolation when making decisions that impact the character of a city. These decisions are critical in enabling vibrant, sustainable environments but must also negotiate a range of complex political and social forces. This requires those shaping the built environment to balance maximizing the value of a new development with its impact on the character of a neighborhood. As a result architects are focused on two issues throughout the decision making process: a) what defines the character of a neighborhood? and b) how will a new development change its neighborhood? In the first, character can be influenced by a variety of factors and understanding the interplay between diverse data sets is crucial; including safety, transportation access, school quality and access to entertainment. In the second, the impact of a new development is measured, for example, by how it impacts the view from the buildings that surround it. In this paper, we work in collaboration with architects to design Urbane, a 3-dimensional multi-resolution framework that enables a data-driven approach for decision making in the design of new urban development. This is accomplished by integrating multiple data layers and impact analysis techniques facilitating architects to explore and assess the effect of these attributes on the character and value of a neighborhood. Several of these data layers, as well as impact analysis, involve working in 3-dimensions and operating in real time. Efficient computation and visualization is accomplished through the use of techniques from computer graphics. We demonstrate the effectiveness of Urbane through a case study of development in Manhattan depicting how a data-driven understanding of the value and impact of speculative buildings can benefit the design-development process between architects, planners and developers.","AuthorNames-Deduped":"Nivan Ferreira;Marcos Lage;Harish Doraiswamy;Huy T. Vo;Luc Wilson;Heidi Werner;Muchan Park;Cláudio T. Silva","AuthorNames":"Nivan Ferreira;Marcos Lage;Harish Doraiswamy;Huy Vo;Luc Wilson;Heidi Werner;Muchan Park;Cláudio Silva","AuthorAffiliation":"New York University, USA;Universidade Federal Fluminense, Brazil;New York University, USA;New York University, USA;Kohn Pedersen Fox Associates PC, USA;Kohn Pedersen Fox Associates PC, USA;Kohn Pedersen Fox Associates PC, USA;New York University, USA","InternalReferences":"10.1109/VAST.2008.4677356;10.1109/TVCG.2014.2346446;10.1109/TVCG.2007.70574;10.1109/TVCG.2013.226;10.1109/TVCG.2007.70523;10.1109/TVCG.2013.228;10.1109/TVCG.2014.2346893;10.1109/TVCG.2014.2346898","AuthorKeywords":null,"AminerCitationCount_02-2020":10,"AminerCitationCount_06-2020":17,"XploreCitationCount - 2020-01":10,"PubsCited":39,"Award":null,"image":"7347636-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2015,"Title":"FeatureInsight: Visual support for error-driven feature ideation in text classification","DOI":"10.1109/VAST.2015.7347637","Link":"http://dx.doi.org/10.1109/VAST.2015.7347637","FirstPage":105,"LastPage":112,"PaperType":"C","Abstract":"Machine learning requires an effective combination of data, features, and algorithms. While many tools exist for working with machine learning data and algorithms, support for thinking of new features, or feature ideation, remains poor. In this paper, we investigate two general approaches to support feature ideation: visual summaries and sets of errors. We present FeatureInsight, an interactive visual analytics tool for building new dictionary features (semantically related groups of words) for text classification problems. FeatureInsight supports an error-driven feature ideation process and provides interactive visual summaries of sets of misclassified documents. We conducted a controlled experiment evaluating both visual summaries and sets of errors in FeatureInsight. Our results show that visual summaries significantly improve feature ideation, especially in combination with sets of errors. Users preferred visual summaries over viewing raw data, and only preferred examining sets when visual summaries were provided. We discuss extensions of both approaches to data types other than text, and point to areas for future research.","AuthorNames-Deduped":"Michael Brooks;Saleema Amershi;Bongshin Lee;Steven Mark Drucker;Ashish Kapoor;Patrice Y. Simard","AuthorNames":"Michael Brooks;Saleema Amershi;Bongshin Lee;Steven M. Drucker;Ashish Kapoor;Patrice Simard","AuthorAffiliation":"University of Washington, USA;Microsoft Research, USA;Microsoft Research, USA;Microsoft Research, USA;Microsoft Research, USA;Microsoft Research, USA","InternalReferences":"10.1109/VAST.2010.5652443","AuthorKeywords":null,"AminerCitationCount_02-2020":19,"AminerCitationCount_06-2020":39,"XploreCitationCount - 2020-01":20,"PubsCited":37,"Award":null,"image":"7347637-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2015,"Title":"Visual scalability of spatial ensemble uncertainty","DOI":"10.1109/VAST.2015.7347671","Link":"http://dx.doi.org/10.1109/VAST.2015.7347671","FirstPage":187,"LastPage":188,"PaperType":"M","Abstract":"Weather Research and Forecasting (WRF) models simulate weather conditions by generating 2D numerical weather prediction ensemble members either through perturbing initial conditions or by changing different parameterization schemes, e.g., cumulus and microphysics schemes. These simulations are often used by weather analysts to analyze the nature of uncertainty attributed by these simulations to forecast weather conditions with good accuracy. The number of simulations used for forecasting is growing with the advent of increase in computing power. Hence, there is a need for providing better visual insights of uncertainty with growing number of ensemble members. We propose a geo visual analytical framework that uses visual analytics approach to resolve visual scalability of these ensemble members. Our approach naturally fits with the workflow of an analyst analyzing ensemble spatial uncertainty. Meteorologists evaluated our framework qualitatively and found it to be effective in acquiring insights of spatial uncertainty associated with multiple ensemble runs that are simulated using multiple parameterization schemes.","AuthorNames-Deduped":"Sujan Anreddy;Song Zhang 0004;Andrew Mercer 0001;Jamie Dyer;J. Edward Swan","AuthorNames":"Sujan Anreddy;Song Zhang;Andrew Mercer;Jamie Dyer;J. Edward Swan","AuthorAffiliation":"Mississippi State University, USA;Mississippi State University, USA;Mississippi State University, USA;Mississippi State University, USA;Mississippi State University, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":6,"Award":null,"image":"7347671-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Visually and statistically guided imputation of missing values in univariate seasonal time series","DOI":"10.1109/VAST.2015.7347672","Link":"http://dx.doi.org/10.1109/VAST.2015.7347672","FirstPage":189,"LastPage":190,"PaperType":"M","Abstract":"Missing values are a problem in many real world applications, for example failing sensor measurements. For further analysis these missing values need to be imputed. Thus, imputation of such missing values is important in a wide range of applications. We propose a visually and statistically guided imputation approach, that allows applying different imputation techniques to estimate the missing values as well as evaluating and fine tuning the imputation by visual guidance. In our approach we include additional visual information about uncertainty and employ the cyclic structure of time inherent in the data. Including this cyclic structure enables visually judging the adequateness of the estimated values with respect to the uncertainty/error boundaries and according to the patterns of the neighbouring time points in linear and cyclic (e.g., the months of the year) time.","AuthorNames-Deduped":"Markus Bögl;Peter Filzmoser;Theresia Gschwandtner;Silvia Miksch;Wolfgang Aigner;Alexander Rind;Tim Lammarsch","AuthorNames":"M. Bögl;P. Filzmoser;T. Gschwandtner;S. Miksch;W. Aigner;A. Rind;T. Lammarsch","AuthorAffiliation":"Vienna University of Technology, Austria;Vienna University of Technology, Austria;Vienna University of Technology, Austria;Vienna University of Technology, Austria;St. Pölten University of Applied Sciences, Germany;St. Pölten University of Applied Sciences, Germany;MODUL University Vienna, Austria","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":3,"PubsCited":11,"Award":null,"image":"7347672-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"StreamVisND: Visualizing relationships in streaming multivariate data","DOI":"10.1109/VAST.2015.7347673","Link":"http://dx.doi.org/10.1109/VAST.2015.7347673","FirstPage":191,"LastPage":192,"PaperType":"M","Abstract":"In streaming acquisitions the data changes over time. ThemeRiver and line charts are common methods to display data over time. However, these methods can only show the values of the variables (or attributes) but not the relationships among them over time. We propose a framework we call StreamVis&lt;sup&gt;ND&lt;/sup&gt; that can display these types of streaming data relations. It first slices the data stream into different time slices, then it visualizes each slice with a sequence of multivariate 2D data layouts, and finally it flattens this series of displays into a parallel coordinate type display. Our framework is fully interactive and lends itself well to real-time displays.","AuthorNames-Deduped":"Shenghui Cheng;Yue Wang;Dan Zhang;Zhifang Jiang;Klaus Mueller","AuthorNames":"Shenghui Cheng;Yue Wang;Dan Zhang;Zhifang Jiang;Klaus Mueller","AuthorAffiliation":"Visual Analytics and Imaging Lab, Computer Science Department, Stony Brook University and SUNY Korea;Department of Computer Science, Shandong University, China;Visual Analytics and Imaging Lab, Computer Science Department, Stony Brook University and SUNY Korea;Department of Computer Science, Shandong University, China;Visual Analytics and Imaging Lab, Computer Science Department, Stony Brook University and SUNY Korea","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":0,"PubsCited":3,"Award":null,"image":"7347673-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2015,"Title":"A software developer's guide to informal evaluation of Visual Analytics environments using VAST Challenge information","DOI":"10.1109/VAST.2015.7347674","Link":"http://dx.doi.org/10.1109/VAST.2015.7347674","FirstPage":193,"LastPage":194,"PaperType":"M","Abstract":"The VAST Challenge has been a popular venue for academic and industry participants for over ten years. Many participants comment that the majority of their time in preparing VAST Challenge entries is discovering elements in their software environments that need to be redesigned in order to solve the given task. Fortunately, there is no need to wait until the VAST Challenge is announced to test out software systems. The Visual Analytics Benchmark Repository contains all past VAST Challenge tasks, data, solutions and submissions. In this poster we describe how developers can perform informal evaluations of various aspects of their visual analytics environments using VAST Challenge information.","AuthorNames-Deduped":"Kristin A. Cook;Jean Scholtz;Mark A. Whiting","AuthorNames":"Kristin A. Cook;Jean Scholtz;Mark A. Whiting","AuthorAffiliation":"Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA;Pacific Northwest National Laboratory, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":2,"Award":null,"image":""},{"Conference":"VAST","Year":2015,"Title":"HTMVS: Visualizing hierarchical topics and their evolution","DOI":"10.1109/VAST.2015.7347675","Link":"http://dx.doi.org/10.1109/VAST.2015.7347675","FirstPage":195,"LastPage":196,"PaperType":"M","Abstract":"Topic model has been an active research area for many years, it can be used for discovering latent semantics and finding hidden knowledge in unstructured data corpus. In this paper, we investigated the problems in visualizing hierarchical topic and their evolution. The contribution of this paper is threefold, first we explore the static visualization of hierarchical topics using the `nested circle' layout, and then in order to present the topic evolution over time, we extended a hierarchical topic model and employ topic transformation visualizations to track the arising, splitting and disappearing of certain topics under the dynamic topical hierarchy. Finally, a Hierarchical Topic Model Visualization System (HTMVS) is designed to take advantage of both static and dynamic hierarchical topic visualization.","AuthorNames-Deduped":"Haoling Dong;Siliang Tang;Si Li;Fei Wu 0001;Yueting Zhuang","AuthorNames":"Haoling Dong;Siliang Tang;Si Li;Fei Wu;Yueting Zhuang","AuthorAffiliation":"College of Computer Science, Zhejiang University, Hangzhou, China;College of Computer Science, Zhejiang University, Hangzhou, China;College of Computer Science, Zhejiang University, Hangzhou, China;College of Computer Science, Zhejiang University, Hangzhou, China;College of Computer Science, Zhejiang University, Hangzhou, China","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":6,"Award":null,"image":"7347675-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Interactive semi-automatic categorization for spinel group minerals","DOI":"10.1109/VAST.2015.7347676","Link":"http://dx.doi.org/10.1109/VAST.2015.7347676","FirstPage":197,"LastPage":198,"PaperType":"M","Abstract":"Spinel group minerals are excellent indicators of geological environments (tectonic settings). In 2001, Barnes and Roeder defined a set of contours corresponding to compositional fields for spinel group minerals. Geologists typically use this contours to estimate the tectonic environment where a particular spinel composition could have been formed. This task is prone to errors and requires tedious manual comparison of overlapping diagrams. We introduce a semi-automatic, interactive detection of tectonic settings for an arbitrary dataset based on the Barnes and Roeder contours. The new approach integrates the mentioned contours and includes a novel interaction called contour brush. The new methodology is integrated in the Spinel Explorer system and it improves the scientist's workflow significantly.","AuthorNames-Deduped":"Maria Luján Ganuza;Maria Florencia Gargiulo;Gabriela Ferracutti;Silvia Mabel Castro;Ernesto A. Bjerg;M. Eduard Gröller;Kresimir Matkovic","AuthorNames":"María Luján Ganuza;Florencia Gargiulo;Gabriela Ferracutti;Silvia Castro;Ernesto Bjerg;Eduard Gröller;Krešimir Matković","AuthorAffiliation":"VyGLab, UNS, USA;INGEOSUR CONICET, Argentina;INGEOSUR CONICET, Argentina;VyGLab, UNS, USA;INGEOSUR CONICET, Argentina;TU Wien, Austria;VRVis, Austria","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":5,"Award":null,"image":"7347676-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"A System for visual exploration of caution spots from vehicle recorder data","DOI":"10.1109/VAST.2015.7347677","Link":"http://dx.doi.org/10.1109/VAST.2015.7347677","FirstPage":199,"LastPage":200,"PaperType":"M","Abstract":"It is vital for the transportation industry, which performs most of its work by automobiles, to reduce its accident rate. This paper proposes a 3D visual interaction method for exploring caution areas from large-scale vehicle recorder data. Our method provides (i) a flexible filtering interface for driving operations such as braking or handling operations by various combinations of their attribute values such as velocity and acceleration, and (ii) a 3D visual environment for spatio-temporal exploration of caution areas. The proposed method was able to extract caution areas where some accidents have actually occurred or that are on very narrow roads with bad visibility by using real data given by one of the biggest transportation companies in Japan.","AuthorNames-Deduped":"Masahiko Itoh;Daisaku Yokoyama;Masashi Toyoda;Masaru Kitsuregawa","AuthorNames":"Masahiko Itoh;Daisaku Yokoyama;Masashi Toyoda;Masaru Kitsuregawa","AuthorAffiliation":"The University of Tokyo, and National Institute of Information and Communications Technology, Japan;The University of Tokyo, Japan;The University of Tokyo, Japan;National Institute of Informatics, and The University of Tokyo, Japan","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":4,"Award":null,"image":"7347677-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Visual Analytics for fraud detection and monitoring","DOI":"10.1109/VAST.2015.7347678","Link":"http://dx.doi.org/10.1109/VAST.2015.7347678","FirstPage":201,"LastPage":202,"PaperType":"M","Abstract":"One of the primary concerns of financial institutions is to guarantee security and legitimacy in their services. Being able to detect and avoid fraudulent schemes also enhances the credibility of these institutions. Currently, fraud detection approaches still lack Visual Analytics techniques. We propose a Visual Analytics process that tackles the main challenges in the area of fraud detection.","AuthorNames-Deduped":"Roger A. Leite;Theresia Gschwandtner;Silvia Miksch;Erich Gstrein;Johannes Kuntner","AuthorNames":"Roger A. Leite;Theresia Gschwandtner;Silvia Miksch;Erich Gstrein;Johannes Kuntner","AuthorAffiliation":"Vienna University of Technology, Austria;Vienna University of Technology, Austria;Vienna University of Technology, Austria;Erste Group IT, USA;Erste Group IT, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":3,"PubsCited":7,"Award":null,"image":"7347678-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Visual analysis of route choice behaviour based on GPS trajectories","DOI":"10.1109/VAST.2015.7347679","Link":"http://dx.doi.org/10.1109/VAST.2015.7347679","FirstPage":203,"LastPage":204,"PaperType":"M","Abstract":"There are often multiple routes between regions. Many factors potentially affect driver's route choice, such as expected time cost, length etc. In this work, we present a visual analysis system to explore driver's route choice behaviour based on taxi GPS trajectory data. With interactive trajectory filtering, the system constructs feasible routes between regions of interest. Using a rank-based visualization, the attributes of multiple routes are explored and compared. Based on a statistical model, the system supports to verify trajectory-related factors' impact on route choice behaviour. The effectiveness of the system is demonstrated by applying to real trajectory dataset.","AuthorNames-Deduped":"Min Lu;Chufan Lai;Tangzhi Ye;Jie Liang 0004;Xiaoru Yuan","AuthorNames":"Min Lu;Chufan Lai;Tangzhi Ye;Jie Liang;Xiaoru Yuan","AuthorAffiliation":"Key Laboratory of Machine Perception (Ministry of Education), and School of EECS, Peking University, China;Key Laboratory of Machine Perception (Ministry of Education), and School of EECS, Peking University, China;Key Laboratory of Machine Perception (Ministry of Education), and School of EECS, Peking University, China;Key Laboratory of Machine Perception (Ministry of Education), and School of EECS, Peking University, China;Key Laboratory of Machine Perception (Ministry of Education), and School of EECS, Peking University, China","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":1,"PubsCited":8,"Award":null,"image":"7347679-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Using visualization and analysis with efficient dimension Reduction to determine underlying factors in hospital inpatient procedure costs","DOI":"10.1109/VAST.2015.7347680","Link":"http://dx.doi.org/10.1109/VAST.2015.7347680","FirstPage":205,"LastPage":206,"PaperType":"M","Abstract":"The Centers for Medicare and Medicaid Services (CMS) has made public a data set showing what hospitals charged and what Medicare paid for the one hundred most common inpatient stays. Here we present the application of Reduced Basis Decomposition (RBD), an efficient novel dimension reduction algorithm for data processing, to the CMS data. This was paired with a comparative visual exploration of the results when put into context with characteristics of the hospitals and marketplaces in which they operate. We used Weave Analyst, a new web-based analysis and visualization environment, to visualize the relationship between the hospital groups, their charge levels, and distinguishing indicator variables. Particular insights to the relatively small number of underlying factors that exert greatest influence on hospital pricing surfaced thanks to the combined synergetic integration of the modeling, reduction, and visualization techniques.","AuthorNames-Deduped":"Miriam Perkins;Yanlai Chen","AuthorNames":"Miriam Perkins;Yanlai Chen","AuthorAffiliation":"University of Massachusetts Lowell, USA;University of Massachusetts Dartmouth, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":6,"Award":null,"image":"7347680-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Topicks: Visualizing complex topic models for user comprehension","DOI":"10.1109/VAST.2015.7347681","Link":"http://dx.doi.org/10.1109/VAST.2015.7347681","FirstPage":207,"LastPage":208,"PaperType":"M","Abstract":"The interactive visualization of topic models is a promising approach to summarizing large sets of textual data. Topicks is the working title for a means to visualize topic modelling outputs. Incorporating a radial layout, users can view the relationships between topics, terms and the corpus as a whole. Interacting with topic and term nodes, as well as a related bar chart, provides the user with various ways to manipulate the visualization and explore the data. We describe the visualization and potential user interactions before discussing future work.","AuthorNames-Deduped":"Jessica Peter;Steve James Szigeti;Ana Jofre;Sara Diamond","AuthorNames":"Jessica Peter;Steve Szigeti;Ana Jofre;Sara Diamond","AuthorAffiliation":"OCAD University, Canada;OCAD University, Canada;OCAD University, Canada;OCAD University, Canada","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":0,"PubsCited":6,"Award":null,"image":"7347681-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"TimeStitch: Interactive multi-focus cohort discovery and comparison","DOI":"10.1109/VAST.2015.7347682","Link":"http://dx.doi.org/10.1109/VAST.2015.7347682","FirstPage":209,"LastPage":210,"PaperType":"M","Abstract":"Whereas event-based timelines for healthcare enable users to visualize the chronology of events surrounding events of interest, they are often not designed to aid the discovery, construction, or comparison of associated cohorts. We present TimeStitch, a system that helps health researchers discover and understand events that may cause abstinent smokers to lapse. TimeStitch extracts common sequences of events performed by abstinent smokers from large amounts of mobile health sensor data, and offers a suite of interactive and visualization techniques to enable cohort discovery, construction, and comparison, using extracted sequences as interactive elements. We are extending TimeStitch to support more complex health conditions with high mortality risk, such as reducing hospital readmission in congestive heart failure.","AuthorNames-Deduped":"Peter J. Polack Jr.;Shang-Tse Chen;Minsuk Kahng;Moushumi Sharmin;Duen Horng Chau","AuthorNames":"Peter J. Polack;Shang-Tse Chen;Minsuk Kahng;Moushumi Sharmin;Duen Horng Chau","AuthorAffiliation":"Georgia Tech., USA;Georgia Tech., USA;Georgia Tech., USA;University of Memphis., USA;Georgia Tech., USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":7,"XploreCitationCount - 2020-01":7,"PubsCited":7,"Award":null,"image":"7347682-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Tell me what do you see: Detecting perceptually-separable visual patterns via clustering of image-space features in visualizations","DOI":"10.1109/VAST.2015.7347683","Link":"http://dx.doi.org/10.1109/VAST.2015.7347683","FirstPage":211,"LastPage":212,"PaperType":"M","Abstract":"Visualization helps users infer structures and relationships in the data by encoding information as visual features that can be processed by the human visual-perceptual system. However, users would typically need to expend significant effort to scan and analyze a large number of views before they can begin to recognize relationships in a visualization. We propose a technique to partially automate the process of analyzing visualizations. By deriving and analyzing image-space features from visualizations, we can detect perceptually-separable patterns in the information space. We summarize these patterns with a tree-based meta-visualization and present it to the user to aid exploration. We illustrate this technique with an example scenario involving the analysis of census data.","AuthorNames-Deduped":"Khairi Reda;Alberto Gonzalez;Jason Leigh;Michael E. Papka","AuthorNames":"Khairi Reda;Alberto González;Jason Leigh;Michael E. Papka","AuthorAffiliation":"Argonne National Laboratory, USA;University of Hawai'i at Mānoa, USA;University of Hawai'i at Mānoa, USA;Argonne National Laboratory, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":6,"Award":null,"image":"7347683-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Sequencing of categorical time series","DOI":"10.1109/VAST.2015.7347684","Link":"http://dx.doi.org/10.1109/VAST.2015.7347684","FirstPage":213,"LastPage":214,"PaperType":"M","Abstract":"Exploring and comparing categorical time series and finding temporal patterns are complex tasks in the field of time series data mining. Although different analysis approaches exist, these tasks remain challenging, especially when numerous time series are considered at once. We propose a visual analysis approach that supports exploring such data by ordering time series in meaningful ways. We provide interaction techniques to steer the automated arrangement and to allow users to investigate patterns in detail.","AuthorNames-Deduped":"Christian Richter;Martin Luboschik;Martin Röhlig;Heidrun Schumann","AuthorNames":"Christian Richter;Martin Luboschik;Martin Röhlig;Heidrun Schumann","AuthorAffiliation":"University of Rostock, Germany;University of Rostock, Germany;University of Rostock, Germany;University of Rostock, Germany","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":2,"PubsCited":5,"Award":null,"image":"7347684-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Visual Pruner: Visually guided cohort selection for observational studies","DOI":"10.1109/VAST.2015.7347685","Link":"http://dx.doi.org/10.1109/VAST.2015.7347685","FirstPage":215,"LastPage":216,"PaperType":"M","Abstract":"Observational studies are a widely used and challenging class of studies. A key challenge is selecting a study cohort from the available data, or “pruning” the data, in a way that produces both sufficient balance in pre-treatment covariates and an easily described cohort from which results can be generalized. Even with advanced pruning methods, it is often difficult for researchers to see how the cohort is being selected; consequently, these methods are underutilized in research. Visual Pruner is a free, easy-to-use web application that can improve both the credibility and generalizability of observational studies by letting analysts use updatable visual displays of estimated propensity scores and key baseline covariates to refine inclusion criteria. By helping researchers see how covariate distributions in their data relate to the estimated probabilities of treatment assignment, the app lets researchers make pruning decisions based on pre-treatment covariate patterns that are otherwise hard to discover. The app yields a set of inclusion criteria that can be used in conjunction with further statistical analysis in any statistical software.","AuthorNames-Deduped":"Lauren R. Samuels;Robert A. Greevy","AuthorNames":"Lauren R. Samuels;Robert A. Greevy","AuthorAffiliation":"Department of Biostatistics, Vanderbilt University School of Medicine, USA;Department of Biostatistics, Vanderbilt University School of Medicine, USA","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":10,"Award":null,"image":"7347685-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"uRank: Visual analytics approach for search result exploration","DOI":"10.1109/VAST.2015.7347686","Link":"http://dx.doi.org/10.1109/VAST.2015.7347686","FirstPage":217,"LastPage":218,"PaperType":"M","Abstract":"uRank is a Web-based tool combining lightweight text analytics and visual methods for topic-wise exploration of document sets. It includes a view summarizing the content of the document set in meaningful terms, a dynamic document ranking view and a detailed view for further inspection of individual documents. Its major strength lies in how it supports users in reorganizing documents on-the-fly as their information interests change. We present a preliminary evaluation showing that uRank helps to reduce cognitive load compared to a traditional list-based representation.","AuthorNames-Deduped":"Cecilia di Sciascio;Vedran Sabol;Eduardo E. Veas","AuthorNames":"Cecilia di Sciascio;Vedran Sabol;Eduardo Veas","AuthorAffiliation":"Know-Center GmbH, Graz, Austria;Know-Center GmbH, Graz, Austria;Know-Center GmbH, Graz, Austria","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":5,"Award":null,"image":"7347686-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Evolution inspector: Interactive visual analysis for evolutionary molecular design","DOI":"10.1109/VAST.2015.7347687","Link":"http://dx.doi.org/10.1109/VAST.2015.7347687","FirstPage":219,"LastPage":220,"PaperType":"M","Abstract":"De novo design is a computational-chemistry method, where a computer program utilizes an optimization method, in our case an evolutionary algorithm, to design compounds with desired chemical properties. The optimization is performed with respect to a quantity called fitness, defined by the chemists. We present a tool that connects interactive visual analysis and evolutionary algorithm-based molecular design. We employ linked views to communicate different aspects of the data: the statistical distribution of molecule fitness, connections between individual molecules during the evolution and 3D molecular structure. The application is already used by chemists to explore and analyze the results of their evolution experiments and has proved to be highly useful.","AuthorNames-Deduped":"Veronika Soltészová;Marco Foscato;Sondre H. Eliasson;Vidar R. Jensen","AuthorNames":"Veronika Solteszova;Marco Foscato;Sondre H. Eliasson;Vidar R. Jensen","AuthorAffiliation":"Christian Michelsen Research, Bergen, Norway;Department of Chemistry, University of Bergen, Norway;Department of Chemistry, University of Bergen, Norway;Department of Chemistry, University of Bergen, Norway","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":10,"Award":null,"image":"7347687-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Trending pool: Visual analytics for trending event compositions for time-series categorical log data","DOI":"10.1109/VAST.2015.7347688","Link":"http://dx.doi.org/10.1109/VAST.2015.7347688","FirstPage":221,"LastPage":222,"PaperType":"M","Abstract":"Although many visualization tools provide us plenty of ways to view the data, users can not easily find the trending events and their explanation from the data. In this work, we address the issue by leveraging the real music streaming log data as an example to better understand a million-scale dataset. Trending event explanation turns out to be challenging when it comes to categorical log data. Therefore, we propose to use a learning-based method with an interface design to uncover the trending event compositions for time-series categorical log data, which can be extend to other datasets, e.g., the hashtags in social media. First, we perform “trending pool” operation to save the memory and time cost. Second, we apply sparse coding to learn important trending candidate combination sets instead of traditional brute-force way or manual investigation for generating combinations. Besides the contributions above, we also observe some interesting user behaviors by exploring detected trending candidate combinations visually through our interface.","AuthorNames-Deduped":"Yi-Chih Tsai;Liang-Chi Hsieh;Wen-Feng Cheng;Yin-Hsi Kuo;Winston H. Hsu;Wen-Chin Chen","AuthorNames":"Yi-Chih Tsai;Liang-Chi Hsieh;Wen-Feng Cheng;Yin-Hsi Kuo;Winston Hsu;Wen-Chin Chen","AuthorAffiliation":"National Taiwan University, Taipei, Taiwan;National Taiwan University, Taipei, Taiwan;National Taiwan University, Taipei, Taiwan;National Taiwan University, Taipei, Taiwan;National Taiwan University, Taipei, Taiwan;National Taiwan University, Taipei, Taiwan","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":4,"Award":null,"image":"7347688-fig-1-source-large.gif"},{"Conference":"VAST","Year":2015,"Title":"Visual data quality analysis for taxi GPS data","DOI":"10.1109/VAST.2015.7347689","Link":"http://dx.doi.org/10.1109/VAST.2015.7347689","FirstPage":223,"LastPage":224,"PaperType":"M","Abstract":"We present a novel visual analysis method to systematically discover data quality problems in raw taxi GPS data. It combines semi-supervised active learning and interactive visual exploration. It helps analysts interactively discover unknown data quality problems, and automatically extract known problems. We report analysis results on Beijing taxi GPS data.","AuthorNames-Deduped":"Zuchao Wang;Xiaoru Yuan;Tangzhi Ye;Youfeng Hao;Siming Chen;Jie Liang 0004;Qiusheng Li;Haiyang Wang;Yadong Wu","AuthorNames":"Zuchao Wang;Xiaoru Yuan;Tangzhi Ye;Youfeng hao;Siming Chen;Jie Liangk;Qiusheng Li;Haiyang Wang;Yadong Wu","AuthorAffiliation":"Key Laboratory of Machine Perception (Ministry of Education), and School of EECS, Peking University, China;Key Laboratory of Machine Perception (Ministry of Education), and School of EECS, Peking University, China;Key Laboratory of Machine Perception (Ministry of Education), and School of EECS, Peking University, China;Key Laboratory of Machine Perception (Ministry of Education), and School of EECS, Peking University, China;Key Laboratory of Machine Perception (Ministry of Education), and School of EECS, Peking University, China;Key Laboratory of Machine Perception (Ministry of Education), and School of EECS, Peking University, China;School of Computer Science and Technology, Southwest University of Science and Technology, China;School of Computer Science and Technology, Southwest University of Science and Technology, China;School of Computer Science and Technology, Southwest University of Science and Technology, China","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":0,"PubsCited":4,"Award":null,"image":"7347689-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"A Visual Analytics Approach for Understanding Reasons behind Snowballing and Comeback in MOBA Games","DOI":"10.1109/TVCG.2016.2598415","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598415","FirstPage":211,"LastPage":220,"PaperType":"J","Abstract":"To design a successful Multiplayer Online Battle Arena (MOBA) game, the ratio of snowballing and comeback occurrences to all matches played must be maintained at a certain level to ensure its fairness and engagement. Although it is easy to identify these two types of occurrences, game developers often find it difficult to determine their causes and triggers with so many game design choices and game parameters involved. In addition, the huge amounts of MOBA game data are often heterogeneous, multi-dimensional and highly dynamic in terms of space and time, which poses special challenges for analysts. In this paper, we present a visual analytics system to help game designers find key events and game parameters resulting in snowballing or comeback occurrences in MOBA game data. We follow a user-centered design process developing the system with game analysts and testing with real data of a trial version MOBA game from NetEase Inc. We apply novel visualization techniques in conjunction with well-established ones to depict the evolution of players' positions, status and the occurrences of events. Our system can reveal players' strategies and performance throughout a single match and suggest patterns, e.g., specific player' actions and game events, that have led to the final occurrences. We further demonstrate a workflow of leveraging human analyzed patterns to improve the scalability and generality of match data analysis. Finally, we validate the usability of our system by proving the identified patterns are representative in snowballing or comeback matches in a one-month-long MOBA tournament dataset.","AuthorNames-Deduped":"Quan Li;Peng Xu;Yeukyin Chan;Yun Wang 0012;Zhipeng Wang;Huamin Qu;Xiaojuan Ma","AuthorNames":"Quan Li;Peng Xu;Yeuk Yin Chan;Yun Wang;Zhipeng Wang;Huamin Qu;Xiaojuan Ma","AuthorAffiliation":"Hong Kong University of Science and Technology;NetEase, Inc.;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;China Academy of Art;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology","InternalReferences":"10.1109/TVCG.2014.2346445;10.1109/VISUAL.2004.120;10.1109/VAST.2015.7347633;10.1109/VAST.2014.7042477;10.1109/VAST.2014.7042478;10.1109/TVCG.2013.192;10.1109/TVCG.2012.263","AuthorKeywords":"Game play data visualization;visual knowledge discovery;visual knowledge representation;and game reconstruction","AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":14,"XploreCitationCount - 2020-01":13,"PubsCited":41,"Award":null,"image":"23tvcg01-li-2598415-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"SemanticTraj: A New Approach to Interacting with Massive Taxi Trajectories","DOI":"10.1109/TVCG.2016.2598416","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598416","FirstPage":11,"LastPage":20,"PaperType":"J","Abstract":"Massive taxi trajectory data is exploited for knowledge discovery in transportation and urban planning. Existing tools typically require users to select and brush geospatial regions on a map when retrieving and exploring taxi trajectories and passenger trips. To answer seemingly simple questions such as “What were the taxi trips starting from Main Street and ending at Wall Street in the morning?” or “Where are the taxis arriving at the Art Museum at noon typically coming from?”, tedious and time consuming interactions are usually needed since the numeric GPS points of trajectories are not directly linked to the keywords such as “Main Street”, “Wall Street”, and “Art Museum”. In this paper, we present SemanticTraj, a new method for managing and visualizing taxi trajectory data in an intuitive, semantic rich, and efficient means. With SemanticTraj, domain and public users can find answers to the aforementioned questions easily through direct queries based on the terms. They can also interactively explore the retrieved data in visualizations enhanced by semantic information of the trajectories and trips. In particular, taxi trajectories are converted into taxi documents through a textualization transformation process. This process maps GPS points into a series of street/POI names and pick-up/drop-off locations. It also converts vehicle speeds into user-defined descriptive terms. Then, a corpus of taxi documents is formed and indexed to enable flexible semantic queries over a text search engine. Semantic labels and meta-summaries of the results are integrated with a set of visualizations in a SemanticTraj prototype, which helps users study taxi trajectories quickly and easily. A set of usage scenarios are presented to show the usability of the system. We also collected feedback from domain experts and conducted a preliminary user study to evaluate the visual system.","AuthorNames-Deduped":"Shamal Al-Dohuki;Yingyu Wu;Farah Kamw;Jing Yang;Xin Li;Ye Zhao;Xinyue Ye;Wei Chen 0001;Chao Ma;Fei Wang 0016","AuthorNames":"Shamal Al-Dohuki;Yingyu Wu;Farah Kamw;Jing Yang;Xin Li;Ye Zhao;Xinyue Ye;Wei Chen;Chao Ma;Fei Wang","AuthorAffiliation":"Kent State University;Kent State University;Kent State University;UNC Charlotte;China Petroleum University;Kent State University;Kent State University;Zhejiang University;Kent State University;Zhejiang University","InternalReferences":"10.1109/TVCG.2015.2467732;10.1109/TVCG.2013.226;10.1109/VAST.2014.7042486;10.1109/VAST.2011.6102455;10.1109/TVCG.2014.2346746;10.1109/TVCG.2013.228;10.1109/VAST.2010.5652885","AuthorKeywords":"Taxi Trajectories;Taxi Document;Textualization;Name Query;Semantic Interaction;Text Search Engine","AminerCitationCount_02-2020":17,"AminerCitationCount_06-2020":38,"XploreCitationCount - 2020-01":43,"PubsCited":44,"Award":null,"image":"23tvcg01-zhao-2598416-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"SmartAdP: Visual Analytics of Large-scale Taxi Trajectories for Selecting Billboard Locations","DOI":"10.1109/TVCG.2016.2598432","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598432","FirstPage":1,"LastPage":10,"PaperType":"J","Abstract":"The problem of formulating solutions immediately and comparing them rapidly for billboard placements has plagued advertising planners for a long time, owing to the lack of efficient tools for in-depth analyses to make informed decisions. In this study, we attempt to employ visual analytics that combines the state-of-the-art mining and visualization techniques to tackle this problem using large-scale GPS trajectory data. In particular, we present SmartAdP, an interactive visual analytics system that deals with the two major challenges including finding good solutions in a huge solution space and comparing the solutions in a visual and intuitive manner. An interactive framework that integrates a novel visualization-driven data mining model enables advertising planners to effectively and efficiently formulate good candidate solutions. In addition, we propose a set of coupled visualizations: a solution view with metaphor-based glyphs to visualize the correlation between different solutions; a location view to display billboard locations in a compact manner; and a ranking view to present multi-typed rankings of the solutions. This system has been demonstrated using case studies with a real-world dataset and domain-expert interviews. Our approach can be adapted for other location selection problems such as selecting locations of retail stores or restaurants using trajectory data.","AuthorNames-Deduped":"Dongyu Liu;Di Weng;Yuhong Li;Jie Bao 0003;Yu Zheng 0004;Huamin Qu;Yingcai Wu","AuthorNames":"Dongyu Liu;Di Weng;Yuhong Li;Jie Bao;Yu Zheng;Huamin Qu;Yingcai Wu","AuthorAffiliation":"Zhejiang UniversityHong Kong University of Science and Technology;State Key Lab of CAD & CGZhejiang University;University of Macau;Microsoft Research, Beijing, China;Microsoft Research, Beijing, China;Hong Kong University of Science and Technology;State Key Lab of CAD & CGZhejiang University","InternalReferences":"10.1109/TVCG.2013.122;10.1109/TVCG.2015.2467051;10.1109/TVCG.2013.226;10.1109/VAST.2011.6102455;10.1109/TVCG.2013.228;10.1109/TVCG.2015.2467112;10.1109/TVCG.2012.265;10.1109/TVCG.2010.183;10.1109/TVCG.2014.2346912;10.1109/TVCG.2007.70521;10.1109/TVCG.2015.2467771;10.1109/TVCG.2013.173;10.1109/TVCG.2011.181;10.1109/TVCG.2009.111","AuthorKeywords":"optimal billboard locations;taxi trajectory;visual analytics;comparative analysis","AminerCitationCount_02-2020":30,"AminerCitationCount_06-2020":58,"XploreCitationCount - 2020-01":53,"PubsCited":51,"Award":null,"image":"23tvcg01-wu-2598432-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"Visual Analysis of MOOC Forums with iForum","DOI":"10.1109/TVCG.2016.2598444","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598444","FirstPage":201,"LastPage":210,"PaperType":"J","Abstract":"Discussion forums of Massive Open Online Courses (MOOC) provide great opportunities for students to interact with instructional staff as well as other students. Exploration of MOOC forum data can offer valuable insights for these staff to enhance the course and prepare the next release. However, it is challenging due to the large, complicated, and heterogeneous nature of relevant datasets, which contain multiple dynamically interacting objects such as users, posts, and threads, each one including multiple attributes. In this paper, we present a design study for developing an interactive visual analytics system, called iForum, that allows for effectively discovering and understanding temporal patterns in MOOC forums. The design study was conducted with three domain experts in an iterative manner over one year, including a MOOC instructor and two official teaching assistants. iForum offers a set of novel visualization designs for presenting the three interleaving aspects of MOOC forums (i.e., posts, users, and threads) at three different scales. To demonstrate the effectiveness and usefulness of iForum, we describe a case study involving field experts, in which they use iForum to investigate real MOOC forum data for a course on JAVA programming.","AuthorNames-Deduped":"Siwei Fu;Jian Zhao 0010;Weiwei Cui;Huamin Qu","AuthorNames":"Siwei Fu;Jian Zhao;Weiwei Cui;Huamin Qu","AuthorAffiliation":"Hong Kong University of Science and Technology;Autodesk Research;Microsoft Research;Hong Kong University of Science and Technology","InternalReferences":"10.1109/TVCG.2006.147;10.1109/TVCG.2007.70535;10.1109/INFVIS.2003.1249028;10.1109/TVCG.2015.2467555","AuthorKeywords":"Discussion forum;MOOC;temporal visualization;visual analytics","AminerCitationCount_02-2020":12,"AminerCitationCount_06-2020":20,"XploreCitationCount - 2020-01":19,"PubsCited":40,"Award":null,"image":"23tvcg01-fu-2598444-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"TopicLens: Efficient Multi-Level Visual Topic Exploration of Large-Scale Document Collections","DOI":"10.1109/TVCG.2016.2598445","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598445","FirstPage":151,"LastPage":160,"PaperType":"J","Abstract":"Topic modeling, which reveals underlying topics of a document corpus, has been actively adopted in visual analytics for large-scale document collections. However, due to its significant processing time and non-interactive nature, topic modeling has so far not been tightly integrated into a visual analytics workflow. Instead, most such systems are limited to utilizing a fixed, initial set of topics. Motivated by this gap in the literature, we propose a novel interaction technique called TopicLens that allows a user to dynamically explore data through a lens interface where topic modeling and the corresponding 2D embedding are efficiently computed on the fly. To support this interaction in real time while maintaining view consistency, we propose a novel efficient topic modeling method and a semi-supervised 2D embedding algorithm. Our work is based on improving state-of-the-art methods such as nonnegative matrix factorization and t-distributed stochastic neighbor embedding. Furthermore, we have built a web-based visual analytics system integrated with TopicLens. We use this system to measure the performance and the visualization quality of our proposed methods. We provide several scenarios showcasing the capability of TopicLens using real-world datasets.","AuthorNames-Deduped":"Minjeong Kim;Kyeongpil Kang;Deok Gun Park 0001;Jaegul Choo;Niklas Elmqvist","AuthorNames":"Minjeong Kim;Kyeongpil Kang;Deokgun Park;Jaegul Choo;Niklas Elmqvist","AuthorAffiliation":"Korea University;Korea University;University of Maryland, College Park, MD, USA;Korea University;University of Maryland, College Park, MD, USA","InternalReferences":"10.1109/INFVIS.2003.1249014;10.1109/TVCG.2013.212;10.1109/INFVIS.2003.1249008;10.1109/INFVIS.2004.43;10.1109/TVCG.2014.2346574;10.1109/TVCG.2011.239;10.1109/TVCG.2010.154;10.1109/VAST.2014.7042494","AuthorKeywords":"topic modeling;nonnegative matrix factorization;t-distributed stochastic neighbor embedding;magic lens;text analytics","AminerCitationCount_02-2020":10,"AminerCitationCount_06-2020":26,"XploreCitationCount - 2020-01":21,"PubsCited":51,"Award":null,"image":"23tvcg01-kim-2598445-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"AxiSketcher: Interactive Nonlinear Axis Mapping of Visualizations through User Drawings","DOI":"10.1109/TVCG.2016.2598446","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598446","FirstPage":221,"LastPage":230,"PaperType":"J","Abstract":"Visual analytics techniques help users explore high-dimensional data. However, it is often challenging for users to express their domain knowledge in order to steer the underlying data model, especially when they have little attribute-level knowledge. Furthermore, users' complex, high-level domain knowledge, compared to low-level attributes, posits even greater challenges. To overcome these challenges, we introduce a technique to interpret a user's drawings with an interactive, nonlinear axis mapping approach called AxiSketcher. This technique enables users to impose their domain knowledge on a visualization by allowing interaction with data entries rather than with data attributes. The proposed interaction is performed through directly sketching lines over the visualization. Using this technique, users can draw lines over selected data points, and the system forms the axes that represent a nonlinear, weighted combination of multidimensional attributes. In this paper, we describe our techniques in three areas: 1) the design space of sketching methods for eliciting users' nonlinear domain knowledge; 2) the underlying model that translates users' input, extracts patterns behind the selected data points, and results in nonlinear axes reflecting users' complex intent; and 3) the interactive visualization for viewing, assessing, and reconstructing the newly formed, nonlinear axes.","AuthorNames-Deduped":"Bum Chul Kwon;Hannah Kim;Emily Wall;Jaegul Choo;Haesun Park;Alex Endert","AuthorNames":"Bum Chul Kwon;Hannah Kim;Emily Wall;Jaegul Choo;Haesun Park;Alex Endert","AuthorAffiliation":"IBM T.J. Watson Research Center, Yorktown Heights, NY, USA;Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;Korea University, Seoul, South Korea;Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA","InternalReferences":"10.1109/INFVIS.2004.60;10.1109/TVCG.2013.190;10.1109/TVCG.2015.2467615;10.1109/TVCG.2013.188;10.1109/TVCG.2014.2346481;10.1109/VAST.2011.6102449;10.1109/TVCG.2012.262;10.1109/TVCG.2015.2467591;10.1109/VAST.2010.5652443;10.1109/TVCG.2011.261;10.1109/TVCG.2013.191;10.1109/TVCG.2013.212;10.1109/TVCG.2013.167;10.1109/VAST.2012.6400486","AuthorKeywords":"axis mapping;interactive model steering;sketch;axis visualization;human-centered visual analytics","AminerCitationCount_02-2020":12,"AminerCitationCount_06-2020":23,"XploreCitationCount - 2020-01":18,"PubsCited":54,"Award":null,"image":"23tvcg01-kwon-2598446-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"TextTile: An Interactive Visualization Tool for Seamless Exploratory Analysis of Structured Data and Unstructured Text","DOI":"10.1109/TVCG.2016.2598447","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598447","FirstPage":161,"LastPage":170,"PaperType":"J","Abstract":"We describe TextTile, a data visualization tool for investigation of datasets and questions that require seamless and flexible analysis of structured data and unstructured text. TextTile is based on real-world data analysis problems gathered through our interaction with a number of domain experts and provides a general purpose solution to such problems. The system integrates a set of operations that can interchangeably be applied to the structured as well as to unstructured text part of the data to generate useful data summaries. Such summaries are then organized in visual tiles in a grid layout to allow their analysis and comparison. We validate TextTile with task analysis, use cases and a user study showing the system can be easily learned and proficiently used to carry out nontrivial tasks.","AuthorNames-Deduped":"Cristian Felix;Anshul Vikram Pandey;Enrico Bertini","AuthorNames":"Cristian Felix;Anshul Vikram Pandey;Enrico Bertini","AuthorAffiliation":"New York University;New York University;New York University","InternalReferences":"10.1109/TVCG.2011.176;10.1109/INFVIS.2000.885098;10.1109/VAST.2012.6400485;10.1109/VAST.2009.5333443;10.1109/TVCG.2015.2467191;10.1109/TVCG.2009.128;10.1109/INFVIS.2000.885086;10.1109/TVCG.2010.183;10.1109/TVCG.2014.2346919","AuthorKeywords":"Exploratory Text Analysis;Knowledge Discovery;Text Visualization","AminerCitationCount_02-2020":8,"AminerCitationCount_06-2020":13,"XploreCitationCount - 2020-01":9,"PubsCited":43,"Award":null,"image":"23tvcg01-felix-2598447-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"Toward Theoretical Techniques for Measuring the Use of Human Effort in Visual Analytic Systems","DOI":"10.1109/TVCG.2016.2598460","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598460","FirstPage":121,"LastPage":130,"PaperType":"J","Abstract":"Visual analytic systems have long relied on user studies and standard datasets to demonstrate advances to the state of the art, as well as to illustrate the efficiency of solutions to domain-specific challenges. This approach has enabled some important comparisons between systems, but unfortunately the narrow scope required to facilitate these comparisons has prevented many of these lessons from being generalized to new areas. At the same time, advanced visual analytic systems have made increasing use of human-machine collaboration to solve problems not tractable by machine computation alone. To continue to make progress in modeling user tasks in these hybrid visual analytic systems, we must strive to gain insight into what makes certain tasks more complex than others. This will require the development of mechanisms for describing the balance to be struck between machine and human strengths with respect to analytical tasks and workload. In this paper, we argue for the necessity of theoretical tools for reasoning about such balance in visual analytic systems and demonstrate the utility of the Human Oracle Model for this purpose in the context of sensemaking in visual analytics. Additionally, we make use of the Human Oracle Model to guide the development of a new system through a case study in the domain of cybersecurity.","AuthorNames-Deduped":"R. Jordan Crouser;Lyndsey Franklin;Alex Endert;Kristin A. Cook","AuthorNames":"R. Jordan Crouser;Lyndsey Franklin;Alex Endert;Kris Cook","AuthorAffiliation":"Smith College;Smith College;Smith College;Smith College","InternalReferences":"10.1109/VAST.2011.6102467;10.1109/VAST.2010.5652910;10.1109/VAST.2011.6102438;10.1109/TVCG.2012.195;10.1109/VAST.2015.7347625;10.1109/VAST.2007.4389009;10.1109/VAST.2011.6102449;10.1109/VAST.2012.6400486","AuthorKeywords":"Theoretical models;human oracle;visual analytics;mixed initiative systems;semantic interaction;sensemaking","AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":8,"XploreCitationCount - 2020-01":7,"PubsCited":87,"Award":null,"image":"23tvcg01-crouser-2598460-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"AnaFe: Visual Analytics of Image-derived Temporal Features Focusing on the Spleen","DOI":"10.1109/TVCG.2016.2598463","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598463","FirstPage":171,"LastPage":180,"PaperType":"J","Abstract":"We present a novel visualization framework, AnaFe, targeted at observing changes in the spleen over time through multiple image-derived features. Accurate monitoring of progressive changes is crucial for diseases that result in enlargement of the organ. Our system is comprised of multiple linked views combining visualization of temporal 3D organ data, related measurements, and features. Thus it enables the observation of progression and allows for simultaneous comparison within and between the subjects. AnaFe offers insights into the overall distribution of robustly extracted and reproducible quantitative imaging features and their changes within the population, and also enables detailed analysis of individual cases. It performs similarity comparison of temporal series of one subject to all other series in both sick and healthy groups. We demonstrate our system through two use case scenarios on a population of 189 spleen datasets from 68 subjects with various conditions observed over time.","AuthorNames-Deduped":"Ievgeniia Gutenko;Konstantin Dmitriev;Arie E. Kaufman;Matthew Barish","AuthorNames":"Ievgeniia Gutenko;Konstantin Dmitriev;Arie E. Kaufman;Matthew A. Barish","AuthorAffiliation":"Computer Science Department, Stony Brook University, NY;Computer Science Department, Stony Brook University, NY;Computer Science Department, Stony Brook University, NY;Computer Science Department, Stony Brook University, NY","InternalReferences":"10.1109/TVCG.2014.2346591;10.1109/TVCG.2009.152;10.1109/TVCG.2015.2467622;10.1109/TVCG.2014.2346682;10.1109/TVCG.2012.225;10.1109/VISUAL.2000.885739;10.1109/TVCG.2013.200;10.1109/VAST.2006.261421","AuthorKeywords":"Visual Knowledge Discovery;Temporal Feature Analysis;Radiomics;Spleen;Abdominal Imaging","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":48,"Award":null,"image":"23tvcg01-gutenko-2598463-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"NameClarifier: A Visual Analytics System for Author Name Disambiguation","DOI":"10.1109/TVCG.2016.2598465","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598465","FirstPage":141,"LastPage":150,"PaperType":"J","Abstract":"In this paper, we present a novel visual analytics system called NameClarifier to interactively disambiguate author names in publications by keeping humans in the loop. Specifically, NameClarifier quantifies and visualizes the similarities between ambiguous names and those that have been confirmed in digital libraries. The similarities are calculated using three key factors, namely, co-authorships, publication venues, and temporal information. Our system estimates all possible allocations, and then provides visual cues to users to help them validate every ambiguous case. By looping users in the disambiguation process, our system can achieve more reliable results than general data mining models for highly ambiguous cases. In addition, once an ambiguous case is resolved, the result is instantly added back to our system and serves as additional cues for all the remaining unidentified names. In this way, we open up the black box in traditional disambiguation processes, and help intuitively and comprehensively explain why the corresponding classifications should hold. We conducted two use cases and an expert review to demonstrate the effectiveness of NameClarifier.","AuthorNames-Deduped":"Qiaomu Shen;Tongshuang Wu;Haiyan Yang;Yanhong Wu;Huamin Qu;Weiwei Cui","AuthorNames":"Qiaomu Shen;Tongshuang Wu;Haiyan Yang;Yanhong Wu;Huamin Qu;Weiwei Cui","AuthorAffiliation":"Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Microsoft Research","InternalReferences":"10.1109/TVCG.2012.252;10.1109/TVCG.2011.188;10.1109/VAST.2006.261429","AuthorKeywords":"Name disambiguation;analytical reasoning","AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":9,"XploreCitationCount - 2020-01":10,"PubsCited":41,"Award":null,"image":"23tvcg01-shen-2598465-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"Visualizing Dimension Coverage to Support Exploratory Analysis","DOI":"10.1109/TVCG.2016.2598466","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598466","FirstPage":21,"LastPage":30,"PaperType":"J","Abstract":"Data analysis involves constantly formulating and testing new hypotheses and questions about data. When dealing with a new dataset, especially one with many dimensions, it can be cumbersome for the analyst to clearly remember which aspects of the data have been investigated (i.e., visually examined for patterns, trends, outliers etc.) and which combinations have not. Yet this information is critical to help the analyst formulate new questions that they have not already answered. We observe that for tabular data, questions are typically comprised of varying combinations of data dimensions (e.g., what are the trends of Sales and Profit for different Regions?). We propose representing analysis history from the angle of dimension coverage (i.e., which data dimensions have been investigated and in which combinations). We use scented widgets to incorporate dimension coverage of the analysts' past work into interaction widgets of a visualization tool. We demonstrate how this approach can assist analysts with the question formation process. Our approach extends the concept of scented widgets to reveal aspects of one's own analysis history, and offers a different perspective on one's past work than typical visualization history tools. Results of our empirical study showed that participants with access to embedded dimension coverage information relied on this information when formulating questions, asked more questions about the data, generated more top-level findings, and showed greater breadth of their analysis without sacrificing depth.","AuthorNames-Deduped":"Ali Sarvghad;Melanie Tory;Narges Mahyar","AuthorNames":"Ali Sarvghad;Melanie Tory;Narges Mahyar","AuthorAffiliation":"University of Victoria;Tableau Research;University of British Columbia","InternalReferences":"10.1109/TVCG.2015.2467191;10.1109/TVCG.2006.120;10.1109/INFVIS.1999.801862;10.1109/INFVIS.2000.885086;10.1109/VISUAL.2005.1532788;10.1109/TVCG.2014.2346452;10.1109/VAST.2009.5333020;10.1109/INFVIS.2001.963289;10.1109/TVCG.2008.137;10.1109/TVCG.2015.2467611;10.1109/VISUAL.1993.398857;10.1109/TVCG.2007.70589;10.1109/TVCG.2013.167;10.1109/TVCG.2008.109","AuthorKeywords":"Dimension coverage;Tabular data;History;Empirical laboratory study;Exploratory data analysis;Scented widgets","AminerCitationCount_02-2020":8,"AminerCitationCount_06-2020":16,"XploreCitationCount - 2020-01":13,"PubsCited":33,"Award":null,"image":"23tvcg01-sarvghad-2598466-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"Magnostics: Image-Based Search of Interesting Matrix Views for Guided Network Exploration","DOI":"10.1109/TVCG.2016.2598467","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598467","FirstPage":31,"LastPage":40,"PaperType":"J","Abstract":"In this work we address the problem of retrieving potentially interesting matrix views to support the exploration of networks. We introduce Matrix Diagnostics (or Magnostics), following in spirit related approaches for rating and ranking other visualization techniques, such as Scagnostics for scatter plots. Our approach ranks matrix views according to the appearance of specific visual patterns, such as blocks and lines, indicating the existence of topological motifs in the data, such as clusters, bi-graphs, or central nodes. Magnostics can be used to analyze, query, or search for visually similar matrices in large collections, or to assess the quality of matrix reordering algorithms. While many feature descriptors for image analyzes exist, there is no evidence how they perform for detecting patterns in matrices. In order to make an informed choice of feature descriptors for matrix diagnostics, we evaluate 30 feature descriptors-27 existing ones and three new descriptors that we designed specifically for MAGNOSTICS-with respect to four criteria: pattern response, pattern variability, pattern sensibility, and pattern discrimination. We conclude with an informed set of six descriptors as most appropriate for Magnostics and demonstrate their application in two scenarios; exploring a large collection of matrices and analyzing temporal networks.","AuthorNames-Deduped":"Michael Behrisch 0001;Benjamin Bach;Michael Blumenschein;Michael Delz;Laura von Rüden;Jean-Daniel Fekete;Tobias Schreck","AuthorNames":"Michael Behrisch;Benjamin Bach;Michael Hund;Michael Delz;Laura Von Rüden;Jean-Daniel Fekete;Tobias Schreck","AuthorAffiliation":"University of Konstanz, Germany;Microsoft Research-Inria Joint Centre, Saclay, France;University of Konstanz, Germany;University of Konstanz, Germany;Capgemini, RWTH Aachen University;Inria, Saclay, France;Graz University of Technology, Austria","InternalReferences":"10.1109/VAST.2012.6400488;10.1109/INFVIS.2004.15;10.1109/VAST.2014.7042480;10.1109/VAST.2010.5652433;10.1109/TVCG.2010.184;10.1109/VAST.2006.261423;10.1109/TVCG.2007.70582;10.1109/TVCG.2007.70535;10.1109/TVCG.2011.229;10.1109/VAST.2010.5652392;10.1109/INFVIS.2005.1532142;10.1109/INFVIS.2004.3","AuthorKeywords":"Matrix Visualization;Visual Quality Measures;Quality Metrics;Feature Detection/Selection;Relational Data","AminerCitationCount_02-2020":6,"AminerCitationCount_06-2020":13,"XploreCitationCount - 2020-01":11,"PubsCited":49,"Award":null,"image":"23tvcg01-behrisch-2598467-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"Characterizing Guidance in Visual Analytics","DOI":"10.1109/TVCG.2016.2598468","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598468","FirstPage":111,"LastPage":120,"PaperType":"J","Abstract":"Visual analytics (VA) is typically applied in scenarios where complex data has to be analyzed. Unfortunately, there is a natural correlation between the complexity of the data and the complexity of the tools to study them. An adverse effect of complicated tools is that analytical goals are more difficult to reach. Therefore, it makes sense to consider methods that guide or assist users in the visual analysis process. Several such methods already exist in the literature, yet we are lacking a general model that facilitates in-depth reasoning about guidance. We establish such a model by extending van Wijk's model of visualization with the fundamental components of guidance. Guidance is defined as a process that gradually narrows the gap that hinders effective continuation of the data analysis. We describe diverse inputs based on which guidance can be generated and discuss different degrees of guidance and means to incorporate guidance into VA tools. We use existing guidance approaches from the literature to illustrate the various aspects of our model. As a conclusion, we identify research challenges and suggest directions for future studies. With our work we take a necessary step to pave the way to a systematic development of guidance techniques that effectively support users in the context of VA.","AuthorNames-Deduped":"Davide Ceneda;Theresia Gschwandtner;Thorsten May;Silvia Miksch;Hans-Jörg Schulz;Marc Streit;Christian Tominski","AuthorNames":"Davide Ceneda;Theresia Gschwandtner;Thorsten May;Silvia Miksch;Hans-Jörg Schulz;Marc Streit;Christian Tominski","AuthorAffiliation":"Vienna University of Technology, Austria;Vienna University of Technology, Austria;Fraunhofer IGD, Darmstadt, Germany;Vienna University of Technology, Austria;University of Rostock, Germany;Johannes Kepler University, Linz, Austria;University of Rostock, Germany","InternalReferences":"10.1109/VISUAL.2000.885678;10.1109/TVCG.2015.2467191;10.1109/VISUAL.1990.146375;10.1109/TVCG.2014.2346260;10.1109/TVCG.2014.2346481;10.1109/INFVIS.2004.2;10.1109/TVCG.2013.120;10.1109/VISUAL.1997.663889;10.1109/TVCG.2015.2467691;10.1109/VISUAL.2002.1183803;10.1109/TVCG.2007.70589;10.1109/TVCG.2008.174;10.1109/TVCG.2014.2346482","AuthorKeywords":"Visual analytics;guidance model;assistance;user support","AminerCitationCount_02-2020":14,"AminerCitationCount_06-2020":37,"XploreCitationCount - 2020-01":37,"PubsCited":55,"Award":null,"image":"23tvcg01-ceneda-2598468-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"PhenoStacks: Cross-Sectional Cohort Phenotype Comparison Visualizations","DOI":"10.1109/TVCG.2016.2598469","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598469","FirstPage":191,"LastPage":200,"PaperType":"J","Abstract":"Cross-sectional phenotype studies are used by genetics researchers to better understand how phenotypes vary across patients with genetic diseases, both within and between cohorts. Analyses within cohorts identify patterns between phenotypes and patients (e.g., co-occurrence) and isolate special cases (e.g., potential outliers). Comparing the variation of phenotypes between two cohorts can help distinguish how different factors affect disease manifestation (e.g., causal genes, age of onset, etc.). PhenoStacks is a novel visual analytics tool that supports the exploration of phenotype variation within and between cross-sectional patient cohorts. By leveraging the semantic hierarchy of the Human Phenotype Ontology, phenotypes are presented in context, can be grouped and clustered, and are summarized via overviews to support the exploration of phenotype distributions. The design of PhenoStacks was motivated by formative interviews with genetics researchers: we distil high-level tasks, present an algorithm for simplifying ontology topologies for visualization, and report the results of a deployment evaluation with four expert genetics researchers. The results suggest that PhenoStacks can help identify phenotype patterns, investigate data quality issues, and inform data collection design.","AuthorNames-Deduped":"Michael Glueck;Alina Gvozdik;Fanny Chevalier;Azam Khan;Michael Brudno;Daniel J. Wigdor","AuthorNames":"Michael Glueck;Alina Gvozdik;Fanny Chevalier;Azam Khan;Michael Brudno;Daniel Wigdor","AuthorAffiliation":"Autodesk ResearchUniversity of Toronto;University of Toronto;Inria;Autodesk Research;Hospital for Sick Children, University of Toronto, Toronto;University of Toronto","InternalReferences":"10.1109/TVCG.2014.2346248;10.1109/TVCG.2011.185;10.1109/TVCG.2014.2346279;10.1109/TVCG.2009.167;10.1109/TVCG.2013.124;10.1109/TVCG.2015.2467622;10.1109/TVCG.2015.2467733;10.1109/TVCG.2009.116","AuthorKeywords":"Cross-sectional cohort analysis;Phenotypes;Human Phenotype Ontology (HPO)","AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":11,"XploreCitationCount - 2020-01":9,"PubsCited":45,"Award":null,"image":"23tvcg01-glueck-2598469-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"Designing Progressive and Interactive Analytics Processes for High-Dimensional Data Analysis","DOI":"10.1109/TVCG.2016.2598470","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598470","FirstPage":131,"LastPage":140,"PaperType":"J","Abstract":"In interactive data analysis processes, the dialogue between the human and the computer is the enabling mechanism that can lead to actionable observations about the phenomena being investigated. It is of paramount importance that this dialogue is not interrupted by slow computational mechanisms that do not consider any known temporal human-computer interaction characteristics that prioritize the perceptual and cognitive capabilities of the users. In cases where the analysis involves an integrated computational method, for instance to reduce the dimensionality of the data or to perform clustering, such non-optimal processes are often likely. To remedy this, progressive computations, where results are iteratively improved, are getting increasing interest in visual analytics. In this paper, we present techniques and design considerations to incorporate progressive methods within interactive analysis processes that involve high-dimensional data. We define methodologies to facilitate processes that adhere to the perceptual characteristics of users and describe how online algorithms can be incorporated within these. A set of design recommendations and according methods to support analysts in accomplishing high-dimensional data analysis tasks are then presented. Our arguments and decisions here are informed by observations gathered over a series of analysis sessions with analysts from finance. We document observations and recommendations from this study and present evidence on how our approach contribute to the efficiency and productivity of interactive visual analysis sessions involving high-dimensional data.","AuthorNames-Deduped":"Cagatay Turkay;Erdem Kaya;Selim Balcisoy;Helwig Hauser","AuthorNames":"Cagatay Turkay;Erdem Kaya;Selim Balcisoy;Helwig Hauser","AuthorAffiliation":"City University, London, UK;Sabanci University, Turkey;Sabanci University, Turkey;University of Bergen, Norway","InternalReferences":"10.1109/TVCG.2007.70539;10.1109/VAST.2008.4677361;10.1109/TVCG.2008.153;10.1109/TVCG.2014.2346481;10.1109/TVCG.2014.2346574;10.1109/TVCG.2007.70515;10.1109/TVCG.2012.213;10.1109/TVCG.2013.125;10.1109/TVCG.2012.256;10.1109/VAST.2008.4677357;10.1109/TVCG.2015.2467613;10.1109/TVCG.2014.2346265;10.1109/TVCG.2011.178;10.1109/INFVIS.2005.1532136;10.1109/INFVIS.1996.559223;10.1109/TVCG.2011.229;10.1109/TVCG.2008.125","AuthorKeywords":"Progressive analytics;high dimensional data;iterative refinement;visual analytics","AminerCitationCount_02-2020":16,"AminerCitationCount_06-2020":24,"XploreCitationCount - 2020-01":24,"PubsCited":48,"Award":null,"image":"23tvcg01-turkay-2598470-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"A Grammar-based Approach for Modeling User Interactions and Generating Suggestions During the Data Exploration Process","DOI":"10.1109/TVCG.2016.2598471","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598471","FirstPage":41,"LastPage":50,"PaperType":"J","Abstract":"Despite the recent popularity of visual analytics focusing on big data, little is known about how to support users that use visualization techniques to explore multi-dimensional datasets and accomplish specific tasks. Our lack of models that can assist end-users during the data exploration process has made it challenging to learn from the user's interactive and analytical process. The ability to model how a user interacts with a specific visualization technique and what difficulties they face are paramount in supporting individuals with discovering new patterns within their complex datasets. This paper introduces the notion of visualization systems understanding and modeling user interactions with the intent of guiding a user through a task thereby enhancing visual data exploration. The challenges faced and the necessary future steps to take are discussed; and to provide a working example, a grammar-based model is presented that can learn from user interactions, determine the common patterns among a number of subjects using a K-Reversible algorithm, build a set of rules, and apply those rules in the form of suggestions to new users with the goal of guiding them along their visual analytic process. A formal evaluation study with 300 subjects was performed showing that our grammar-based model is effective at capturing the interactive process followed by users and that further research in this area has the potential to positively impact how users interact with a visualization system.","AuthorNames-Deduped":"Filip Dabek;Jesus J. Caban","AuthorNames":"Filip Dabek;Jesus J Caban","AuthorAffiliation":"National Intrepid Center of Excellence, Walter Reed National Military Medical Center, Bethesda, MD;National Intrepid Center of Excellence, Walter Reed National Military Medical Center, Bethesda, MD","InternalReferences":"10.1109/TVCG.2014.2346575;10.1109/TVCG.2015.2467613;10.1109/VAST.2010.5650854;10.1109/TVCG.2015.2467871;10.1109/VISUAL.2005.1532788;10.1109/TVCG.2012.219;10.1109/VAST.2009.5333020;10.1109/VAST.2006.261436;10.1109/TVCG.2015.2467611;10.1109/TVCG.2015.2467551;10.1109/VAST.2008.4677365;10.1109/TVCG.2007.70589","AuthorKeywords":"Machine Learning;Visual Analytics;User Interactions;Analytic Provenance","AminerCitationCount_02-2020":6,"AminerCitationCount_06-2020":12,"XploreCitationCount - 2020-01":9,"PubsCited":41,"Award":null,"image":"23tvcg01-dabek-2598471-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"Blockwise Human Brain Network Visual Comparison Using NodeTrix Representation","DOI":"10.1109/TVCG.2016.2598472","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598472","FirstPage":181,"LastPage":190,"PaperType":"J","Abstract":"Visually comparing human brain networks from multiple population groups serves as an important task in the field of brain connectomics. The commonly used brain network representation, consisting of nodes and edges, may not be able to reveal the most compelling network differences when the reconstructed networks are dense and homogeneous. In this paper, we leveraged the block information on the Region Of Interest (ROI) based brain networks and studied the problem of blockwise brain network visual comparison. An integrated visual analytics framework was proposed. In the first stage, a two-level ROI block hierarchy was detected by optimizing the anatomical structure and the predictive comparison performance simultaneously. In the second stage, the NodeTrix representation was adopted and customized to visualize the brain network with block information. We conducted controlled user experiments and case studies to evaluate our proposed solution. Results indicated that our visual analytics method outperformed the commonly used node-link graph and adjacency matrix design in the blockwise network comparison tasks. We have shown compelling findings from two real-world brain network data sets, which are consistent with the prior connectomics studies.","AuthorNames-Deduped":"Xinsong Yang;Lei Shi;Madelaine Daianu;Hanghang Tong;Qingsong Liu;Paul M. Thompson","AuthorNames":"Xinsong Yang;Lei Shi;Madelaine Daianu;Hanghang Tong;Qingsong Liu;Paul Thompson","AuthorAffiliation":"Chinese Academy of Sciences, SKLCSInstitute of Software;Chinese Academy of Sciences, SKLCSInstitute of Software;Imaging Genetics CenterMark &#x0026; Mary Stevens Institute for Neuroimaging &#x0026; InformaticsUniversity of Southern California;School of Computing, Informatics and Decision Systems EngineeringArizona State University;Chinese Academy of Sciences, SKLCSInstitute of Software;Imaging Genetics CenterMark &#x0026; Mary Stevens Institute for Neuroimaging &#x0026; InformaticsUniversity of Southern California","InternalReferences":"10.1109/TVCG.2014.2346312;10.1109/VISUAL.2005.1532773;10.1109/TVCG.2007.70582","AuthorKeywords":"Brain Network;Visual Comparison;Hybrid Representation","AminerCitationCount_02-2020":10,"AminerCitationCount_06-2020":17,"XploreCitationCount - 2020-01":13,"PubsCited":42,"Award":null,"image":"23tvcg01-yang-2598472-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"A Visual Analytics Approach for Categorical Joint Distribution Reconstruction from Marginal Projections","DOI":"10.1109/TVCG.2016.2598479","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598479","FirstPage":51,"LastPage":60,"PaperType":"J","Abstract":"Oftentimes multivariate data are not available as sets of equally multivariate tuples, but only as sets of projections into subspaces spanned by subsets of these attributes. For example, one may find data with five attributes stored in six tables of two attributes each, instead of a single table of five attributes. This prohibits the visualization of these data with standard high-dimensional methods, such as parallel coordinates or MDS, and there is hence the need to reconstruct the full multivariate (joint) distribution from these marginal ones. Most of the existing methods designed for this purpose use an iterative procedure to estimate the joint distribution. With insufficient marginal distributions and domain knowledge, they lead to results whose joint errors can be large. Moreover, enforcing smoothness for regularizations in the joint space is not applicable if the attributes are not numerical but categorical. We propose a visual analytics approach that integrates both anecdotal data and human experts to iteratively narrow down a large set of plausible solutions. The solution space is populated using a Monte Carlo procedure which uniformly samples the solution space. A level-of-detail high dimensional visualization system helps the user understand the patterns and the uncertainties. Constraints that narrow the solution space can then be added by the user interactively during the iterative exploration, and eventually a subset of solutions with narrow uncertainty intervals emerges.","AuthorNames-Deduped":"Cong Xie;Wen Zhong;Klaus Mueller","AuthorNames":"Cong Xie;Wen Zhong;Klaus Mueller","AuthorAffiliation":"Computer Science Department, Stony Brook University;Computer Science Department, Stony Brook University;Computer Science Department, Stony Brook University","InternalReferences":"10.1109/TVCG.2010.181;10.1109/TVCG.2013.190;10.1109/VAST.2009.5332586;10.1109/TVCG.2015.2467552;10.1109/VAST.2009.5332611;10.1109/TVCG.2010.183;10.1109/TVCG.2011.248","AuthorKeywords":"Parallel Coordinates;Joint Distribution Reconstruction;Solution Space;High-dimensional Data;Multivariate Data","AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":9,"XploreCitationCount - 2020-01":9,"PubsCited":35,"Award":"HM","image":"23tvcg01-xie-2598479-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"Visual Interaction with Dimensionality Reduction: A Structured Literature Analysis","DOI":"10.1109/TVCG.2016.2598495","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598495","FirstPage":241,"LastPage":250,"PaperType":"J","Abstract":"Dimensionality Reduction (DR) is a core building block in visualizing multidimensional data. For DR techniques to be useful in exploratory data analysis, they need to be adapted to human needs and domain-specific problems, ideally, interactively, and on-the-fly. Many visual analytics systems have already demonstrated the benefits of tightly integrating DR with interactive visualizations. Nevertheless, a general, structured understanding of this integration is missing. To address this, we systematically studied the visual analytics and visualization literature to investigate how analysts interact with automatic DR techniques. The results reveal seven common interaction scenarios that are amenable to interactive control such as specifying algorithmic constraints, selecting relevant features, or choosing among several DR algorithms. We investigate specific implementations of visual analysis systems integrating DR, and analyze ways that other machine learning methods have been combined with DR. Summarizing the results in a “human in the loop” process model provides a general lens for the evaluation of visual interactive DR systems. We apply the proposed model to study and classify several systems previously described in the literature, and to derive future research opportunities.","AuthorNames-Deduped":"Dominik Sacha;Leishi Zhang;Michael Sedlmair;John A. Lee;Jaakko Peltonen;Daniel Weiskopf;Stephen C. North;Daniel A. Keim","AuthorNames":"Dominik Sacha;Leishi Zhang;Michael Sedlmair;John A. Lee;Jaakko Peltonen;Daniel Weiskopf;Stephen C. North;Daniel A. Keim","AuthorAffiliation":"University of Konstanz, Germany;Middlesex University, UK;University of Vienna, Austria;SSS, IREC, MIRO, Université catholique de LouvainUCLBelgian F.R.S.-FNRS.;Helsinki Institute for Information Technology HIIT, Aalto University, University of Tampere, Finland;University of Konstanz, Germany;Infovisible LLC, Oldwick, U.S.A.;VISUS, University of Stuttgart, Germany","InternalReferences":"10.1109/TVCG.2012.195;10.1109/TVCG.2009.153;10.1109/VAST.2012.6400486;10.1109/TVCG.2014.2346481;10.1109/VAST.2011.6102449;10.1109/TVCG.2007.70515;10.1109/VAST.2008.4677350;10.1109/VAST.2009.5332629;10.1109/VAST.2010.5652443;10.1109/VAST.2014.7042492;10.1109/TVCG.2015.2467132;10.1109/TVCG.2015.2467553;10.1109/TVCG.2014.2346321;10.1109/TVCG.2013.153;10.1109/VAST.2010.5652484;10.1109/TVCG.2006.156;10.1109/TVCG.2015.2467717;10.1109/TVCG.2011.229;10.1109/TVCG.2013.124;10.1109/VAST.2010.5652392;10.1109/TVCG.2013.126","AuthorKeywords":"Interactive visualization;machine learning;visual analytics;dimensionality reduction","AminerCitationCount_02-2020":26,"AminerCitationCount_06-2020":64,"XploreCitationCount - 2020-01":57,"PubsCited":59,"Award":null,"image":"23tvcg01-sacha-2598495-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"VisFlow - Web-based Visualization Framework for Tabular Data with a Subset Flow Model","DOI":"10.1109/TVCG.2016.2598497","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598497","FirstPage":251,"LastPage":260,"PaperType":"J","Abstract":"Data flow systems allow the user to design a flow diagram that specifies the relations between system components which process, filter or visually present the data. Visualization systems may benefit from user-defined data flows as an analysis typically consists of rendering multiple plots on demand and performing different types of interactive queries across coordinated views. In this paper, we propose VisFlow, a web-based visualization framework for tabular data that employs a specific type of data flow model called the subset flow model. VisFlow focuses on interactive queries within the data flow, overcoming the limitation of interactivity from past computational data flow systems. In particular, VisFlow applies embedded visualizations and supports interactive selections, brushing and linking within a visualization-oriented data flow. The model requires all data transmitted by the flow to be a data item subset (i.e. groups of table rows) of some original input table, so that rendering properties can be assigned to the subset unambiguously for tracking and comparison. VisFlow features the analysis flexibility of a flow diagram, and at the same time reduces the diagram complexity and improves usability. We demonstrate the capability of VisFlow on two case studies with domain experts on real-world datasets showing that VisFlow is capable of accomplishing a considerable set of visualization and analysis tasks. The VisFlow system is available as open source on GitHub.","AuthorNames-Deduped":"Bowen Yu;Cláudio T. Silva","AuthorNames":"Bowen Yu;Cláudio T. Silva","AuthorAffiliation":"New York University;New York University","InternalReferences":"10.1109/TVCG.2009.195;10.1109/INFVIS.2004.12;10.1109/VAST.2011.6102440;10.1109/INFVIS.1998.729560;10.1109/TVCG.2014.2346260;10.1109/INFVIS.2005.1532136;10.1109/TVCG.2011.225;10.1109/INFVIS.2003.1249013;10.1109/VISUAL.2005.1532788;10.1109/TVCG.2014.2346753;10.1109/TVCG.2011.185;10.1109/TVCG.2014.2346291","AuthorKeywords":"Visualization framework;data flow;subset flow model;tabular data","AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":8,"PubsCited":47,"Award":null,"image":"23tvcg01-yu-2598497-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"Annotation Graphs: A Graph-Based Visualization for Meta-Analysis of Data Based on User-Authored Annotations","DOI":"10.1109/TVCG.2016.2598543","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598543","FirstPage":261,"LastPage":270,"PaperType":"J","Abstract":"User-authored annotations of data can support analysts in the activity of hypothesis generation and sensemaking, where it is not only critical to document key observations, but also to communicate insights between analysts. We present annotation graphs, a dynamic graph visualization that enables meta-analysis of data based on user-authored annotations. The annotation graph topology encodes annotation semantics, which describe the content of and relations between data selections, comments, and tags. We present a mixed-initiative approach to graph layout that integrates an analyst's manual manipulations with an automatic method based on similarity inferred from the annotation semantics. Various visual graph layout styles reveal different perspectives on the annotation semantics. Annotation graphs are implemented within C8, a system that supports authoring annotations during exploratory analysis of a dataset. We apply principles of Exploratory Sequential Data Analysis (ESDA) in designing C8, and further link these to an existing task typology in the visualization literature. We develop and evaluate the system through an iterative user-centered design process with three experts, situated in the domain of analyzing HCI experiment data. The results suggest that annotation graphs are effective as a method of visually extending user-authored annotations to data meta-analysis for discovery and organization of ideas.","AuthorNames-Deduped":"Jian Zhao 0010;Michael Glueck;Simon Breslav;Fanny Chevalier;Azam Khan","AuthorNames":"Jian Zhao;Michael Glueck;Simon Breslav;Fanny Chevalier;Azam Khan","AuthorAffiliation":"Autodesk Research;Autodesk Research;Autodesk Research;INRIA;Autodesk Research","InternalReferences":"10.1109/VAST.2009.5333878;10.1109/TVCG.2015.2467871;10.1109/VAST.2009.5333023;10.1109/VAST.2011.6102447;10.1109/TVCG.2008.137;10.1109/TVCG.2014.2346573;10.1109/VAST.2008.4677365;10.1109/TVCG.2013.124;10.1109/TVCG.2007.70577;10.1109/VAST.2010.5652879","AuthorKeywords":"Externalization user-authored annotation;exploratory sequential data analysis;graph-based visualization","AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":11,"XploreCitationCount - 2020-01":11,"PubsCited":39,"Award":null,"image":"23tvcg01-zhao-2598543-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"Familiarity Vs Trust: A Comparative Study of Domain Scientists' Trust in Visual Analytics and Conventional Analysis Methods","DOI":"10.1109/TVCG.2016.2598544","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598544","FirstPage":271,"LastPage":280,"PaperType":"J","Abstract":"Combining interactive visualization with automated analytical methods like statistics and data mining facilitates data-driven discovery. These visual analytic methods are beginning to be instantiated within mixed-initiative systems, where humans and machines collaboratively influence evidence-gathering and decision-making. But an open research question is that, when domain experts analyze their data, can they completely trust the outputs and operations on the machine-side? Visualization potentially leads to a transparent analysis process, but do domain experts always trust what they see? To address these questions, we present results from the design and evaluation of a mixed-initiative, visual analytics system for biologists, focusing on analyzing the relationships between familiarity of an analysis medium and domain experts' trust. We propose a trust-augmented design of the visual analytics system, that explicitly takes into account domain-specific tasks, conventions, and preferences. For evaluating the system, we present the results of a controlled user study with 34 biologists where we compare the variation of the level of trust across conventional and visual analytic mediums and explore the influence of familiarity and task complexity on trust. We find that despite being unfamiliar with a visual analytic medium, scientists seem to have an average level of trust that is comparable with the same in conventional analysis medium. In fact, for complex sense-making tasks, we find that the visual analytic system is able to inspire greater trust than other mediums. We summarize the implications of our findings with directions for future research on trustworthiness of visual analytic systems.","AuthorNames-Deduped":"Aritra Dasgupta;Joon-Yong Lee;Ryan Wilson;Robert A. Lafrance;Nick Cramer;Kristin A. Cook;Samuel H. Payne","AuthorNames":"Aritra Dasgupta;Joon-Yong Lee;Ryan Wilson;Robert A. Lafrance;Nick Cramer;Kristin Cook;Samuel Payne","AuthorAffiliation":"Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory","InternalReferences":"10.1109/TVCG.2015.2467591;10.1109/VAST.2015.7347625;10.1109/TVCG.2012.224;10.1109/INFVIS.2005.1532136;10.1109/VAST.2006.261416;10.1109/TVCG.2013.124;10.1109/TVCG.2013.120","AuthorKeywords":"trust;transparency;familiarity;uncertainty;biological data analysis","AminerCitationCount_02-2020":6,"AminerCitationCount_06-2020":14,"XploreCitationCount - 2020-01":9,"PubsCited":41,"Award":null,"image":"23tvcg01-dasgupta-2598544-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"What do Constraint Programming Users Want to See? Exploring the Role of Visualisation in Profiling of Models and Search","DOI":"10.1109/TVCG.2016.2598545","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598545","FirstPage":281,"LastPage":290,"PaperType":"J","Abstract":"Constraint programming allows difficult combinatorial problems to be modelled declaratively and solved automatically. Advances in solver technologies over recent years have allowed the successful use of constraint programming in many application areas. However, when a particular solver's search for a solution takes too long, the complexity of the constraint program execution hinders the programmer's ability to profile that search and understand how it relates to their model. Therefore, effective tools to support such profiling and allow users of constraint programming technologies to refine their model or experiment with different search parameters are essential. This paper details the first user-centred design process for visual profiling tools in this domain. We report on: our insights and opportunities identified through an on-line questionnaire and a creativity workshop with domain experts carried out to elicit requirements for analytical and visual profiling techniques; our designs and functional prototypes realising such techniques; and case studies demonstrating how these techniques shed light on the behaviour of the solvers in practice.","AuthorNames-Deduped":"Sarah Goodwin;Christopher Mears;Tim Dwyer;Maria Garcia de la Banda;Guido Tack;Mark Wallace 0001","AuthorNames":"Sarah Goodwin;Christopher Mears;Tim Dwyer;Maria Garcia de la Banda;Guido Tack;Mark Wallace","AuthorAffiliation":"Adaptive Visualisation LabMonash University;Faculty of Information Technology, Monash University;Adaptive Visualisation LabMonash University;Faculty of Information Technology, Monash University;Faculty of Information Technology, Monash University;Faculty of Information Technology, Monash University","InternalReferences":"10.1109/TVCG.2013.145;10.1109/TVCG.2014.2346321;10.1109/TVCG.2015.2467751;10.1109/INFVIS.2004.70;10.1109/TVCG.2014.2346578;10.1109/TVCG.2013.151;10.1109/TVCG.2015.2467851;10.1109/INFVIS.2000.885103","AuthorKeywords":"visual analytics;user-centred design;profiling;constraint programming;tree visualisations","AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":8,"XploreCitationCount - 2020-01":8,"PubsCited":39,"Award":null,"image":"23tvcg01-goodwin-2598545-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"ViDX: Visual Diagnostics of Assembly Line Performance in Smart Factories","DOI":"10.1109/TVCG.2016.2598664","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598664","FirstPage":291,"LastPage":300,"PaperType":"J","Abstract":"Visual analytics plays a key role in the era of connected industry (or industry 4.0, industrial internet) as modern machines and assembly lines generate large amounts of data and effective visual exploration techniques are needed for troubleshooting, process optimization, and decision making. However, developing effective visual analytics solutions for this application domain is a challenging task due to the sheer volume and the complexity of the data collected in the manufacturing processes. We report the design and implementation of a comprehensive visual analytics system, ViDX. It supports both real-time tracking of assembly line performance and historical data exploration to identify inefficiencies, locate anomalies, and form hypotheses about their causes and effects. The system is designed based on a set of requirements gathered through discussions with the managers and operators from manufacturing sites. It features interlinked views displaying data at different levels of detail. In particular, we apply and extend the Marey's graph by introducing a time-aware outlier-preserving visual aggregation technique to support effective troubleshooting in manufacturing processes. We also introduce two novel interaction techniques, namely the quantiles brush and samples brush, for the users to interactively steer the outlier detection algorithms. We evaluate the system with example use cases and an in-depth user interview, both conducted together with the managers and operators from manufacturing plants. The result demonstrates its effectiveness and reports a successful pilot application of visual analytics for manufacturing in smart factories.","AuthorNames-Deduped":"Panpan Xu;Honghui Mei;Liu Ren;Wei Chen 0001","AuthorNames":"Panpan Xu;Honghui Mei;Liu Ren;Wei Chen","AuthorAffiliation":"Bosch Research North America;Zhejiang University;Bosch Research North America;Zhejiang University","InternalReferences":"10.1109/TVCG.2014.2346454;10.1109/TVCG.2015.2467592;10.1109/TVCG.2006.170;10.1109/TVCG.2015.2467622;10.1109/TVCG.2014.2346682;10.1109/TVCG.2012.225;10.1109/TVCG.2013.200;10.1109/INFVIS.2002.1173149;10.1109/TVCG.2011.185","AuthorKeywords":"Temporal Data;Marey's Graph;Visual Analytics;Manufacturing;Smart Factory;Connected Industry;Industry 4.0","AminerCitationCount_02-2020":11,"AminerCitationCount_06-2020":32,"XploreCitationCount - 2020-01":30,"PubsCited":36,"Award":"HM","image":"23tvcg01-xu-2598664-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"Visual Analytics for Mobile Eye Tracking","DOI":"10.1109/TVCG.2016.2598695","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598695","FirstPage":301,"LastPage":310,"PaperType":"J","Abstract":"The analysis of eye tracking data often requires the annotation of areas of interest (AOIs) to derive semantic interpretations of human viewing behavior during experiments. This annotation is typically the most time-consuming step of the analysis process. Especially for data from wearable eye tracking glasses, every independently recorded video has to be annotated individually and corresponding AOIs between videos have to be identified. We provide a novel visual analytics approach to ease this annotation process by image-based, automatic clustering of eye tracking data integrated in an interactive labeling and analysis system. The annotation and analysis are tightly coupled by multiple linked views that allow for a direct interpretation of the labeled data in the context of the recorded video stimuli. The components of our analytics environment were developed with a user-centered design approach in close cooperation with an eye tracking expert. We demonstrate our approach with eye tracking data from a real experiment and compare it to an analysis of the data by manual annotation of dynamic AOIs. Furthermore, we conducted an expert user study with 6 external eye tracking researchers to collect feedback and identify analysis strategies they used while working with our application.","AuthorNames-Deduped":"Kuno Kurzhals;Marcel Hlawatsch;Christof Seeger;Daniel Weiskopf","AuthorNames":"Kuno Kurzhals;Marcel Hlawatsch;Christof Seeger;Daniel Weiskopf","AuthorAffiliation":"University of Stuttgart;University of Stuttgart;Stuttgart Media University;University of Stuttgart","InternalReferences":"10.1109/TVCG.2010.149;10.1109/TVCG.2015.2468091;10.1109/VAST.2006.261433;10.1109/TVCG.2009.111","AuthorKeywords":"Eye tracking;visual analytics;video visualization","AminerCitationCount_02-2020":7,"AminerCitationCount_06-2020":16,"XploreCitationCount - 2020-01":9,"PubsCited":34,"Award":null,"image":"23tvcg01-kurzhals-2598695-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"GazeDx: Interactive Visual Analytics Framework for Comparative Gaze Analysis with Volumetric Medical Images","DOI":"10.1109/TVCG.2016.2598796","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598796","FirstPage":311,"LastPage":320,"PaperType":"J","Abstract":"We present an interactive visual analytics framework, GazeDx (abbr. of GazeDiagnosis), for the comparative analysis of gaze data from multiple readers examining volumetric images while integrating important contextual information with the gaze data. Gaze pattern comparison is essential to understanding how radiologists examine medical images, and to identifying factors influencing the examination. Most prior work depended upon comparisons with manually juxtaposed static images of gaze tracking results. Comparative gaze analysis with volumetric images is more challenging due to the additional cognitive load on 3D perception. A recent study proposed a visualization design based on direct volume rendering (DVR) for visualizing gaze patterns in volumetric images; however, effective and comprehensive gaze pattern comparison is still challenging due to a lack of interactive visualization tools for comparative gaze analysis. We take the challenge with GazeDx while integrating crucial contextual information such as pupil size and windowing into the analysis process for more in-depth and ecologically valid findings. Among the interactive visualization components in GazeDx, a context-embedded interactive scatterplot is especially designed to help users examine abstract gaze data in diverse contexts by embedding medical imaging representations well known to radiologists in it. We present the results from two case studies with two experienced radiologists, where they compared the gaze patterns of 14 radiologists reading two patients' volumetric CT images.","AuthorNames-Deduped":"Hyunjoo Song;Jeongjin Lee;Tae Jung Kim;Kyoung Ho Lee;Bo Hyoung Kim;Jinwook Seo","AuthorNames":"Hyunjoo Song;Jeongjin Lee;Tae Jung Kim;Kyoung Ho Lee;Bohyoung Kim;Jinwook Seo","AuthorAffiliation":"Seoul National University;Soongsil University;Samsung Medical Center;Bundang Hospital, Seoul National University;Hankuk University of Foreign Studies;Seoul National University","InternalReferences":"10.1109/VAST.2011.6102435;10.1109/TVCG.2010.149","AuthorKeywords":"Eye tracking;gaze visualization;gaze pattern comparison;volumetric medical images;context-embedded interactive scatterplot;interactive temporal chart","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":32,"Award":null,"image":"23tvcg01-song-2598796-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"Patterns and Sequences: Interactive Exploration of Clickstreams to Understand Common Visitor Paths","DOI":"10.1109/TVCG.2016.2598797","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598797","FirstPage":321,"LastPage":330,"PaperType":"J","Abstract":"Modern web clickstream data consists of long, high-dimensional sequences of multivariate events, making it difficult to analyze. Following the overarching principle that the visual interface should provide information about the dataset at multiple levels of granularity and allow users to easily navigate across these levels, we identify four levels of granularity in clickstream analysis: patterns, segments, sequences and events. We present an analytic pipeline consisting of three stages: pattern mining, pattern pruning and coordinated exploration between patterns and sequences. Based on this approach, we discuss properties of maximal sequential patterns, propose methods to reduce the number of patterns and describe design considerations for visualizing the extracted sequential patterns and the corresponding raw sequences. We demonstrate the viability of our approach through an analysis scenario and discuss the strengths and limitations of the methods based on user feedback.","AuthorNames-Deduped":"Zhicheng Liu;Yang Wang;Mira Dontcheva;Matthew Hoffman;Seth Walker;Alan Wilson 0004","AuthorNames":"Zhicheng Liu;Yang Wang;Mira Dontcheva;Matthew Hoffman;Seth Walker;Alan Wilson","AuthorAffiliation":"Adobe Research;University of California, Davis;Adobe Research;Adobe Research;Adobe Research;Adobe Systems Inc.","InternalReferences":"10.1109/VAST.2010.5652910;10.1109/VAST.2010.5652926;10.1109/TVCG.2013.225;10.1109/TVCG.2013.200;10.1109/INFVIS.2005.1532152;10.1109/INFVIS.2000.885091;10.1109/TVCG.2014.2346574;10.1109/VAST.2007.4389008;10.1109/TVCG.2011.185;10.1109/VAST.2014.7042487;10.1109/TVCG.2015.2467622;10.1109/VAST.2012.6400494","AuthorKeywords":"event sequences;Clickstream Data;sequence mining;visual analytics","AminerCitationCount_02-2020":16,"AminerCitationCount_06-2020":46,"XploreCitationCount - 2020-01":38,"PubsCited":38,"Award":null,"image":"23tvcg01-liu-2598797-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"Squares: Supporting Interactive Performance Analysis for Multiclass Classifiers","DOI":"10.1109/TVCG.2016.2598828","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598828","FirstPage":61,"LastPage":70,"PaperType":"J","Abstract":"Performance analysis is critical in applied machine learning because it influences the models practitioners produce. Current performance analysis tools suffer from issues including obscuring important characteristics of model behavior and dissociating performance from data. In this work, we present Squares, a performance visualization for multiclass classification problems. Squares supports estimating common performance metrics while displaying instance-level distribution information necessary for helping practitioners prioritize efforts and access data. Our controlled study shows that practitioners can assess performance significantly faster and more accurately with Squares than a confusion matrix, a common performance analysis tool in machine learning.","AuthorNames-Deduped":"Donghao Ren;Saleema Amershi;Bongshin Lee;Jina Suh;Jason D. Williams","AuthorNames":"Donghao Ren;Saleema Amershi;Bongshin Lee;Jina Suh;Jason D. Williams","AuthorAffiliation":"University of California, Santa Barbara;Microsoft Research;Microsoft Research;Microsoft Research;Microsoft Research","InternalReferences":"10.1109/VISUAL.2000.885740;10.1109/VAST.2010.5652443;10.1109/TVCG.2012.277;10.1109/TVCG.2014.2346660;10.1109/VAST.2011.6102453;10.1109/TVCG.2011.185","AuthorKeywords":"Performance analysis;classification;usable machine learning","AminerCitationCount_02-2020":18,"AminerCitationCount_06-2020":52,"XploreCitationCount - 2020-01":33,"PubsCited":38,"Award":null,"image":"23tvcg01-amershi-2598828-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"An Analysis of Machine- and Human-Analytics in Classification","DOI":"10.1109/TVCG.2016.2598829","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598829","FirstPage":71,"LastPage":80,"PaperType":"J","Abstract":"In this work, we present a study that traces the technical and cognitive processes in two visual analytics applications to a common theoretic model of soft knowledge that may be added into a visual analytics process for constructing a decision-tree model. Both case studies involved the development of classification models based on the “bag of features” approach. Both compared a visual analytics approach using parallel coordinates with a machine-learning approach using information theory. Both found that the visual analytics approach had some advantages over the machine learning approach, especially when sparse datasets were used as the ground truth. We examine various possible factors that may have contributed to such advantages, and collect empirical evidence for supporting the observation and reasoning of these factors. We propose an information-theoretic model as a common theoretic basis to explain the phenomena exhibited in these two case studies. Together we provide interconnected empirical and theoretical evidence to support the usefulness of visual analytics.","AuthorNames-Deduped":"Gary K. L. Tam;Vivek Kothari;Min Chen","AuthorNames":"Gary K. L. Tam;Vivek Kothari;Min Chen","AuthorAffiliation":"Swansea University;University of Oxford;University of Oxford","InternalReferences":"10.1109/VAST.2010.5652467;10.1109/TVCG.2015.2467615;10.1109/VAST.2012.6400492;10.1109/TVCG.2013.207;10.1109/TVCG.2015.2467552;10.1109/TVCG.2015.2467612;10.1109/VAST.2010.5652398;10.1109/TVCG.2010.132;10.1109/TVCG.2014.2346660;10.1109/VAST.2015.7347629;10.1109/VAST.2011.6102453;10.1109/VAST.2011.6102448","AuthorKeywords":"information theory;Visual analytics;classification;decision tree;model;facial expression;visualization image","AminerCitationCount_02-2020":13,"AminerCitationCount_06-2020":24,"XploreCitationCount - 2020-01":20,"PubsCited":51,"Award":"BP","image":"23tvcg01-tam-2598829-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"Multi-Resolution Climate Ensemble Parameter Analysis with Nested Parallel Coordinates Plots","DOI":"10.1109/TVCG.2016.2598830","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598830","FirstPage":81,"LastPage":90,"PaperType":"J","Abstract":"Due to the uncertain nature of weather prediction, climate simulations are usually performed multiple times with different spatial resolutions. The outputs of simulations are multi-resolution spatial temporal ensembles. Each simulation run uses a unique set of values for multiple convective parameters. Distinct parameter settings from different simulation runs in different resolutions constitute a multi-resolution high-dimensional parameter space. Understanding the correlation between the different convective parameters, and establishing a connection between the parameter settings and the ensemble outputs are crucial to domain scientists. The multi-resolution high-dimensional parameter space, however, presents a unique challenge to the existing correlation visualization techniques. We present Nested Parallel Coordinates Plot (NPCP), a new type of parallel coordinates plots that enables visualization of intra-resolution and inter-resolution parameter correlations. With flexible user control, NPCP integrates superimposition, juxtaposition and explicit encodings in a single view for comparative data visualization and analysis. We develop an integrated visual analytics system to help domain scientists understand the connection between multi-resolution convective parameters and the large spatial temporal ensembles. Our system presents intricate climate ensembles with a comprehensive overview and on-demand geographic details. We demonstrate NPCP, along with the climate ensemble visualization system, based on real-world use-cases from our collaborators in computational and predictive science.","AuthorNames-Deduped":"Junpeng Wang;Xiaotong Liu;Han-Wei Shen;Guang Lin","AuthorNames":"Junpeng Wang;Xiaotong Liu;Han-Wei Shen;Guang Lin","AuthorAffiliation":"The Ohio State University;The Ohio State University;The Ohio State University;Purdue University","InternalReferences":"10.1109/TVCG.2010.181;10.1109/TVCG.2008.153;10.1109/INFVIS.1998.729559;10.1109/TVCG.2012.237;10.1109/INFVIS.2004.68;10.1109/TVCG.2014.2346755;10.1109/SciVis.2015.7429487;10.1109/VISUAL.1999.809866;10.1109/TVCG.2013.122;10.1109/INFVIS.2004.15;10.1109/TVCG.2015.2467431;10.1109/TVCG.2015.2468093;10.1109/TVCG.2010.184;10.1109/TVCG.2014.2346321","AuthorKeywords":"Parallel coordinates plots;parameter analysis;multi-resolution climate ensembles","AminerCitationCount_02-2020":6,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":27,"PubsCited":48,"Award":null,"image":"23tvcg01-wang-2598830-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"Towards Better Analysis of Deep Convolutional Neural Networks","DOI":"10.1109/TVCG.2016.2598831","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598831","FirstPage":91,"LastPage":100,"PaperType":"J","Abstract":"Deep convolutional neural networks (CNNs) have achieved breakthrough performance in many pattern recognition tasks such as image classification. However, the development of high-quality deep models typically relies on a substantial amount of trial-and-error, as there is still no clear understanding of when and why a deep model works. In this paper, we present a visual analytics approach for better understanding, diagnosing, and refining deep CNNs. We formulate a deep CNN as a directed acyclic graph. Based on this formulation, a hybrid visualization is developed to disclose the multiple facets of each neuron and the interactions between them. In particular, we introduce a hierarchical rectangle packing algorithm and a matrix reordering algorithm to show the derived features of a neuron cluster. We also propose a biclustering-based edge bundling method to reduce visual clutter caused by a large number of connections between neurons. We evaluated our method on a set of CNNs and the results are generally favorable.","AuthorNames-Deduped":"Mengchen Liu;Jiaxin Shi;Zhen Li;Chongxuan Li;Jun Zhu 0001;Shixia Liu","AuthorNames":"Mengchen Liu;Jiaxin Shi;Zhen Li;Chongxuan Li;Jun Zhu;Shixia Liu","AuthorAffiliation":"School of Software and TNListTsinghua University;Dept. of Comp. Sci. &#x0026; Tech., State Key Lab of Intell. Tech. &#x0026; Sys.TNList LabCBICR Center;School of Software and TNListTsinghua University;Dept. of Comp. Sci. &#x0026; Tech., State Key Lab of Intell. Tech. &#x0026; Sys.TNList LabCBICR Center;School of Software and TNListTsinghua University;School of Software and TNListTsinghua University","InternalReferences":"10.1109/TVCG.2015.2468151;10.1109/TVCG.2015.2467554;10.1109/TVCG.2015.2467813;10.1109/TVCG.2010.132;10.1109/TVCG.2008.135;10.1109/TVCG.2014.2346919;10.1109/TVCG.2011.239;10.1109/VISUAL.1991.175815;10.1109/VISUAL.2005.1532820;10.1109/TVCG.2007.70582;10.1109/TVCG.2014.2346433","AuthorKeywords":"Deep convolutional neural networks;rectangle packing;matrix reordering;edge bundling;biclustering","AminerCitationCount_02-2020":40,"AminerCitationCount_06-2020":122,"XploreCitationCount - 2020-01":97,"PubsCited":60,"Award":null,"image":"23tvcg01-liu-2598831-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"Visualizing the Hidden Activity of Artificial Neural Networks","DOI":"10.1109/TVCG.2016.2598838","Link":"http://dx.doi.org/10.1109/TVCG.2016.2598838","FirstPage":101,"LastPage":110,"PaperType":"J","Abstract":"In machine learning, pattern classification assigns high-dimensional vectors (observations) to classes based on generalization from examples. Artificial neural networks currently achieve state-of-the-art results in this task. Although such networks are typically used as black-boxes, they are also widely believed to learn (high-dimensional) higher-level representations of the original observations. In this paper, we propose using dimensionality reduction for two tasks: visualizing the relationships between learned representations of observations, and visualizing the relationships between artificial neurons. Through experiments conducted in three traditional image classification benchmark datasets, we show how visualization can provide highly valuable feedback for network designers. For instance, our discoveries in one of these datasets (SVHN) include the presence of interpretable clusters of learned representations, and the partitioning of artificial neurons into groups with apparently related discriminative roles.","AuthorNames-Deduped":"Paulo E. Rauber;Samuel G. Fadel;Alexandre X. Falcão;Alexandru Telea","AuthorNames":"Paulo E. Rauber;Samuel G. Fadel;Alexandre X. Falcão;Alexandru C. Telea","AuthorAffiliation":"University of GroningenUniversity of Campinas;University of São Paulo;University of Campinas;University of Groningen","InternalReferences":"10.1109/TVCG.2011.178;10.1109/TVCG.2011.220;10.1109/TVCG.2013.150;10.1109/TVCG.2014.2346578;10.1109/TVCG.2008.125;10.1109/TVCG.2015.2467553","AuthorKeywords":"Artificial neural networks;dimensionality reduction;algorithm understanding","AminerCitationCount_02-2020":21,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":70,"PubsCited":50,"Award":null,"image":"23tvcg01-rauber-2598838-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"VisMatchmaker: Cooperation of the User and the Computer in Centralized Matching Adjustment","DOI":"10.1109/TVCG.2016.2599378","Link":"http://dx.doi.org/10.1109/TVCG.2016.2599378","FirstPage":231,"LastPage":240,"PaperType":"J","Abstract":"Centralized matching is a ubiquitous resource allocation problem. In a centralized matching problem, each agent has a preference list ranking the other agents and a central planner is responsible for matching the agents manually or with an algorithm. While algorithms can find a matching which optimizes some performance metrics, they are used as a black box and preclude the central planner from applying his domain knowledge to find a matching which aligns better with the user tasks. Furthermore, the existing matching visualization techniques (i.e. bipartite graph and adjacency matrix) fail in helping the central planner understand the differences between matchings. In this paper, we present VisMatchmaker, a visualization system which allows the central planner to explore alternatives to an algorithm-generated matching. We identified three common tasks in the process of matching adjustment: problem detection, matching recommendation and matching evaluation. We classified matching comparison into three levels and designed visualization techniques for them, including the number line view and the stacked graph view. Two types of algorithmic support, namely direct assignment and range search, and their interactive operations are also provided to enable the user to apply his domain knowledge in matching adjustment.","AuthorNames-Deduped":"Po-Ming Law;Wenchao Wu;Yixian Zheng;Huamin Qu","AuthorNames":"Po-Ming Law;Wenchao Wu;Yixian Zheng;Huamin Qu","AuthorAffiliation":"Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology","InternalReferences":"10.1109/INFVIS.2004.1;10.1109/TVCG.2006.122;10.1109/TVCG.2014.2346249;10.1109/TVCG.2014.2346441;10.1109/VAST.2011.6102453","AuthorKeywords":"Centralized matching;matching visualization;interaction techniques;visual analytics","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":2,"PubsCited":32,"Award":null,"image":"23tvcg01-law-2599378-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"Supporting visual exploration for multiple users in large display environments","DOI":"10.1109/VAST.2016.7883506","Link":"http://dx.doi.org/10.1109/VAST.2016.7883506","FirstPage":1,"LastPage":10,"PaperType":"C","Abstract":"We present a design space exploration of interaction techniques for supporting multiple collaborators exploring data on a shared large display. Our proposed solution is based on users controlling individual lenses using both explicit gestures as well as proxemics: the spatial relations between people and physical artifacts such as their distance, orientation, and movement. We discuss different design considerations for implicit and explicit interactions through the lens, and evaluate the user experience to find a balance between the implicit and explicit interaction styles. Our findings indicate that users favor implicit interaction through proxemics for navigation and collaboration, but prefer using explicit mid-air gestures to perform actions that are perceived to be direct, such as terminating a lens composition. Based on these results, we propose a hybrid technique utilizing both proxemics and mid-air gestures, along with examples applying this technique to other datasets. Finally, we performed a usability evaluation of the hybrid technique and observed user performance improvements in the presence of both implicit and explicit interaction styles.","AuthorNames-Deduped":"Sriram Karthik Badam;Fereshteh Amini;Niklas Elmqvist;Pourang Irani","AuthorNames":"Sriram Karthik Badam;Fereshteh Amini;Niklas Elmqvist;Pourang Irani","AuthorAffiliation":"University of Maryland, College Park, USA;University of Manitoba, Winnipeg, Canada;University of Maryland, College Park, USA;University of Manitoba, Winnipeg, Canada","InternalReferences":"10.1109/TVCG.2013.166;10.1109/TVCG.2009.162;10.1109/TVCG.2013.163;10.1109/TVCG.2011.185","AuthorKeywords":null,"AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":7,"XploreCitationCount - 2020-01":5,"PubsCited":41,"Award":null,"image":"7883506-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"DocuCompass: Effective exploration of document landscapes","DOI":"10.1109/VAST.2016.7883507","Link":"http://dx.doi.org/10.1109/VAST.2016.7883507","FirstPage":11,"LastPage":20,"PaperType":"C","Abstract":"The creation of interactive visualization to analyze text documents has gained an impressive momentum in recent years. This is not surprising in the light of massive and still increasing amounts of available digitized texts. Websites, social media, news wire, and digital libraries are just few examples of the diverse text sources whose visual analysis and exploration offers new opportunities to effectively mine and manage the information and knowledge hidden within them. A popular visualization method for large text collections is to represent each document by a glyph in 2D space. These landscapes can be the result of optimizing pairwise distances in 2D to represent document similarities, or they are provided directly as meta data, such as geo-locations. For well-defined information needs, suitable interaction methods are available for these spatializations. However, free exploration and navigation on a level of abstraction between a labeled document spatialization and reading single documents is largely unsupported. As a result, vital foraging steps for task-tailored actions, such as selecting subgroups of documents for detailed inspection, or subsequent sense-making steps are hampered. To fill in this gap, we propose DocuCompass, a focus+context approach based on the lens metaphor. It comprises multiple methods to characterize local groups of documents, and to efficiently guide exploration based on users' requirements. DocuCompass thus allows for effective interactive exploration of document landscapes without disrupting the mental map of users by changing the layout itself. We discuss the suitability of multiple navigation and characterization methods for different spatializations and texts. Finally, we provide insights generated through user feedback and discuss the effectiveness of our approach.","AuthorNames-Deduped":"Florian Heimerl;Markus John;Qi Han;Steffen Koch;Thomas Ertl","AuthorNames":"Florian Heimerl;Markus John;Qi Han;Steffen Koch;Thomas Ertl","AuthorAffiliation":null,"InternalReferences":"10.1109/INFVIS.1995.528686;10.1109/VAST.2009.5333443;10.1109/TVCG.2012.277;10.1109/VAST.2007.4389006;10.1109/VAST.2011.6102449;10.1109/TVCG.2013.186;10.1109/VAST.2012.6400487;10.1109/TVCG.2014.2346433;10.1109/TVCG.2008.152;10.1109/TVCG.2013.212;10.1109/TVCG.2013.162;10.1109/TVCG.2015.2467717;10.1109/VAST.2011.6102488;10.1109/VAST.2011.6102456","AuthorKeywords":null,"AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":11,"XploreCitationCount - 2020-01":8,"PubsCited":54,"Award":null,"image":"7883507-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"C2A: Crowd consensus analytics for virtual colonoscopy","DOI":"10.1109/VAST.2016.7883508","Link":"http://dx.doi.org/10.1109/VAST.2016.7883508","FirstPage":21,"LastPage":30,"PaperType":"C","Abstract":"We present a medical crowdsourcing visual analytics platform called C<sup>2</sup>A to visualize, classify and filter crowdsourced clinical data. More specifically, C<sup>2</sup>A is used to build consensus on a clinical diagnosis by visualizing crowd responses and filtering out anomalous activity. Crowdsourcing medical applications have recently shown promise where the non-expert users (the crowd) were able to achieve accuracy similar to the medical experts. This has the potential to reduce interpretation/reading time and possibly improve accuracy by building a consensus on the findings beforehand and letting the medical experts make the final diagnosis. In this paper, we focus on a virtual colonoscopy (VC) application with the clinical technicians as our target users, and the radiologists acting as consultants and classifying segments as benign or malignant. In particular, C<sup>2</sup>A is used to analyze and explore crowd responses on video segments, created from fly-throughs in the virtual colon. C<sup>2</sup>A provides several interactive visualization components to build crowd consensus on video segments, to detect anomalies in the crowd data and in the VC video segments, and finally, to improve the non-expert user's work quality and performance by A/B testing for the optimal crowdsourcing platform and application-specific parameters. Case studies and domain experts feedback demonstrate the effectiveness of our framework in improving workers' output quality, the potential to reduce the radiologists' interpretation time, and hence, the potential to improve the traditional clinical workflow by marking the majority of the video segments as benign based on the crowd consensus.","AuthorNames-Deduped":"Ji Hwan Park;Saad Nadeem;Seyedkoosha Mirhosseini;Arie E. Kaufman","AuthorNames":"Ji Hwan Park;Saad Nadeem;Seyedkoosha Mirhosseini;Arie Kaufman","AuthorAffiliation":"Stony Brook University, United States of America;Stony Brook University, United States of America;Stony Brook University, United States of America;Stony Brook University, United States of America","InternalReferences":"10.1109/TVCG.2015.2467196;10.1109/TVCG.2006.112;10.1109/TVCG.2009.171;10.1109/TVCG.2006.158;10.1109/VAST.2015.7347631;10.1109/TVCG.2013.164;10.1109/TVCG.2015.2467555","AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":1,"PubsCited":39,"Award":null,"image":"7883508-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"The DataSpace for HIV vaccine studies","DOI":"10.1109/VAST.2016.7883509","Link":"http://dx.doi.org/10.1109/VAST.2016.7883509","FirstPage":31,"LastPage":40,"PaperType":"C","Abstract":"The DataSpace for HIV vaccine studies is a discovery tool available on the web to hundreds of investigators. We designed it to help them better understand activity in the field and explore new ideas latent in completed research. The DataSpace harmonizes immunoassay results and study metadata so that a broader research community can pursue more flexible discovery than the typical centrally planned analyses. Insights from human-centered design and beta evaluation suggest strong potential for visual analytics that may also apply to other efforts in open science. The contribution of this paper is to elucidate key domain challenges and demonstrate an application that addresses them. We made several changes to familiar visualizations to support key tasks such as identifying and filtering to a cohort of interest, making meaningful comparisons of time series data from multiple studies that have different plans, and preserving analytic context when making data transformations and comparisons that would normally exclude some data.","AuthorNames-Deduped":"David McColgin;Paul Hoover;Mark Igra","AuthorNames":"David McColgin;Paul Hoover;Mark Igra","AuthorAffiliation":"LabKey Software, United States of America;LabKey Software, United States of America;LabKey Software, United States of America","InternalReferences":null,"AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":18,"Award":null,"image":"7883509-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"D-Map: Visual Analysis of Ego-centric Information Diffusion Patterns in Social Media","DOI":"10.1109/VAST.2016.7883510","Link":"http://dx.doi.org/10.1109/VAST.2016.7883510","FirstPage":41,"LastPage":50,"PaperType":"C","Abstract":"Popular social media platforms could rapidly propagate vital information over social networks among a significant number of people. In this work we present D-Map (Diffusion Map), a novel visualization method to support exploration and analysis of social behaviors during such information diffusion and propagation on typical social media through a map metaphor. In D-Map, users who participated in reposting (i.e., resending a message initially posted by others) one central user's posts (i.e., a series of original tweets) are collected and mapped to a hexagonal grid based on their behavior similarities and in chronological order of the repostings. With additional interaction and linking, D-Map is capable of providing visual portraits of the influential users and describing their social behaviors. A comprehensive visual analysis system is developed to support interactive exploration with D-Map. We evaluate our work with real world social media data and find interesting patterns among users. Key players, important information diffusion paths, and interactions among social communities can be identified.","AuthorNames-Deduped":"Siming Chen;Shuai Chen 0001;Zhenhuang Wang;Jie Liang 0004;Xiaoru Yuan;Nan Cao;Yadong Wu","AuthorNames":"Siming Chen;Shuai Chen;Zhenhuang Wang;Jie Liang;Xiaoru Yuan;Nan Cao;Yadong Wu","AuthorAffiliation":"Key Laboratory of Machine Perception (Ministry of Education), Peking University, China;Key Laboratory of Machine Perception (Ministry of Education), Peking University, China;Key Laboratory of Machine Perception (Ministry of Education), Peking University, China;Faculty of Engineer and Information Technology, The University of Technology, Sydney, Australia;Key Laboratory of Machine Perception (Ministry of Education), Peking University, China;New York University, Shanghai, China;Southwest University of Science and Technology, China","InternalReferences":"10.1109/TVCG.2015.2467196;10.1109/TVCG.2014.2346922;10.1109/TVCG.2012.291;10.1109/TVCG.2010.154;10.1109/TVCG.2007.70582;10.1109/TVCG.2014.2346433;10.1109/INFVIS.2005.1532126;10.1109/TVCG.2013.221;10.1109/TVCG.2014.2346919;10.1109/TVCG.2014.2346920;10.1109/TVCG.2011.185;10.1109/TVCG.2014.2346277","AuthorKeywords":null,"AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":12,"XploreCitationCount - 2020-01":16,"PubsCited":52,"Award":null,"image":"7883510-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"How ideas flow across multiple social groups","DOI":"10.1109/VAST.2016.7883511","Link":"http://dx.doi.org/10.1109/VAST.2016.7883511","FirstPage":51,"LastPage":60,"PaperType":"C","Abstract":"Tracking how correlated ideas flow within and across multiple social groups facilitates the understanding of the transfer of information, opinions, and thoughts on social media. In this paper, we present IdeaFlow, a visual analytics system for analyzing the lead-lag changes within and across pre-defined social groups regarding a specific set of correlated ideas, each of which is described by a set of words. To model idea flows accurately, we develop a random-walk-based correlation model and integrate it with Bayesian conditional cointegration and a tensor-based technique. To convey complex lead-lag relationships over time, IdeaFlow combines the strengths of a bubble tree, a flow map, and a timeline. In particular, we develop a Voronoi-treemap-based bubble tree to help users get an overview of a set of ideas quickly. A correlated-clustering-based layout algorithm is used to simultaneously generate multiple flow maps with less ambiguity. We also introduce a focus+context timeline to explore huge amounts of temporal data at different levels of time granularity. Quantitative evaluation and case studies demonstrate the accuracy and effectiveness of IdeaFlow.","AuthorNames-Deduped":"Xiting Wang;Shixia Liu;Yang Chen;Tai-Quan Peng;Jing Su;Jing Yang;Baining Guo","AuthorNames":"Xiting Wang;Shixia Liu;Yang Chen;Tai-Quan Peng;Jing Su;Jing Yang;Baining Guo","AuthorAffiliation":"School of Software, Tsinghua University, China;School of Software, Tsinghua University, China;School of Software, Tsinghua University, China;Michigan State University, United States of America;Tsinghua University, China;UNCC, United States of America;Microsoft Research, United States of America","InternalReferences":"10.1109/VAST.2011.6102461;10.1109/VAST.2010.5652931;10.1109/TVCG.2015.2467554;10.1109/TVCG.2014.2346433;10.1109/TVCG.2015.2467992;10.1109/TVCG.2015.2467691;10.1109/TVCG.2011.202;10.1109/VAST.2012.6400485;10.1109/TVCG.2013.196;10.1109/TVCG.2015.2467757;10.1109/TVCG.2012.212;10.1109/TVCG.2010.129;10.1109/TVCG.2013.162;10.1109/TVCG.2013.221;10.1109/TVCG.2014.2346919;10.1109/INFVIS.2005.1532152;10.1109/TVCG.2014.2346920;10.1109/TVCG.2015.2467991;10.1109/TVCG.2011.239;10.1109/INFVIS.2005.1532150;10.1109/TVCG.2009.111;10.1109/INFVIS.2005.1532128","AuthorKeywords":null,"AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":12,"XploreCitationCount - 2020-01":12,"PubsCited":56,"Award":null,"image":"7883511-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"EventAction: Visual analytics for temporal event sequence recommendation","DOI":"10.1109/VAST.2016.7883512","Link":"http://dx.doi.org/10.1109/VAST.2016.7883512","FirstPage":61,"LastPage":70,"PaperType":"C","Abstract":"Recommender systems are being widely used to assist people in making decisions, for example, recommending films to watch or books to buy. Despite its ubiquity, the problem of presenting the recommendations of temporal event sequences has not been studied. We propose EventAction, which to our knowledge, is the first attempt at a prescriptive analytics interface designed to present and explain recommendations of temporal event sequences. EventAction provides a visual analytics approach to (1) identify similar records, (2) explore potential outcomes, (3) review recommended temporal event sequences that might help achieve the users' goals, and (4) interactively assist users as they define a personalized action plan associated with a probability of success. Following the design study framework, we designed and deployed EventAction in the context of student advising and reported on the evaluation with a student review manager and three graduate students.","AuthorNames-Deduped":"Fan Du;Catherine Plaisant;Neil Spring;Ben Shneiderman","AuthorNames":"Fan Du;Catherine Plaisant;Neil Spring;Ben Shneiderman","AuthorAffiliation":"University of Maryland, United States of America;University of Maryland, United States of America;University of Maryland, United States of America;University of Maryland, United States of America","InternalReferences":"10.1109/TVCG.2009.187;10.1109/TVCG.2012.225;10.1109/TVCG.2012.213;10.1109/TVCG.2015.2467622;10.1109/TVCG.2014.2346682","AuthorKeywords":null,"AminerCitationCount_02-2020":7,"AminerCitationCount_06-2020":30,"XploreCitationCount - 2020-01":21,"PubsCited":45,"Award":null,"image":"7883512-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"SocialBrands: Visual analysis of public perceptions of brands on social media","DOI":"10.1109/VAST.2016.7883513","Link":"http://dx.doi.org/10.1109/VAST.2016.7883513","FirstPage":71,"LastPage":80,"PaperType":"C","Abstract":"Public perceptions of a brand is critical to its performance. While social media has demonstrated a huge potential to shape public perceptions of brands, existing tools are not intuitive and explanatory for domain users to use as they fail to provide a comprehensive analysis framework for perceptions of brands. In this paper, we present SocialBrands, a novel visual analysis tool for brand managers to understand public perceptions of brands on social media. Social-Brands leverages brand personality framework in marketing literature and social computing approaches to compute the personality of brands from three driving factors (user imagery, employee imagery, and official announcement) on social media, and construct an evidence network explaining the association between brand personality and driving factors. These computational results are then integrated with new interactive visualizations to help brand managers understand personality traits and their driving factors. We demonstrate the usefulness and effectiveness of SocialBrands through a series of user studies with brand managers in an enterprise context. Design lessons are also derived from our studies.","AuthorNames-Deduped":"Xiaotong Liu;Anbang Xu;Liang Gou;Haibin Liu;Rama Akkiraju;Han-Wei Shen","AuthorNames":"Xiaotong Liu;Anbang Xu;Liang Gou;Haibin Liu;Rama Akkiraju;Han-Wei Shen","AuthorAffiliation":"Ohio State University, United States of America;IBM Research, United States of America;Visa Research, United States of America;IBM Research, United States of America;IBM Research, United States of America;Ohio State University, United States of America","InternalReferences":"10.1109/TVCG.2014.2346922;10.1109/VAST.2014.7042496;10.1109/TVCG.2013.227;10.1109/TVCG.2012.291;10.1109/TVCG.2010.129;10.1109/TVCG.2013.221;10.1109/INFVIS.2000.885091;10.1109/TVCG.2011.183","AuthorKeywords":null,"AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":3,"PubsCited":42,"Award":null,"image":"7883513-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"DimScanner: A Relation-based Visual Exploration Approach Towards Data Dimension Inspection","DOI":"10.1109/VAST.2016.7883514","Link":"http://dx.doi.org/10.1109/VAST.2016.7883514","FirstPage":81,"LastPage":90,"PaperType":"C","Abstract":"Exploring multi-dimensional datasets can be cumbersome if data analysts have little knowledge about the data. Various dimension relation inspection tools and dimension exploration tools have been proposed for efficient data examining and understanding. However, the needed workload varies largely with respect to data complexity and user expertise, which can only be reduced with rich background knowledge over the data. In this paper we address the workload challenge with a data structuring and exploration scheme that affords dimension relation detection and that serves as the background knowledge for further investigation. We contribute a novel data structuring scheme that leverages an information-theoretic view structuring algorithm to uncover information-aware relations among different data views, and thereby discloses redundancy and other relation patterns among dimensions. The integrated system, DimScanner, empowers analysts with rich user controls and assistance widgets to interactively detect the relations of multi-dimensional data.","AuthorNames-Deduped":"Jing Xia;Wei Chen 0001;Yumeng Hou;Wanqi Hu;Xinxin Huang;David S. Ebert","AuthorNames":"Jing Xia;Wei Chen;Yumeng Hou;Wanqi Hu;Xinxin Huang;David S. Ebertk","AuthorAffiliation":"State Key Lab of CAD&amp;CG, Zhejiang University, China;State Key Lab of CAD&amp;CG, Zhejiang University, China;State Key Lab of CAD&amp;CG, Zhejiang University, China;State Key Lab of CAD&amp;CG, Zhejiang University, China;State Key Lab of CAD&amp;CG, Zhejiang University, China;Purdue University, United States of America","InternalReferences":"10.1109/TVCG.2015.2467191;10.1109/TVCG.2009.153;10.1109/INFVIS.1998.729559;10.1109/VAST.2009.5332628;10.1109/TVCG.2010.184;10.1109/VAST.2010.5652450;10.1109/VAST.2006.261423;10.1109/TVCG.2013.160;10.1109/TVCG.2013.150;10.1109/TVCG.2011.229;10.1109/INFVIS.2005.1532142;10.1109/INFVIS.2004.3","AuthorKeywords":null,"AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":13,"XploreCitationCount - 2020-01":3,"PubsCited":35,"Award":null,"image":"7883514-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"SenseMap: Supporting browser-based online sensemaking through analytic provenance","DOI":"10.1109/VAST.2016.7883515","Link":"http://dx.doi.org/10.1109/VAST.2016.7883515","FirstPage":91,"LastPage":100,"PaperType":"C","Abstract":"Sensemaking is described as the process in which people collect, organize and create representations of information, all centered around some problem they need to understand. People often get lost when solving complicated tasks using big datasets over long periods of exploration and analysis. They may forget what they have done, are unaware of where they are in the context of the overall task, and are unsure where to continue. In this paper, we introduce a tool, SenseMap, to address these issues in the context of browser-based online sensemaking. We conducted a semi-structured interview with nine participants to explore their behaviors in online sensemaking with existing browser functionality. A simplified sensemaking model based on Pirolli and Card's model is derived to better represent the behaviors we found: users iteratively collect information sources relevant to the task, curate them in a way that makes sense, and finally communicate their findings to others. SenseMap automatically captures provenance of user sensemaking actions and provides multi-linked views to visualize the collected information and enable users to curate and communicate their findings. To explore how SenseMap is used, we conducted a user study in a naturalistic work setting with five participants completing the same sensemaking task related to their daily work activities. All participants found the visual representation and interaction of the tool intuitive to use. Three of them engaged with the tool and produced successful outcomes. It helped them to organize information sources, to quickly find and navigate to the sources they wanted, and to effectively communicate their findings.","AuthorNames-Deduped":"Phong H. Nguyen;Kai Xu 0003;Andy Bardill;Betul Salman;Kate Herd;B. L. William Wong","AuthorNames":"Phong H. Nguyen;Kai Xu;Andy Bardill;Betul Salman;Kate Herd;B.L. William Wong","AuthorAffiliation":"Middlesex University, London, UK;Middlesex University, London, UK;Middlesex University, London, UK;Middlesex University, London, UK;Middlesex University, London, UK;Middlesex University, London, UK","InternalReferences":"10.1109/TVCG.2008.137;10.1109/TVCG.2015.2467611;10.1109/VAST.2008.4677365;10.1109/TVCG.2013.132;10.1109/VISUAL.2005.1532788;10.1109/TVCG.2013.124;10.1109/TVCG.2011.185","AuthorKeywords":null,"AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":7,"XploreCitationCount - 2020-01":6,"PubsCited":41,"Award":null,"image":"7883515-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"PorosityAnalyzer: Visual Analysis and Evaluation of Segmentation Pipelines to Determine the Porosity in Fiber-Reinforced Polymers","DOI":"10.1109/VAST.2016.7883516","Link":"http://dx.doi.org/10.1109/VAST.2016.7883516","FirstPage":101,"LastPage":110,"PaperType":"C","Abstract":"In this paper we present PorosityAnalyzer, a novel tool for detailed evaluation and visual analysis of pore segmentation pipelines to determine the porosity in fiber-reinforced polymers (FRPs). The presented tool consists of two modules: the computation module and the analysis module. The computation module enables a convenient setup and execution of distributed off-line-computations on industrial 3D X-ray computed tomography datasets. It allows the user to assemble individual segmentation pipelines in the form of single pipeline steps, and to specify the parameter ranges as well as the sampling of the parameter-space of each pipeline segment. The result of a single segmentation run consists of the input parameters, the calculated 3D binary-segmentation mask, the resulting porosity value, and other derived results (e.g., segmentation pipeline run-time). The analysis module presents the data at different levels of detail by drill-down filtering in order to determine accurate and robust segmentation pipelines. Overview visualizations allow to initially compare and evaluate the segmentation pipelines. With a scatter plot matrix (SPLOM), the segmentation pipelines are examined in more detail based on their input and output parameters. Individual segmentation-pipeline runs are selected in the SPLOM and visually examined and compared in 2D slice views and 3D renderings by using aggregated segmentation masks and statistical contour renderings. PorosityAnalyzer has been thoroughly evaluated with the help of twelve domain experts. Two case studies demonstrate the applicability of our proposed concepts and visualization techniques, and show that our tool helps domain experts to gain new insights and improve their workflow efficiency.","AuthorNames-Deduped":"Johannes Weissenböck;Artem Amirkhanov;M. Eduard Gröller;Johann Kastner;Christoph Heinzl","AuthorNames":"Johannes Weissenböck;Artem Amirkhanov;Eduard Gröller;Johann Kastner;Christoph Heinzl","AuthorAffiliation":"University of Applied Sciences, Upper Austria, Wels, Austria;University of Applied Sciences, Upper Austria, Wels, Austria;TU Wien, Vienna, Austria;University of Applied Sciences, Upper Austria, Wels, Austria;University of Applied Sciences, Upper Austria, Wels, Austria","InternalReferences":"10.1109/TVCG.2013.147;10.1109/TVCG.2008.153;10.1109/VISUAL.1993.398859;10.1109/TVCG.2012.200;10.1109/TVCG.2011.253;10.1109/TVCG.2014.2346321;10.1109/TVCG.2013.177;10.1109/TVCG.2011.248","AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":5,"PubsCited":33,"Award":null,"image":"7883516-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"DropoutSeer: Visualizing learning patterns in Massive Open Online Courses for dropout reasoning and prediction","DOI":"10.1109/VAST.2016.7883517","Link":"http://dx.doi.org/10.1109/VAST.2016.7883517","FirstPage":111,"LastPage":120,"PaperType":"C","Abstract":"Aiming at massive participation and open access education, Massive Open Online Courses (MOOCs) have attracted millions of learners over the past few years. However, the high dropout rate of learners is considered to be one of the most crucial factors that may hinder the development of MOOCs. To tackle this problem, statistical models have been developed to predict dropout behavior based on learner activity logs. Although predictive models can foresee the dropout behavior, it is still difficult for users to understand the reasons behind the predicted results and further design interventions to prevent dropout. In addition, with a better understanding of dropout, researchers in the area of predictive modeling in turn can improve the models. In this paper, we introduce DropoutSeer, a visual analytics system which not only helps instructors and education experts understand the reasons for dropout, but also allows researchers to identify crucial features which can further improve the performance of the models. Both the heterogeneous data extracted from three different kinds of learner activity logs (i.e., clickstream, forum posts and assignment records) and the predicted results are visualized in the proposed system. Case studies and expert interviews have been conducted to demonstrate the usefulness and effectiveness of DropoutSeer.","AuthorNames-Deduped":"Yuanzhe Chen;Qing Chen;Mingqian Zhao;Sebastien Boyer;Kalyan Veeramachaneni;Huamin Qu","AuthorNames":"Yuanzhe Chen;Qing Chen;Mingqian Zhao;Sebastien Boyer;Kalyan Veeramachaneni;Huamin Qu","AuthorAffiliation":"Hong Kong University of Science and Technology, China;Hong Kong University of Science and Technology, China;Hong Kong University of Science and Technology, China;Massachusetts Institute of Technology, United States of America;Massachusetts Institute of Technology, United States of America;Hong Kong University of Science and Technology, China","InternalReferences":"10.1109/INFVIS.1999.801851;10.1109/TVCG.2015.2468151;10.1109/INFVIS.2000.885098;10.1109/VAST.2010.5652931;10.1109/TVCG.2010.129;10.1109/VAST.2012.6400557;10.1109/INFVIS.2001.963273;10.1109/TVCG.2013.221;10.1109/TVCG.2007.70515;10.1109/TVCG.2011.239","AuthorKeywords":null,"AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":11,"XploreCitationCount - 2020-01":8,"PubsCited":41,"Award":null,"image":"7883517-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"Shape Grammar Extraction for Efficient Query-by-Sketch Pattern Matching in Long Time Series","DOI":"10.1109/VAST.2016.7883518","Link":"http://dx.doi.org/10.1109/VAST.2016.7883518","FirstPage":121,"LastPage":130,"PaperType":"C","Abstract":"Long time-series, involving thousands or even millions of time steps, are common in many application domains but remain very difficult to explore interactively. Often the analytical task in such data is to identify specific patterns, but this is a very complex and computationally difficult problem and so focusing the search in order to only identify interesting patterns is a common solution. We propose an efficient method for exploring user-sketched patterns, incorporating the domain expert's knowledge, in time series data through a shape grammar based approach. The shape grammar is extracted from the time series by considering the data as a combination of basic elementary shapes positioned across different amplitudes. We represent these basic shapes using a ratio value, perform binning on ratio values and apply a symbolic approximation. Our proposed method for pattern matching is amplitude-, scale- and translation-invariant and, since the pattern search and pattern constraint relaxation happen at the symbolic level, is very efficient permitting its use in a real-time/online system. We demonstrate the effectiveness of our method in a case study on stock market data although it is applicable to any numeric time series data.","AuthorNames-Deduped":"Prithiviraj K. Muthumanickam;Katerina Vrotsou;Matthew D. Cooper;Jimmy Johansson","AuthorNames":"Prithiviraj K. Muthumanickam;Katerina Vrotsou;Matthew Cooper;Jimmy Johansson","AuthorAffiliation":"Link&#x00F6;ping University, Sweden;Link&#x00F6;ping University, Sweden;Link&#x00F6;ping University, Sweden;Link&#x00F6;ping University, Sweden","InternalReferences":"10.1109/TVCG.2008.184;10.1109/VAST.2010.5652530;10.1109/TVCG.2009.200;10.1109/TVCG.2010.137","AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":55,"Award":null,"image":"7883518-fig-1-source-large.gif"},{"Conference":"VAST","Year":2016,"Title":"The semantics of sketch: Flexibility in visual query systems for time series data","DOI":"10.1109/VAST.2016.7883519","Link":"http://dx.doi.org/10.1109/VAST.2016.7883519","FirstPage":131,"LastPage":140,"PaperType":"C","Abstract":"Sketching allows analysts to specify complex and free-form patterns of interest. Visual query systems can make use of sketches to locate these patterns of interest in large datasets. However, sketching is ambiguous: the same drawing could represent a multitude of potential queries. In this work, we investigate these ambiguities as they apply to visual query systems for time series data. We define a class of “invariants” - the properties of a time series that the analyst wishes to ignore when performing a sketch-based query. We present the results of a crowd-sourced study, showing that these invariants are key components of how people rate the strength of match between sketch and target. We adapt a number of algorithms for time series matching to support invariants in sketches. Lastly, we present a web-deployed prototype sketch-based visual query system that relies on these invariants. We apply the prototype to data from finance, the digital humanities, and political science.","AuthorNames-Deduped":"Michael Correll;Michael Gleicher","AuthorNames":"Michael Correll;Michael Gleicher","AuthorAffiliation":"University of Washington, United States of America;University of Wisconsin-Madison, United States of America","InternalReferences":"10.1109/TVCG.2014.2346455;10.1109/INFVIS.2005.1532144;10.1109/TVCG.2013.191;10.1109/TVCG.2012.204;10.1109/TVCG.2014.2346452;10.1109/TVCG.2010.162","AuthorKeywords":null,"AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":7,"XploreCitationCount - 2020-01":7,"PubsCited":35,"Award":null,"image":"7883519-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2016,"Title":"Visual analysis and coding of data-rich user behavior","DOI":"10.1109/VAST.2016.7883520","Link":"http://dx.doi.org/10.1109/VAST.2016.7883520","FirstPage":141,"LastPage":150,"PaperType":"C","Abstract":"Investigating user behavior involves abstracting low-level events to higher-level concepts. This requires an analyst to study individual user activities, assign codes which categorize behavior, and develop a consistent classification scheme. To better support this reasoning process of an analyst, we suggest a novel visual analytics approach which integrates rich user data including transcripts, videos, eye movement data, and interaction logs. Word-sized visualizations embedded into a tabular representation provide a space-efficient and detailed overview of user activities. An analyst assigns codes, grouped into code categories, as part of an interactive process. Filtering and searching helps to select specific activities and focus an analysis. A comparison visualization summarizes results of coding and reveals relationships between codes. Editing features support efficient assignment, refinement, and aggregation of codes. We demonstrate the practical applicability and usefulness of our approach in a case study and describe expert feedback.","AuthorNames-Deduped":"Tanja Blascheck;Fabian Beck 0001;Sebastian Baltes;Thomas Ertl;Daniel Weiskopf","AuthorNames":"Tanja Blascheck;Fabian Beck;Sebastian Baltes;Thomas Ertl;Daniel Weiskopf","AuthorAffiliation":"University of Stuttgart, Germany;University of Stuttgart, Germany;University of Trier, Germany;University of Stuttgart, Germany;University of Stuttgart, Germany","InternalReferences":"10.1109/VAST.2009.5333443;10.1109/VISUAL.1990.146402;10.1109/TVCG.2011.226;10.1109/TVCG.2014.2346452;10.1109/TVCG.2008.137;10.1109/TVCG.2015.2467611;10.1109/TVCG.2015.2467757;10.1109/TVCG.2010.194;10.1109/TVCG.2014.2346677;10.1109/VAST.2008.4677365;10.1109/TVCG.2013.124","AuthorKeywords":null,"AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":7,"XploreCitationCount - 2020-01":3,"PubsCited":58,"Award":null,"image":"7883520-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"The Interactive Visualization Gap in Initial Exploratory Data Analysis","DOI":"10.1109/TVCG.2017.2743990","Link":"http://dx.doi.org/10.1109/TVCG.2017.2743990","FirstPage":"278","LastPage":"287","PaperType":"J","Abstract":"Data scientists and other analytic professionals often use interactive visualization in the dissemination phase at the end of a workflow during which findings are communicated to a wider audience. Visualization scientists, however, hold that interactive representation of data can also be used during exploratory analysis itself. Since the use of interactive visualization is optional rather than mandatory, this leaves a “visualization gap” during initial exploratory analysis that is the onus of visualization researchers to fill. In this paper, we explore areas where visualization would be beneficial in applied research by conducting a design study using a novel variation on contextual inquiry conducted with professional data analysts. Based on these interviews and experiments, we propose a set of interactive initial exploratory visualization guidelines which we believe will promote adoption by this type of user.","AuthorNames-Deduped":"Andrea Batch;Niklas Elmqvist","AuthorNames":"Andrea Batch;Niklas Elmqvist","AuthorAffiliation":"College Park, University of Maryland, MD, USA;College Park, University of Maryland, MD, USA","InternalReferences":"10.1109/TVCG.2011.185;10.1109/TVCG.2013.124;10.1109/TVCG.2008.166;10.1109/TVCG.2016.2598545;10.1109/TVCG.2012.219;10.1109/TVCG.2014.2346747;10.1109/TVCG.2014.2346578;10.1109/TVCG.2016.2599030;10.1109/TVCG.2014.2346321;10.1109/INFVIS.2000.885086;10.1109/TVCG.2009.171","AuthorKeywords":"Data science,visualization,visual analytics,contextual inquiry,semi-structured interviews","AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":13,"XploreCitationCount - 2020-01":5,"PubsCited":62,"Award":null,"image":"24tvcg01-julca-2743990-fig-1-source-large.gif"},{"Conference":"VAST","Year":2017,"Title":"How Do Ancestral Traits Shape Family Trees Over Generations?","DOI":"10.1109/TVCG.2017.2744080","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744080","FirstPage":"205","LastPage":"214","PaperType":"J","Abstract":"Whether and how does the structure of family trees differ by ancestral traits over generations? This is a fundamental question regarding the structural heterogeneity of family trees for the multi-generational transmission research. However, previous work mostly focuses on parent-child scenarios due to the lack of proper tools to handle the complexity of extending the research to multi-generational processes. Through an iterative design study with social scientists and historians, we develop TreeEvo that assists users to generate and test empirical hypotheses for multi-generational research. TreeEvo summarizes and organizes family trees by structural features in a dynamic manner based on a traditional Sankey diagram. A pixel-based technique is further proposed to compactly encode trees with complex structures in each Sankey Node. Detailed information of trees is accessible through a space-efficient visualization with semantic zooming. Moreover, TreeEvo embeds Multinomial Logit Model (MLM) to examine statistical associations between tree structure and ancestral traits. We demonstrate the effectiveness and usefulness of TreeEvo through an in-depth case-study with domain experts using a real-world dataset (containing 54,128 family trees of 126,196 individuals).","AuthorNames-Deduped":"Siwei Fu;Hao Dong 0008;Weiwei Cui;Jian Zhao 0010;Huamin Qu","AuthorNames":"Siwei Fu;Hao Dong;Weiwei Cui;Jian Zhao;Huamin Qu","AuthorAffiliation":"Hong Kong University of Science and Technology;Princeton University;Microsoft Research;FX Palo Alto Laboratory;Hong Kong University of Science and Technology","InternalReferences":"10.1109/INFVIS.2002.1173150;10.1109/TVCG.2010.159;10.1109/VAST.2006.261450;10.1109/TVCG.2014.2346433;10.1109/TVCG.2014.2346276;10.1109/TVCG.2007.70556;10.1109/INFVIS.2005.1532124;10.1109/TVCG.2013.200;10.1109/INFVIS.2005.1532152;10.1109/TVCG.2012.213;10.1109/TVCG.2012.226","AuthorKeywords":"Quantitative social science,Design study,Multiple tree visualization,Sankey diagram","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":6,"XploreCitationCount - 2020-01":3,"PubsCited":48,"Award":null,"image":"24tvcg01-fu-2744080-fig-1-source-large.gif"},{"Conference":"VAST","Year":2017,"Title":"LDSScanner: Exploratory Analysis of Low-Dimensional Structures in High-Dimensional Datasets","DOI":"10.1109/TVCG.2017.2744098","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744098","FirstPage":"236","LastPage":"245","PaperType":"J","Abstract":"Many approaches for analyzing a high-dimensional dataset assume that the dataset contains specific structures, e.g., clusters in linear subspaces or non-linear manifolds. This yields a trial-and-error process to verify the appropriate model and parameters. This paper contributes an exploratory interface that supports visual identification of low-dimensional structures in a high-dimensional dataset, and facilitates the optimized selection of data models and configurations. Our key idea is to abstract a set of global and local feature descriptors from the neighborhood graph-based representation of the latent low-dimensional structure, such as pairwise geodesic distance (GD) among points and pairwise local tangent space divergence (LTSD) among pointwise local tangent spaces (LTS). We propose a new LTSD-GD view, which is constructed by mapping LTSD and GD to the<inline-formula><tex-math notation=\"LaTeX\">$x$</tex-math><alternatives><inline-graphic xlink:href=\"24tvcg01-xia-2744098-ieq-1-source.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula>axis and<inline-formula><tex-math notation=\"LaTeX\">$y$</tex-math><alternatives><inline-graphic xlink:href=\"24tvcg01-xia-2744098-ieq-2-source.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula>axis using 1D multidimensional scaling, respectively. Unlike traditional dimensionality reduction methods that preserve various kinds of distances among points, the LTSD-GD view presents the distribution of pointwise LTS (<inline-formula><tex-math notation=\"LaTeX\">$x$</tex-math><alternatives><inline-graphic xlink:href=\"24tvcg01-xia-2744098-ieq-3-source.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula>axis) and the variation of LTS in structures (the combination of<inline-formula><tex-math notation=\"LaTeX\">$x$</tex-math><alternatives><inline-graphic xlink:href=\"24tvcg01-xia-2744098-ieq-4-source.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula>axis and<inline-formula><tex-math notation=\"LaTeX\">$y$</tex-math><alternatives><inline-graphic xlink:href=\"24tvcg01-xia-2744098-ieq-5-source.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula>axis). We design and implement a suite of visual tools for navigating and reasoning about intrinsic structures of a high-dimensional dataset. Three case studies verify the effectiveness of our approach.","AuthorNames-Deduped":"Jiazhi Xia;Fenjin Ye;Wei Chen 0001;Yusi Wang;Weifeng Chen 0002;Yuxin Ma;Anthony K. H. Tung","AuthorNames":"Jiazhi Xia;Fenjin Ye;Wei Chen;Yusi Wang;Weifeng Chen;Yuxin Ma;Anthony K.H. Tung","AuthorAffiliation":"Central South University;Central South University;Zhejiang University;Central South University;Zhejiang University of Finance & Economics;Zhejiang University;National University of Singapore","InternalReferences":"10.1109/TVCG.2011.229;10.1109/VAST.2010.5652450;10.1109/VISUAL.1997.663916;10.1109/TVCG.2013.160;10.1109/VAST.2010.5652392;10.1109/VISUAL.1990.146402;10.1109/INFVIS.2003.1249013;10.1109/TVCG.2015.2467324;10.1109/TVCG.2016.2598495;10.1109/TVCG.2016.2598466;10.1109/INFVIS.2004.3;10.1109/TVCG.2015.2467717;10.1109/VAST.2009.5332628;10.1109/VAST.2012.6400488;10.1109/TVCG.2015.2467191;10.1109/VAST.2016.7883514;10.1109/TVCG.2013.150","AuthorKeywords":"High-dimensional data,low-dimensional structure,subspace,manifold,visual exploration","AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":13,"XploreCitationCount - 2020-01":10,"PubsCited":44,"Award":null,"image":"24tvcg01-xia-2744098-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"Clustering Trajectories by Relevant Parts for Air Traffic Analysis","DOI":"10.1109/TVCG.2017.2744322","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744322","FirstPage":"34","LastPage":"44","PaperType":"J","Abstract":"Clustering of trajectories of moving objects by similarity is an important technique in movement analysis. Existing distance functions assess the similarity between trajectories based on properties of the trajectory points or segments. The properties may include the spatial positions, times, and thematic attributes. There may be a need to focus the analysis on certain parts of trajectories, i.e., points and segments that have particular properties. According to the analysis focus, the analyst may need to cluster trajectories by similarity of their relevant parts only. Throughout the analysis process, the focus may change, and different parts of trajectories may become relevant. We propose an analytical workflow in which interactive filtering tools are used to attach relevance flags to elements of trajectories, clustering is done using a distance function that ignores irrelevant elements, and the resulting clusters are summarized for further analysis. We demonstrate how this workflow can be useful for different analysis tasks in three case studies with real data from the domain of air traffic. We propose a suite of generic techniques and visualization guidelines to support movement data analysis by means of relevance-aware trajectory clustering.","AuthorNames-Deduped":"Gennady L. Andrienko;Natalia V. Andrienko;Georg Fuchs;Jose Manuel Cordero Garcia","AuthorNames":"Gennady Andrienko;Natalia Andrienko;Georg Fuchs;Jose Manuel Cordero Garcia","AuthorAffiliation":"Fraunhofer IAIS, City University, London;Fraunhofer IAIS, City University, London;Fraunhofer Institute IAIS;CRIDA (Reference Center for Research, Development and Innovation in ATM)","InternalReferences":"10.1109/VAST.2009.5332584;10.1109/TVCG.2013.193;10.1109/TVCG.2011.233;10.1109/TVCG.2015.2468292;10.1109/VAST.2008.4677350","AuthorKeywords":"Visual analytics,movement data analysis,trajectory clustering,air traffic","AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":16,"XploreCitationCount - 2020-01":10,"PubsCited":53,"Award":null,"image":"24tvcg01-andrienko-2744322-fig-1-source-large.gif"},{"Conference":"VAST","Year":2017,"Title":"DeepEyes: Progressive Visual Analytics for Designing Deep Neural Networks","DOI":"10.1109/TVCG.2017.2744358","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744358","FirstPage":"98","LastPage":"108","PaperType":"J","Abstract":"Deep neural networks are now rivaling human accuracy in several pattern recognition problems. Compared to traditional classifiers, where features are handcrafted, neural networks learn increasingly complex features directly from the data. Instead of handcrafting the features, it is now the network architecture that is manually engineered. The network architecture parameters such as the number of layers or the number of filters per layer and their interconnections are essential for good performance. Even though basic design guidelines exist, designing a neural network is an iterative trial-and-error process that takes days or even weeks to perform due to the large datasets used for training. In this paper, we present DeepEyes, a Progressive Visual Analytics system that supports the design of neural networks during training. We present novel visualizations, supporting the identification of layers that learned a stable set of patterns and, therefore, are of interest for a detailed analysis. The system facilitates the identification of problems, such as superfluous filters or layers, and information that is not being captured by the network. We demonstrate the effectiveness of our system through multiple use cases, showing how a trained network can be compressed, reshaped and adapted to different problems.","AuthorNames-Deduped":"Nicola Pezzotti;Thomas Höllt;Jan C. van Gemert;Boudewijn P. F. Lelieveldt;Elmar Eisemann;Anna Vilanova","AuthorNames":"Nicola Pezzotti;Thomas Höllt;Jan Van Gemert;Boudewijn P.F. Lelieveldt;Elmar Eisemann;Anna Vilanova","AuthorAffiliation":"Intelligent Systems department, Delft University of Technology, Delft, The Netherlands;Intelligent Systems department, Delft University of Technology, Delft, The Netherlands;Intelligent Systems department, Delft University of Technology, Delft, The Netherlands;Department of Radiology, Division of Image Processing, Leiden University Medical Center, Leiden, The Netherlands;Intelligent Systems department, Delft University of Technology, Delft, The Netherlands;Intelligent Systems department, Delft University of Technology, Delft, The Netherlands","InternalReferences":"10.1109/TVCG.2016.2598468;10.1109/TVCG.2014.2346578;10.1109/TVCG.2016.2598838;10.1109/TVCG.2014.2346574;10.1109/TVCG.2016.2598470","AuthorKeywords":"Progressive visual analytics,deep neural networks,machine learning","AminerCitationCount_02-2020":10,"AminerCitationCount_06-2020":51,"XploreCitationCount - 2020-01":23,"PubsCited":50,"Award":null,"image":"24tvcg01-pezzotti-2744358-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"Visual Diagnosis of Tree Boosting Methods","DOI":"10.1109/TVCG.2017.2744378","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744378","FirstPage":"163","LastPage":"173","PaperType":"J","Abstract":"Tree boosting, which combines weak learners (typically decision trees) to generate a strong learner, is a highly effective and widely used machine learning method. However, the development of a high performance tree boosting model is a time-consuming process that requires numerous trial-and-error experiments. To tackle this issue, we have developed a visual diagnosis tool, BOOSTVis, to help experts quickly analyze and diagnose the training process of tree boosting. In particular, we have designed a temporal confusion matrix visualization, and combined it with a t-SNE projection and a tree visualization. These visualization components work together to provide a comprehensive overview of a tree boosting model, and enable an effective diagnosis of an unsatisfactory training process. Two case studies that were conducted on the Otto Group Product Classification Challenge dataset demonstrate that BOOSTVis can provide informative feedback and guidance to improve understanding and diagnosis of tree boosting algorithms.","AuthorNames-Deduped":"Shixia Liu;Jiannan Xiao;Junlin Liu;Xiting Wang;Jing Wu;Jun Zhu 0001","AuthorNames":"Shixia Liu;Jiannan Xiao;Junlin Liu;Xiting Wang;Jing Wu;Jun Zhu","AuthorAffiliation":"Tsinghua University and National Engineering Lab for Big Data Software;Tsinghua University and National Engineering Lab for Big Data Software;Tsinghua University and National Engineering Lab for Big Data Software;Microsoft Research;Cardiff University;Tsinghua University and National Engineering Lab for Big Data Software","InternalReferences":"10.1109/TVCG.2014.2346660;10.1109/VAST.2010.5652443;10.1109/TVCG.2012.277;10.1109/VAST.2012.6400492;10.1109/TVCG.2016.2598831;10.1109/TVCG.2016.2598838;10.1109/TVCG.2016.2598828;10.1109/VISUAL.2000.885740;10.1109/VISUAL.2005.1532820;10.1109/VAST.2011.6102453","AuthorKeywords":"tree boosting,model analysis,temporal confusion matrix,tree visualization","AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":18,"XploreCitationCount - 2020-01":6,"PubsCited":68,"Award":null,"image":"24tvcg01-xiao-2744378-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"Understanding the Relationship Between Interactive Optimisation and Visual Analytics in the Context of Prostate Brachytherapy","DOI":"10.1109/TVCG.2017.2744418","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744418","FirstPage":"319","LastPage":"329","PaperType":"J","Abstract":"The fields of operations research and computer science have long sought to find automatic solver techniques that can find high-quality solutions to difficult real-world optimisation problems. The traditional workflow is to exactly model the problem and then enter this model into a general-purpose “black-box” solver. In practice, however, many problems cannot be solved completely automatically, but require a “human-in-the-loop” to iteratively refine the model and give hints to the solver. In this paper, we explore the parallels between this interactive optimisation workflow and the visual analytics sense-making loop. We assert that interactive optimisation is essentially a visual analytics task and propose a problem-solving loop analogous to the sense-making loop. We explore these ideas through an in-depth analysis of a use-case in prostate brachytherapy, an application where interactive optimisation may be able to provide significant assistance to practitioners in creating prostate cancer treatment plans customised to each patient's tumour characteristics. However, current brachytherapy treatment planning is usually a careful, mostly manual process involving multiple professionals. We developed a prototype interactive optimisation tool for brachytherapy that goes beyond current practice in supporting focal therapy - targeting tumour cells directly rather than simply seeking coverage of the whole prostate gland. We conducted semi-structured interviews, in two stages, with seven radiation oncology professionals in order to establish whether they would prefer to use interactive optimisation for treatment planning and whether such a tool could improve their trust in the novel focal therapy approach and in machine generated solutions to the problem.","AuthorNames-Deduped":"Jie Liu;Tim Dwyer;Kim Marriott;Jeremy Millar;Annette Haworth","AuthorNames":"Jie Liu;Tim Dwyer;Kim Marriott;Jeremy Millar;Annette Haworth","AuthorAffiliation":"Monash University and Data61;Monash University;Monash University and Data61;Monash University and Alfred Health;University of Sydney","InternalReferences":"10.1109/TVCG.2009.170;10.1109/TVCG.2010.190;10.1109/TVCG.2013.147;10.1109/TVCG.2016.2598545;10.1109/VAST.2014.7042481;10.1109/TVCG.2014.2346321;10.1109/TVCG.2011.248","AuthorKeywords":"Visual analytics,interactive optimisation,interactive systems and tools,prostate brachytherapy","AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":0,"PubsCited":49,"Award":null,"image":"24tvcg01-liu-2744418-fig-1-source-large.gif"},{"Conference":"VAST","Year":2017,"Title":"Voila: Visual Anomaly Detection and Monitoring with Streaming Spatiotemporal Data","DOI":"10.1109/TVCG.2017.2744419","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744419","FirstPage":"23","LastPage":"33","PaperType":"J","Abstract":"The increasing availability of spatiotemporal data continuously collected from various sources provides new opportunities for a timely understanding of the data in their spatial and temporal context. Finding abnormal patterns in such data poses significant challenges. Given that there is often no clear boundary between normal and abnormal patterns, existing solutions are limited in their capacity of identifying anomalies in large, dynamic and heterogeneous data, interpreting anomalies in their multifaceted, spatiotemporal context, and allowing users to provide feedback in the analysis loop. In this work, we introduce a unified visual interactive system and framework, Voila, for interactively detecting anomalies in spatiotemporal data collected from a streaming data source. The system is designed to meet two requirements in real-world applications, i.e., online monitoring and interactivity. We propose a novel tensor-based anomaly analysis algorithm with visualization and interaction design that dynamically produces contextualized, interpretable data summaries and allows for interactively ranking anomalous patterns based on user input. Using the “smart city” as an example scenario, we demonstrate the effectiveness of the proposed framework through quantitative evaluation and qualitative case studies.","AuthorNames-Deduped":"Nan Cao;Chaoguang Lin;Qiuhan Zhu;Yu-Ru Lin;Xian Teng;Xidao Wen","AuthorNames":"Nan Cao;Chaoguang Lin;Qiuhan Zhu;Yu-Ru Lin;Xian Teng;Xidao Wen","AuthorAffiliation":"Intelligent Big Data Visualization (iDVx) LabTongji University;Intelligent Big Data Visualization (iDVx) LabTongji University;Intelligent Big Data Visualization (iDVx) LabTongji University;University of Pittsburgh;University of Pittsburgh;University of Pittsburgh","InternalReferences":"10.1109/TVCG.2015.2467196;10.1109/VAST.2012.6400557;10.1109/TVCG.2013.226;10.1109/VAST.2011.6102438;10.1109/VAST.2010.5652467;10.1109/TVCG.2016.2598432;10.1109/TVCG.2016.2598829;10.1109/TVCG.2015.2467194;10.1109/TVCG.2014.2346922","AuthorKeywords":"Anomaly Detection,Visual Analysis","AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":16,"XploreCitationCount - 2020-01":7,"PubsCited":61,"Award":null,"image":"24tvcg01-lin-2744419-fig-1-source-large.gif"},{"Conference":"VAST","Year":2017,"Title":"BiDots: Visual Exploration of Weighted Biclusters","DOI":"10.1109/TVCG.2017.2744458","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744458","FirstPage":"195","LastPage":"204","PaperType":"J","Abstract":"Discovering and analyzing biclusters, i.e., two sets of related entities with close relationships, is a critical task in many real-world applications, such as exploring entity co-occurrences in intelligence analysis, and studying gene expression in bio-informatics. While the output of biclustering techniques can offer some initial low-level insights, visual approaches are required on top of that due to the algorithmic output complexity. This paper proposes a visualization technique, called BiDots, that allows analysts to interactively explore biclusters over multiple domains. BiDots overcomes several limitations of existing bicluster visualizations by encoding biclusters in a more compact and cluster-driven manner. A set of handy interactions is incorporated to support flexible analysis of biclustering results. More importantly, BiDots addresses the cases of weighted biclusters, which has been underexploited in the literature. The design of BiDots is grounded by a set of analytical tasks derived from previous work. We demonstrate its usefulness and effectiveness for exploring computed biclusters with an investigative document analysis task, in which suspicious people and activities are identified from a text corpus.","AuthorNames-Deduped":"Jian Zhao 0010;Maoyuan Sun;Francine Chen;Patrick Chiu","AuthorNames":"Jian Zhao;Maoyuan Sun;Francine Chen;Patrick Chiu","AuthorAffiliation":"FX Palo Alto Laboratory;University of Massachusetts, Dartmouth;FX Palo Alto Laboratory;FX Palo Alto Laboratory","InternalReferences":"10.1109/TVCG.2012.252;10.1109/TVCG.2008.153;10.1109/TVCG.2013.223;10.1109/TVCG.2007.70582;10.1109/VISUAL.1990.146402;10.1109/TVCG.2011.250;10.1109/TVCG.2010.138;10.1109/TVCG.2016.2598831;10.1109/TVCG.2014.2346752;10.1109/VAST.2007.4389006;10.1109/TVCG.2015.2467813;10.1109/TVCG.2014.2346665;10.1109/TVCG.2013.167","AuthorKeywords":"Biclustering,coordinated relationship analysis,visual analytics","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":5,"XploreCitationCount - 2020-01":5,"PubsCited":41,"Award":null,"image":"24tvcg01-zhao-2744458-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"ConceptVector: Text Visual Analytics via Interactive Lexicon Building Using Word Embedding","DOI":"10.1109/TVCG.2017.2744478","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744478","FirstPage":"361","LastPage":"370","PaperType":"J","Abstract":"Central to many text analysis methods is the notion of a concept: a set of semantically related keywords characterizing a specific object, phenomenon, or theme. Advances in word embedding allow building a concept from a small set of seed terms. However, naive application of such techniques may result in false positive errors because of the polysemy of natural language. To mitigate this problem, we present a visual analytics system called ConceptVector that guides a user in building such concepts and then using them to analyze documents. Document-analysis case studies with real-world datasets demonstrate the fine-grained analysis provided by ConceptVector. To support the elaborate modeling of concepts, we introduce a bipolar concept model and support for specifying irrelevant words. We validate the interactive lexicon building interface by a user study and expert reviews. Quantitative evaluation shows that the bipolar lexicon generated with our methods is comparable to human-generated ones.","AuthorNames-Deduped":"Deok Gun Park 0001;Seungyeon Kim;Jurim Lee;Jaegul Choo;Nicholas Diakopoulos;Niklas Elmqvist","AuthorNames":"Deokgun Park;Seungyeon Kim;Jurim Lee;Jaegul Choo;Nicholas Diakopoulos;Niklas Elmqvist","AuthorAffiliation":"University of Maryland, College Park, MD, USA;Google Inc., Mountain View, CA, USA;Korea University, Seoul, Republic of Korea;Korea University, Seoul, Republic of Korea;Northwestern University, Evanston, IL, USA;University of Maryland, College Park, MD, USA","InternalReferences":"10.1109/TVCG.2016.2598667;10.1109/TVCG.2013.212;10.1109/TVCG.2011.239;10.1109/VAST.2011.6102461;10.1109/TVCG.2016.2598446;10.1109/TVCG.2015.2467555","AuthorKeywords":"Text analytics,visual analytics,word embedding,text summarization,text classification,concepts","AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":21,"XploreCitationCount - 2020-01":6,"PubsCited":44,"Award":null,"image":"24tvcg01-choo-2744478-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"Do Convolutional Neural Networks Learn Class Hierarchy?","DOI":"10.1109/TVCG.2017.2744683","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744683","FirstPage":"152","LastPage":"162","PaperType":"J","Abstract":"Convolutional Neural Networks (CNNs) currently achieve state-of-the-art accuracy in image classification. With a growing number of classes, the accuracy usually drops as the possibilities of confusion increase. Interestingly, the class confusion patterns follow a hierarchical structure over the classes. We present visual-analytics methods to reveal and analyze this hierarchy of similar classes in relation with CNN-internal data. We found that this hierarchy not only dictates the confusion patterns between the classes, it furthermore dictates the learning behavior of CNNs. In particular, the early layers in these networks develop feature detectors that can separate high-level groups of classes quite well, even after a few training epochs. In contrast, the latter layers require substantially more epochs to develop specialized feature detectors that can separate individual classes. We demonstrate how these insights are key to significant improvement in accuracy by designing hierarchy-aware CNNs that accelerate model convergence and alleviate overfitting. We further demonstrate how our methods help in identifying various quality issues in the training data.","AuthorNames-Deduped":"Bilal Alsallakh;Amin Jourabloo;Mao Ye;Xiaoming Liu 0002;Liu Ren","AuthorNames":"Alsallakh Bilal;Amin Jourabloo;Mao Ye;Xiaoming Liu;Liu Ren","AuthorAffiliation":"Bosch Research North AmericaPalo Alto, CA;Michigan State University;Bosch Research North AmericaPalo Alto, CA;Michigan State University;Bosch Research North AmericaPalo Alto, CA","InternalReferences":"10.1109/TVCG.2014.2346660;10.1109/VAST.2015.7347637;10.1109/TVCG.2016.2598831;10.1109/TVCG.2016.2598838;10.1109/TVCG.2016.2598828;10.1109/TVCG.2017.2744158;10.1109/VISUAL.2005.1532820;10.1109/VAST.2011.6102453","AuthorKeywords":"Convolutional Neural Networks,deep learning,image classification,large-scale classification,confusion matrix","AminerCitationCount_02-2020":9,"AminerCitationCount_06-2020":41,"XploreCitationCount - 2020-01":12,"PubsCited":77,"Award":null,"image":"24tvcg01-alsallakh-2744683-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"Applying Pragmatics Principles for Interaction with Visual Analytics","DOI":"10.1109/TVCG.2017.2744684","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744684","FirstPage":"309","LastPage":"318","PaperType":"J","Abstract":"Interactive visual data analysis is most productive when users can focus on answering the questions they have about their data, rather than focusing on how to operate the interface to the analysis tool. One viable approach to engaging users in interactive conversations with their data is a natural language interface to visualizations. These interfaces have the potential to be both more expressive and more accessible than other interaction paradigms. We explore how principles from language pragmatics can be applied to the flow of visual analytical conversations, using natural language as an input modality. We evaluate the effectiveness of pragmatics support in our system Evizeon, and present design considerations for conversation interfaces to visual analytics tools.","AuthorNames-Deduped":"Enamul Hoque;Vidya Setlur;Melanie Tory;Isaac Dykeman","AuthorNames":"Enamul Hoque;Vidya Setlur;Melanie Tory;Isaac Dykeman","AuthorAffiliation":"Stanford University;Tableau Research;Tableau Research;Rice University","InternalReferences":"10.1109/TVCG.2014.2346435;10.1109/TVCG.2010.164;10.1109/TVCG.2007.70594;10.1109/INFVIS.2005.1532146","AuthorKeywords":"natural language,interaction,language pragmatics,visual analytics,ambiguity,feedback","AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":10,"XploreCitationCount - 2020-01":4,"PubsCited":41,"Award":null,"image":"24tvcg01-setlur-2744684-fig-1-source-large.gif"},{"Conference":"VAST","Year":2017,"Title":"Visualizing Big Data Outliers Through Distributed Aggregation","DOI":"10.1109/TVCG.2017.2744685","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744685","FirstPage":"256","LastPage":"266","PaperType":"J","Abstract":"Visualizing outliers in massive datasets requires statistical pre-processing in order to reduce the scale of the problem to a size amenable to rendering systems like D3, Plotly or analytic systems like R or SAS. This paper presents a new algorithm, called<monospace>hdoutliers</monospace>, for detecting multidimensional outliers. It is unique for a) dealing with a mixture of categorical and continuous variables, b) dealing with big-p (many columns of data), c) dealing with big-<inline-formula><tex-math notation=\"LaTeX\">$n$</tex-math><alternatives><inline-graphic xlink:href=\"24tvcg01-wilkinson-2744685-ieq-1-source.tif\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"/></alternatives></inline-formula>(many rows of data), d) dealing with outliers that mask other outliers, and e) dealing consistently with unidimensional and multidimensional datasets. Unlike ad hoc methods found in many machine learning papers,<monospace>hdoutliers</monospace>is based on a distributional model that allows outliers to be tagged with a probability. This critical feature reduces the likelihood of false discoveries.","AuthorNames-Deduped":"Leland Wilkinson","AuthorNames":"Leland Wilkinson","AuthorAffiliation":"H2O.aiUIC","InternalReferences":"10.1109/INFVIS.2004.68;10.1109/TVCG.2010.197;10.1109/TVCG.2014.2346572;10.1109/INFVIS.2005.1532138;10.1109/VAST.2012.6400487;10.1109/TVCG.2006.170;10.1109/INFVIS.2003.1249016;10.1109/INFVIS.2005.1532142","AuthorKeywords":"Outliers,Anomalies","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":13,"XploreCitationCount - 2020-01":5,"PubsCited":84,"Award":null,"image":"24tvcg01-wilkinson-2744685-fig-1-source-large.gif"},{"Conference":"VAST","Year":2017,"Title":"Understanding a Sequence of Sequences: Visual Exploration of Categorical States in Lake Sediment Cores","DOI":"10.1109/TVCG.2017.2744686","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744686","FirstPage":"66","LastPage":"76","PaperType":"J","Abstract":"This design study focuses on the analysis of a time sequence of categorical sequences. Such data is relevant for the geoscientific research field of landscape and climate development. It results from microscopic analysis of lake sediment cores. The goal is to gain hypotheses about landscape evolution and climate conditions in the past. To this end, geoscientists identify which categorical sequences are similar in the sense that they indicate similar conditions. Categorical sequences are similar if they have similar meaning (semantic similarity) and appear in similar time periods (temporal similarity). For data sets with many different categorical sequences, the task to identify similar sequences becomes a challenge. Our contribution is a tailored visual analysis concept that effectively supports the analytical process. Our visual interface comprises coupled visualizations of semantics and temporal context for the exploration and assessment of the similarity of categorical sequences. Integrated automatic methods reduce the analytical effort substantially. They (1) extract unique sequences in the data and (2) rank sequences by a similarity measure during the search for similar sequences. We evaluated our concept by demonstrations of our prototype to a larger audience and hands-on analysis sessions for two different lakes. According to geoscientists, our approach fills an important methodological gap in the application domain.","AuthorNames-Deduped":"Andrea Unger;Nadine Drager;Mike Sips;Dirk J. Lehmann","AuthorNames":"Andrea Unger;Nadine Dräger;Mike Sips;Dirk J. Lehmann","AuthorAffiliation":"GFZ German Research Centre for Geosciences;GFZ German Research Centre for Geosciences;GFZ German Research Centre for Geosciences;University of Magdeburg","InternalReferences":"10.1109/TVCG.2011.232;10.1109/VISUAL.1997.663916;10.1109/TVCG.2013.182;10.1109/TVCG.2013.200;10.1109/TVCG.2011.212;10.1109/TVCG.2009.117;10.1109/TVCG.2015.2467751;10.1109/VAST.2009.5332595","AuthorKeywords":"Visualization in Earth Science,Time Series Data,Categorical Data,Design Study","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":6,"XploreCitationCount - 2020-01":2,"PubsCited":46,"Award":null,"image":"24tvcg01-unger-2744686-fig-1-source-large.gif"},{"Conference":"VAST","Year":2017,"Title":"ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models","DOI":"10.1109/TVCG.2017.2744718","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744718","FirstPage":"88","LastPage":"97","PaperType":"J","Abstract":"While deep learning models have achieved state-of-the-art accuracies for many prediction tasks, understanding these models remains a challenge. Despite the recent interest in developing visual tools to help users interpret deep learning models, the complexity and wide variety of models deployed in industry, and the large-scale datasets that they used, pose unique design challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have developed, deployed, and iteratively improved ActiVis, an interactive visualization system for interpreting large-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both the instance-and subset-level. ActiVis has been deployed on Facebook's machine learning platform. We present case studies with Facebook researchers and engineers, and usage scenarios of how ActiVis may work with different models.","AuthorNames-Deduped":"Minsuk Kahng;Pierre Y. Andrews;Aditya Kalro;Duen Horng Chau","AuthorNames":"Minsuk Kahng;Pierre Y. Andrews;Aditya Kalro;Duen Horng (Polo) Chau","AuthorAffiliation":"Georgia Institute of Technology;Facebook;Facebook;Georgia Institute of Technology","InternalReferences":"10.1109/VAST.2015.7347637;10.1109/VAST.2010.5652443;10.1109/TVCG.2013.157;10.1109/TVCG.2014.2346482;10.1109/TVCG.2015.2467622;10.1109/TVCG.2016.2598831;10.1109/TVCG.2016.2598838;10.1109/TVCG.2016.2598828;10.1109/VISUAL.2005.1532820;10.1109/VAST.2011.6102453","AuthorKeywords":"Visual analytics,deep learning,machine learning,information visualization","AminerCitationCount_02-2020":18,"AminerCitationCount_06-2020":68,"XploreCitationCount - 2020-01":32,"PubsCited":38,"Award":null,"image":"24tvcg01-kahng-2744718-fig-1-source-large.gif"},{"Conference":"VAST","Year":2017,"Title":"SkyLens: Visual Analysis of Skyline on Multi-Dimensional Data","DOI":"10.1109/TVCG.2017.2744738","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744738","FirstPage":"246","LastPage":"255","PaperType":"J","Abstract":"Skyline queries have wide-ranging applications in fields that involve multi-criteria decision making, including tourism, retail industry, and human resources. By automatically removing incompetent candidates, skyline queries allow users to focus on a subset of superior data items (i.e., the skyline), thus reducing the decision-making overhead. However, users are still required to interpret and compare these superior items manually before making a successful choice. This task is challenging because of two issues. First, people usually have fuzzy, unstable, and inconsistent preferences when presented with multiple candidates. Second, skyline queries do not reveal the reasons for the superiority of certain skyline points in a multi-dimensional space. To address these issues, we propose SkyLens, a visual analytic system aiming at revealing the superiority of skyline points from different perspectives and at different scales to aid users in their decision making. Two scenarios demonstrate the usefulness of SkyLens on two datasets with a dozen of attributes. A qualitative study is also conducted to show that users can efficiently accomplish skyline understanding and comparison tasks with SkyLens.","AuthorNames-Deduped":"Xun Zhao;Yanhong Wu;Weiwei Cui;Xinnan Du;Yuan Chen;Yong Wang 0021;Dik Lun Lee;Huamin Qu","AuthorNames":"Xun Zhao;Yanhong Wu;Weiwei Cui;Xinnan Du;Yuan Chen;Yong Wang;Dik Lun Lee;Huamin Qu","AuthorAffiliation":"Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Microsoft Research Asia;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology","InternalReferences":"10.1109/TVCG.2013.173;10.1109/TVCG.2016.2598432;10.1109/TVCG.2016.2598589;10.1109/TVCG.2015.2468011","AuthorKeywords":"Skyline query,skyline visualization,multi-dimensional data,visual analytics,multi-criteria decision making","AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":7,"XploreCitationCount - 2020-01":3,"PubsCited":49,"Award":null,"image":"24tvcg01-zhao-2744738-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"EVA: Visual Analytics to Identify Fraudulent Events","DOI":"10.1109/TVCG.2017.2744758","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744758","FirstPage":"330","LastPage":"339","PaperType":"J","Abstract":"Financial institutions are interested in ensuring security and quality for their customers. Banks, for instance, need to identify and stop harmful transactions in a timely manner. In order to detect fraudulent operations, data mining techniques and customer profile analysis are commonly used. However, these approaches are not supported by Visual Analytics techniques yet. Visual Analytics techniques have potential to considerably enhance the knowledge discovery process and increase the detection and prediction accuracy of financial fraud detection systems. Thus, we propose EVA, a Visual Analytics approach for supporting fraud investigation, fine-tuning fraud detection algorithms, and thus, reducing false positive alarms.","AuthorNames-Deduped":"Roger A. Leite;Theresia Gschwandtner;Silvia Miksch;Simone Kriglstein;Margit Pohl;Erich Gstrein;Johannes Kuntner","AuthorNames":"Roger A. Leite;Theresia Gschwandtner;Silvia Miksch;Simone Kriglstein;Margit Pohl;Erich Gstrein;Johannes Kuntner","AuthorAffiliation":"Vienna University of Technology;Vienna University of Technology;Vienna University of Technology;Faculty of Computer Science, University of Vienna, Austria;Vienna University of Technology;Erste Group IT International, Austria;Erste Group IT International, Austria","InternalReferences":"10.1109/VAST.2007.4389009;10.1109/TVCG.2013.126;10.1109/VAST.2015.7347678;10.1109/TVCG.2013.200;10.1109/TVCG.2009.111;10.1109/TVCG.2012.273","AuthorKeywords":"Visual Knowledge Discovery,Time Series Data,Business and Finance Visualization,Financial Fraud Detection","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":8,"XploreCitationCount - 2020-01":2,"PubsCited":36,"Award":null,"image":"24tvcg01-leite-2744758-fig-1-source-large.gif"},{"Conference":"VAST","Year":2017,"Title":"SOMFlow: Guided Exploratory Cluster Analysis with Self-Organizing Maps and Analytic Provenance","DOI":"10.1109/TVCG.2017.2744805","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744805","FirstPage":"120","LastPage":"130","PaperType":"J","Abstract":"Clustering is a core building block for data analysis, aiming to extract otherwise hidden structures and relations from raw datasets, such as particular groups that can be effectively related, compared, and interpreted. A plethora of visual-interactive cluster analysis techniques has been proposed to date, however, arriving at useful clusterings often requires several rounds of user interactions to fine-tune the data preprocessing and algorithms. We present a multi-stage Visual Analytics (VA) approach for iterative cluster refinement together with an implementation (SOMFlow) that uses Self-Organizing Maps (SOM) to analyze time series data. It supports exploration by offering the analyst a visual platform to analyze intermediate results, adapt the underlying computations, iteratively partition the data, and to reflect previous analytical activities. The history of previous decisions is explicitly visualized within a flow graph, allowing to compare earlier cluster refinements and to explore relations. We further leverage quality and interestingness measures to guide the analyst in the discovery of useful patterns, relations, and data partitions. We conducted two pair analytics experiments together with a subject matter expert in speech intonation research to demonstrate that the approach is effective for interactive data analysis, supporting enhanced understanding of clustering results as well as the interactive process itself.","AuthorNames-Deduped":"Dominik Sacha;Matthias Kraus;Jürgen Bernard;Michael Behrisch 0001;Tobias Schreck;Yuki Asano;Daniel A. Keim","AuthorNames":"Dominik Sacha;Matthias Kraus;Jürgen Bernard;Michael Behrisch;Tobias Schreck;Yuki Asano;Daniel A. Keim","AuthorAffiliation":"University of Konstanz, Germany;University of Konstanz, Germany;TU Darmstadt, Germany;University of Konstanz, Germany;Graz University of Technology;University of Tübingen;University of Konstanz, Germany","InternalReferences":"10.1109/VAST.2009.5332584;10.1109/VAST.2014.7042480;10.1109/TVCG.2013.178;10.1109/TVCG.2011.229;10.1109/TVCG.2011.188;10.1109/TVCG.2016.2598468;10.1109/VAST.2010.5652443;10.1109/VAST.2015.7347625;10.1109/VAST.2007.4389013;10.1109/TVCG.2014.2346260;10.1109/TVCG.2007.70582;10.1109/VAST.2007.4388999;10.1109/TVCG.2014.2346481;10.1109/TVCG.2016.2598495;10.1109/VAST.2011.6102453","AuthorKeywords":"Visual Analytics,Interaction,Visual Cluster Analysis,Quality Metrics,Guidance,Self-Organizing Maps,Time Series","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":14,"XploreCitationCount - 2020-01":7,"PubsCited":58,"Award":null,"image":"24tvcg01-sacha-2744805-fig-1-source-large.gif"},{"Conference":"VAST","Year":2017,"Title":"Comparing Visual-Interactive Labeling with Active Learning: An Experimental Study","DOI":"10.1109/TVCG.2017.2744818","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744818","FirstPage":"298","LastPage":"308","PaperType":"J","Abstract":"Labeling data instances is an important task in machine learning and visual analytics. Both fields provide a broad set of labeling strategies, whereby machine learning (and in particular active learning) follows a rather model-centered approach and visual analytics employs rather user-centered approaches (visual-interactive labeling). Both approaches have individual strengths and weaknesses. In this work, we conduct an experiment with three parts to assess and compare the performance of these different labeling strategies. In our study, we (1) identify different visual labeling strategies for user-centered labeling, (2) investigate strengths and weaknesses of labeling strategies for different labeling tasks and task complexities, and (3) shed light on the effect of using different visual encodings to guide the visual-interactive labeling process. We further compare labeling of single versus multiple instances at a time, and quantify the impact on efficiency. We systematically compare the performance of visual interactive labeling with that of active learning. Our main findings are that visual-interactive labeling can outperform active learning, given the condition that dimension reduction separates well the class distributions. Moreover, using dimension reduction in combination with additional visual encodings that expose the internal state of the learning model turns out to improve the performance of visual-interactive labeling.","AuthorNames-Deduped":"Jürgen Bernard;Marco Hutter 0002;Matthias Zeppelzauer;Dieter W. Fellner;Michael Sedlmair","AuthorNames":"Jürgen Bernard;Marco Hutter;Matthias Zeppelzauer;Dieter Fellner;Michael Sedlmair","AuthorAffiliation":"Technische Universität Darmstadt, Darmstadt, Germany;Technische Universität Darmstadt, Darmstadt, Germany;St. Pölten University of Applied Sciences, St. Pölten, Austria;Fraunhofer IGD, Darmstadt, Germany;University of Vienna, Vienna, Austria","InternalReferences":"10.1109/VAST.2014.7042480;10.1109/VAST.2012.6400486;10.1109/TVCG.2012.277;10.1109/VAST.2012.6400492;10.1109/VAST.2010.5652392;10.1109/TVCG.2014.2346482;10.1109/TVCG.2016.2598589;10.1109/TVCG.2016.2598495;10.1109/TVCG.2013.153;10.1109/TVCG.2015.2467717","AuthorKeywords":"Labeling,Visual-Interactive Labeling,Information Visualization,Visual Analytics,Active Learning,Machine Learning,Classification,Evaluation,Experiment,Dimensionality Reduction","AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":23,"XploreCitationCount - 2020-01":8,"PubsCited":72,"Award":null,"image":"24tvcg01-bernard-2744818-fig-1-source-large.gif"},{"Conference":"VAST","Year":2017,"Title":"Graphiti: Interactive Specification of Attribute-Based Edges for Network Modeling and Visualization","DOI":"10.1109/TVCG.2017.2744843","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744843","FirstPage":"226","LastPage":"235","PaperType":"J","Abstract":"Network visualizations, often in the form of node-link diagrams, are an effective means to understand relationships between entities, discover entities with interesting characteristics, and to identify clusters. While several existing tools allow users to visualize pre-defined networks, creating these networks from raw data remains a challenging task, often requiring users to program custom scripts or write complex SQL commands. Some existing tools also allow users to both visualize and model networks. Interaction techniques adopted by these tools often assume users know the exact conditions for defining edges in the resulting networks. This assumption may not always hold true, however. In cases where users do not know much about attributes in the dataset or when there are several attributes to choose from, users may not know which attributes they could use to formulate linking conditions. We propose an alternate interaction technique to model networks that allows users to demonstrate to the system a subset of nodes and links they wish to see in the resulting network. The system, in response, recommends conditions that can be used to model networks based on the specified nodes and links. In this paper, we show how such a demonstration-based interaction technique can be used to model networks by employing it in a prototype tool, Graphiti. Through multiple usage scenarios, we show how Graphiti not only allows users to model networks from a tabular dataset but also facilitates updating a pre-defined network with additional edge types.","AuthorNames-Deduped":"Arjun Srinivasan;Hyunwoo Park;Alex Endert;Rahul C. Basole","AuthorNames":"Arjun Srinivasan;Hyunwoo Park;Alex Endert;Rahul C. Basole","AuthorAffiliation":"Georgia Institute of Technology;The Ohio State University;Georgia Institute of Technology;Georgia Institute of Technology","InternalReferences":"10.1109/VAST.2006.261429;10.1109/TVCG.2011.185;10.1109/VAST.2011.6102441;10.1109/VAST.2006.261426;10.1109/TVCG.2009.151;10.1109/TVCG.2006.122;10.1109/TVCG.2016.2598839;10.1109/TVCG.2006.166;10.1109/TVCG.2009.108;10.1109/VAST.2010.5652520","AuthorKeywords":"Network modeling,visual analytics,user interaction","AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":1,"PubsCited":49,"Award":null,"image":"24tvcg01-srinivasan-2744843-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow","DOI":"10.1109/TVCG.2017.2744878","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744878","FirstPage":"1","LastPage":"12","PaperType":"J","Abstract":"We present a design study of the TensorFlow Graph Visualizer, part of the TensorFlow machine intelligence platform. This tool helps users understand complex machine learning architectures by visualizing their underlying dataflow graphs. The tool works by applying a series of graph transformations that enable standard layout techniques to produce a legible interactive diagram. To declutter the graph, we decouple non-critical nodes from the layout. To provide an overview, we build a clustered graph using the hierarchical structure annotated in the source code. To support exploration of nested structure on demand, we perform edge bundling to enable stable and responsive cluster expansion. Finally, we detect and highlight repeated structures to emphasize a model's modular composition. To demonstrate the utility of the visualizer, we describe example usage scenarios and report user feedback. Overall, users find the visualizer useful for understanding, debugging, and sharing the structures of their models.","AuthorNames-Deduped":"Kanit Wongsuphasawat;Daniel Smilkov;James Wexler;Jimbo Wilson;Dan Mané;Doug Fritz;Dilip Krishnan;Fernanda B. Viégas;Martin Wattenberg","AuthorNames":"Kanit Wongsuphasawat;Daniel Smilkov;James Wexler;Jimbo Wilson;Dandelion Mané;Doug Fritz;Dilip Krishnan;Fernanda B. Viégas;Martin Wattenberg","AuthorAffiliation":"Paul G. Allen School of Computer Science & EngineeringUniversity of Washington;Google Research;Google Research;Google Research;Google Research;Google Research;Google Research;Google Research;Google Research","InternalReferences":"10.1109/INFVIS.2005.1532130;10.1109/TVCG.2006.156;10.1109/INFVIS.2004.66;10.1109/TVCG.2015.2467451;10.1109/TVCG.2016.2598831;10.1109/VISUAL.2005.1532820;10.1109/INFVIS.2004.43;10.1109/TVCG.2015.2467251","AuthorKeywords":"Neural Network,Graph Visualization,Dataflow Graph,Clustered Graph","AminerCitationCount_02-2020":14,"AminerCitationCount_06-2020":78,"XploreCitationCount - 2020-01":33,"PubsCited":57,"Award":"BP","image":"24tvcg01-wongsuphasawat-2744878-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"VIGOR: Interactive Visual Exploration of Graph Query Results","DOI":"10.1109/TVCG.2017.2744898","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744898","FirstPage":"215","LastPage":"225","PaperType":"J","Abstract":"Finding patterns in graphs has become a vital challenge in many domains from biological systems, network security, to finance (e.g., finding money laundering rings of bankers and business owners). While there is significant interest in graph databases and querying techniques, less research has focused on helping analysts make sense of underlying patterns within a group of subgraph results. Visualizing graph query results is challenging, requiring effective summarization of a large number of subgraphs, each having potentially shared node-values, rich node features, and flexible structure across queries. We present VIGOR, a novel interactive visual analytics system, for exploring and making sense of query results. VIGOR uses multiple coordinated views, leveraging different data representations and organizations to streamline analysts sensemaking process. VIGOR contributes: (1) an exemplar-based interaction technique, where an analyst starts with a specific result and relaxes constraints to find other similar results or starts with only the structure (i.e., without node value constraints), and adds constraints to narrow in on specific results; and (2) a novel feature-aware subgraph result summarization. Through a collaboration with Symantec, we demonstrate how VIGOR helps tackle real-world problems through the discovery of security blindspots in a cybersecurity dataset with over 11,000 incidents. We also evaluate VIGOR with a within-subjects study, demonstrating VIGOR's ease of use over a leading graph database management system, and its ability to help analysts understand their results at higher speed and make fewer errors.","AuthorNames-Deduped":"Robert Pienta;Fred Hohman;Alex Endert;Acar Tamersoy;Kevin A. Roundy;Christopher Gates 0002;Shamkant B. Navathe;Duen Horng Chau","AuthorNames":"Robert Pienta;Fred Hohman;Alex Endert;Acar Tamersoy;Kevin Roundy;Chris Gates;Shamkant Navathe;Duen Horng Chau","AuthorAffiliation":null,"InternalReferences":"10.1109/TVCG.2015.2467717;10.1109/TVCG.2015.2468078","AuthorKeywords":"graph querying,subgraph results,query result visualization","AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":13,"XploreCitationCount - 2020-01":3,"PubsCited":49,"Award":null,"image":"24tvcg01-pienta-2744898-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"Analyzing the Training Processes of Deep Generative Models","DOI":"10.1109/TVCG.2017.2744938","Link":"http://dx.doi.org/10.1109/TVCG.2017.2744938","FirstPage":"77","LastPage":"87","PaperType":"J","Abstract":"Among the many types of deep models, deep generative models (DGMs) provide a solution to the important problem of unsupervised and semi-supervised learning. However, training DGMs requires more skill, experience, and know-how because their training is more complex than other types of deep models such as convolutional neural networks (CNNs). We develop a visual analytics approach for better understanding and diagnosing the training process of a DGM. To help experts understand the overall training process, we first extract a large amount of time series data that represents training dynamics (e.g., activation changes over time). A blue-noise polyline sampling scheme is then introduced to select time series samples, which can both preserve outliers and reduce visual clutter. To further investigate the root cause of a failed training process, we propose a credit assignment algorithm that indicates how other neurons contribute to the output of the neuron causing the training failure. Two case studies are conducted with machine learning experts to demonstrate how our approach helps understand and diagnose the training processes of DGMs. We also show how our approach can be directly used to analyze other types of deep models, such as CNNs.","AuthorNames-Deduped":"Mengchen Liu;Jiaxin Shi;Kelei Cao;Jun Zhu 0001;Shixia Liu","AuthorNames":"Mengchen Liu;Jiaxin Shi;Kelei Cao;Jun Zhu;Shixia Liu","AuthorAffiliation":"Tsinghua UniversityNational Engineering Lab for Big Data Software;Tsinghua University;Tsinghua UniversityNational Engineering Lab for Big Data Software;Tsinghua University;Tsinghua UniversityNational Engineering Lab for Big Data Software","InternalReferences":"10.1109/TVCG.2016.2598496;10.1109/TVCG.2014.2346594;10.1109/TVCG.2010.131;10.1109/TVCG.2011.239;10.1109/TVCG.2016.2598831;10.1109/TVCG.2016.2598797;10.1109/TVCG.2016.2598838;10.1109/TVCG.2016.2598829;10.1109/VISUAL.2005.1532820;10.1109/VAST.2016.7883511;10.1109/TVCG.2016.2598664","AuthorKeywords":"deep learning,deep generative models,blue noise sampling,credit assignment","AminerCitationCount_02-2020":6,"AminerCitationCount_06-2020":41,"XploreCitationCount - 2020-01":21,"PubsCited":55,"Award":null,"image":"24tvcg01-liu-2744938-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"Podium: Ranking Data Using Mixed-Initiative Visual Analytics","DOI":"10.1109/TVCG.2017.2745078","Link":"http://dx.doi.org/10.1109/TVCG.2017.2745078","FirstPage":"288","LastPage":"297","PaperType":"J","Abstract":"People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user's data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user's subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.","AuthorNames-Deduped":"Emily Wall;Subhajit Das;Ravish Chawla;Bharath Kalidindi;Eli T. Brown;Alex Endert","AuthorNames":"Emily Wall;Subhajit Das;Ravish Chawla;Bharath Kalidindi;Eli T. Brown;Alex Endert","AuthorAffiliation":"Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;DePaul University, Chicago, IL, USA;Georgia Institute of Technology, Atlanta, GA, USA","InternalReferences":"10.1109/INFVIS.2005.1532136;10.1109/VAST.2012.6400486;10.1109/TVCG.2014.2346575;10.1109/VAST.2015.7347625;10.1109/TVCG.2016.2598594;10.1109/VAST.2011.6102449;10.1109/TVCG.2013.173;10.1109/TVCG.2015.2467615;10.1109/TVCG.2016.2598446;10.1109/TVCG.2015.2467551;10.1109/TVCG.2016.2598839;10.1109/TVCG.2012.253;10.1109/VAST.2017.8585669","AuthorKeywords":"Mixed-initiative visual analytics,multi-attribute ranking,user interaction","AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":15,"XploreCitationCount - 2020-01":6,"PubsCited":48,"Award":null,"image":"24tvcg01-wall-2745078-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"Progressive Learning of Topic Modeling Parameters: A Visual Analytics Framework","DOI":"10.1109/TVCG.2017.2745080","Link":"http://dx.doi.org/10.1109/TVCG.2017.2745080","FirstPage":"382","LastPage":"391","PaperType":"J","Abstract":"Topic modeling algorithms are widely used to analyze the thematic composition of text corpora but remain difficult to interpret and adjust. Addressing these limitations, we present a modular visual analytics framework, tackling the understandability and adaptability of topic models through a user-driven reinforcement learning process which does not require a deep understanding of the underlying topic modeling algorithms. Given a document corpus, our approach initializes two algorithm configurations based on a parameter space analysis that enhances document separability. We abstract the model complexity in an interactive visual workspace for exploring the automatic matching results of two models, investigating topic summaries, analyzing parameter distributions, and reviewing documents. The main contribution of our work is an iterative decision-making technique in which users provide a document-based relevance feedback that allows the framework to converge to a user-endorsed topic distribution. We also report feedback from a two-stage study which shows that our technique results in topic model quality improvements on two independent measures.","AuthorNames-Deduped":"Mennatallah El-Assady;Rita Sevastjanova;Fabian Sperrle;Daniel A. Keim;Christopher Collins 0001","AuthorNames":"Mennatallah El-Assady;Rita Sevastjanova;Fabian Sperrle;Daniel Keim;Christopher Collins","AuthorAffiliation":"University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Ontario Institute of Technology, Canada","InternalReferences":"10.1109/TVCG.2015.2467618;10.1109/VAST.2014.7042493;10.1109/TVCG.2013.212;10.1109/VAST.2009.5333443;10.1109/TVCG.2011.239;10.1109/TVCG.2014.2346433;10.1109/TVCG.2010.129;10.1109/VAST.2011.6102461;10.1109/TVCG.2013.162;10.1109/TVCG.2013.126;10.1109/TVCG.2014.2346321","AuthorKeywords":"Topic Model Configuration,Reinforcement Learning,Feature Detection and Tracking,Iterative Optimization","AminerCitationCount_02-2020":5,"AminerCitationCount_06-2020":16,"XploreCitationCount - 2020-01":10,"PubsCited":43,"Award":"HM","image":"24tvcg01-elassady-2745080-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"Sequence Synopsis: Optimize Visual Summary of Temporal Event Data","DOI":"10.1109/TVCG.2017.2745083","Link":"http://dx.doi.org/10.1109/TVCG.2017.2745083","FirstPage":"45","LastPage":"55","PaperType":"J","Abstract":"Event sequences analysis plays an important role in many application domains such as customer behavior analysis, electronic health record analysis and vehicle fault diagnosis. Real-world event sequence data is often noisy and complex with high event cardinality, making it a challenging task to construct concise yet comprehensive overviews for such data. In this paper, we propose a novel visualization technique based on the minimum description length (MDL) principle to construct a coarse-level overview of event sequence data while balancing the information loss in it. The method addresses a fundamental trade-off in visualization design: reducing visual clutter vs. increasing the information content in a visualization. The method enables simultaneous sequence clustering and pattern extraction and is highly tolerant to noises such as missing or additional events in the data. Based on this approach we propose a visual analytics framework with multiple levels-of-detail to facilitate interactive data exploration. We demonstrate the usability and effectiveness of our approach through case studies with two real-world datasets. One dataset showcases a new application domain for event sequence visualization, i.e., fault development path analysis in vehicles for predictive maintenance. We also discuss the strengths and limitations of the proposed method based on user feedback.","AuthorNames-Deduped":"Yuanzhe Chen;Panpan Xu;Liu Ren","AuthorNames":"Yuanzhe Chen;Panpan Xu;Liu Ren","AuthorAffiliation":"Hong Kong University of Science and Technology;Bosch Research North America, Palo Alto, CA;Bosch Research North America, Palo Alto, CA","InternalReferences":"10.1109/VAST.2016.7883512;10.1109/TVCG.2013.214;10.1109/TVCG.2014.2346682;10.1109/TVCG.2015.2467622;10.1109/TVCG.2011.179;10.1109/TVCG.2016.2598797;10.1109/TVCG.2015.2467991;10.1109/TVCG.2013.200;10.1109/VAST.2015.7347682;10.1109/INFVIS.2000.885091;10.1109/TVCG.2016.2598591;10.1109/TVCG.2016.2598591;10.1109/TVCG.2009.117;10.1109/TVCG.2009.187;10.1109/VAST.2012.6400494;10.1109/TVCG.2012.225;10.1109/VAST.2009.5332595;10.1109/TVCG.2013.167","AuthorKeywords":"Time Series Data,Data Transformation and Representation,Visual Knowledge Representation,Visual Analytics","AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":23,"XploreCitationCount - 2020-01":15,"PubsCited":60,"Award":null,"image":"24tvcg01-xu-2745083-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"Clustervision: Visual Supervision of Unsupervised Clustering","DOI":"10.1109/TVCG.2017.2745085","Link":"http://dx.doi.org/10.1109/TVCG.2017.2745085","FirstPage":"142","LastPage":"151","PaperType":"J","Abstract":"Clustering, the process of grouping together similar items into distinct partitions, is a common type of unsupervised machine learning that can be useful for summarizing and aggregating complex multi-dimensional data. However, data can be clustered in many ways, and there exist a large body of algorithms designed to reveal different patterns. While having access to a wide variety of algorithms is helpful, in practice, it is quite difficult for data scientists to choose and parameterize algorithms to get the clustering results relevant for their dataset and analytical tasks. To alleviate this problem, we built Clustervision, a visual analytics tool that helps ensure data scientists find the right clustering among the large amount of techniques and parameters available. Our system clusters data using a variety of clustering techniques and parameters and then ranks clustering results utilizing five quality metrics. In addition, users can guide the system to produce more relevant results by providing task-relevant constraints on the data. Our visual user interface allows users to find high quality clustering results, explore the clusters using several coordinated visualization techniques, and select the cluster result that best suits their task. We demonstrate this novel approach using a case study with a team of researchers in the medical domain and showcase that our system empowers users to choose an effective representation of their complex data.","AuthorNames-Deduped":"Bum Chul Kwon;Benjamin Eysenbach;Janu Verma;Kenney Ng;Christopher deFilippi;Walter F. Stewart;Adam Perer","AuthorNames":"Bum Chul Kwon;Ben Eysenbach;Janu Verma;Kenney Ng;Christopher De Filippi;Walter F. Stewart;Adam Perer","AuthorAffiliation":"IBM T.J. Watson Research Center, NY, USA;Massachusetts Institute of Technology, Cambridge, MA, USA;IBM T.J. Watson Research Center, NY, USA;IBM T.J. Watson Research Center, NY, USA;Inova Heart and Vascular Institute, Fairfax, VA, USA;Sutter Health Research, Walnut Creek, California, USA;IBM T.J. Watson Research Center, NY, USA","InternalReferences":"10.1109/TVCG.2011.188;10.1109/TVCG.2014.2346321;10.1109/TVCG.2015.2467717","AuthorKeywords":"Unsupervised Clustering,Visual Analytics,Quality Metrics,Interactive Visual Clustering","AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":27,"XploreCitationCount - 2020-01":11,"PubsCited":46,"Award":null,"image":"24tvcg01-kwon-2745085-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"PhenoLines: Phenotype Comparison Visualizations for Disease Subtyping via Topic Models","DOI":"10.1109/TVCG.2017.2745118","Link":"http://dx.doi.org/10.1109/TVCG.2017.2745118","FirstPage":"371","LastPage":"381","PaperType":"J","Abstract":"PhenoLines is a visual analysis tool for the interpretation of disease subtypes, derived from the application of topic models to clinical data. Topic models enable one to mine cross-sectional patient comorbidity data (e.g., electronic health records) and construct disease subtypes-each with its own temporally evolving prevalence and co-occurrence of phenotypes-without requiring aligned longitudinal phenotype data for all patients. However, the dimensionality of topic models makes interpretation challenging, and de facto analyses provide little intuition regarding phenotype relevance or phenotype interrelationships. PhenoLines enables one to compare phenotype prevalence within and across disease subtype topics, thus supporting subtype characterization, a task that involves identifying a proposed subtype's dominant phenotypes, ages of effect, and clinical validity. We contribute a data transformation workflow that employs the Human Phenotype Ontology to hierarchically organize phenotypes and aggregate the evolving probabilities produced by topic models. We introduce a novel measure of phenotype relevance that can be used to simplify the resulting topology. The design of PhenoLines was motivated by formative interviews with machine learning and clinical experts. We describe the collaborative design process, distill high-level tasks, and report on initial evaluations with machine learning experts and a medical domain expert. These results suggest that PhenoLines demonstrates promising approaches to support the characterization and optimization of topic models.","AuthorNames-Deduped":"Michael Glueck;Mahdi Pakdaman Naeini;Finale Doshi-Velez;Fanny Chevalier;Azam Khan;Daniel J. Wigdor;Michael Brudno","AuthorNames":"Michael Glueck;Mahdi Pakdaman Naeini;Finale Doshi-Velez;Fanny Chevalier;Azam Khan;Daniel Wigdor;Michael Brudno","AuthorAffiliation":"Autodesk Research and University, Toronto;Harvard University;Harvard University;Inria;Autodesk Research;University of Toronto;Hospital for Sick ChildrenUniversity of Toronto","InternalReferences":"10.1109/TVCG.2011.185;10.1109/TVCG.2013.124;10.1109/TVCG.2009.140;10.1109/TVCG.2011.239;10.1109/TVCG.2014.2346433;10.1109/VAST.2011.6102461;10.1109/TVCG.2013.162;10.1109/TVCG.2016.2598469;10.1109/TVCG.2015.2467733;10.1109/TVCG.2015.2467622;10.1109/VAST.2014.7042494;10.1109/TVCG.2009.111;10.1109/TVCG.2012.213;10.1109/TVCG.2016.2598591","AuthorKeywords":"Developmental disorder,Human Phenotype Ontology (HPO),Phenotypes,Topic models,Topology simplification","AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":7,"XploreCitationCount - 2020-01":4,"PubsCited":57,"Award":null,"image":"24tvcg01-glueck-2745118-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"A Utility-Aware Visual Approach for Anonymizing Multi-Attribute Tabular Data","DOI":"10.1109/TVCG.2017.2745139","Link":"http://dx.doi.org/10.1109/TVCG.2017.2745139","FirstPage":"351","LastPage":"360","PaperType":"J","Abstract":"Sharing data for public usage requires sanitization to prevent sensitive information from leaking. Previous studies have presented methods for creating privacy preserving visualizations. However, few of them provide sufficient feedback to users on how much utility is reduced (or preserved) during such a process. To address this, we design a visual interface along with a data manipulation pipeline that allows users to gauge utility loss while interactively and iteratively handling privacy issues in their data. Widely known and discussed types of privacy models, i.e., syntactic anonymity and differential privacy, are integrated and compared under different use case scenarios. Case study results on a variety of examples demonstrate the effectiveness of our approach.","AuthorNames-Deduped":"Xu-Meng Wang;Jia-Kai Chou;Wei Chen 0001;Huihua Guan;Wenlong Chen;Tianyi Lao;Kwan-Liu Ma","AuthorNames":"Xumeng Wang;Jia-Kai Chou;Wei Chen;Huihua Guan;Wenlong Chen;Tianyi Lao;Kwan-Liu Ma","AuthorAffiliation":"Zhejiang University;University of California, Davis;Zhejiang University;Zhejiang University;Zhejiang University;Zhejiang University;University of California, Davis","InternalReferences":"10.1109/TVCG.2011.163;10.1109/TVCG.2015.2467671","AuthorKeywords":"Privacy preserving visualization,utility aware anonymization,syntactic anonymity,differential privacy","AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":8,"XploreCitationCount - 2020-01":3,"PubsCited":38,"Award":null,"image":"24tvcg01-wang-2745139-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"TreePOD: Sensitivity-Aware Selection of Pareto-Optimal Decision Trees","DOI":"10.1109/TVCG.2017.2745158","Link":"http://dx.doi.org/10.1109/TVCG.2017.2745158","FirstPage":"174","LastPage":"183","PaperType":"J","Abstract":"Balancing accuracy gains with other objectives such as interpretability is a key challenge when building decision trees. However, this process is difficult to automate because it involves know-how about the domain as well as the purpose of the model. This paper presents TreePOD, a new approach for sensitivity-aware model selection along trade-offs. TreePOD is based on exploring a large set of candidate trees generated by sampling the parameters of tree construction algorithms. Based on this set, visualizations of quantitative and qualitative tree aspects provide a comprehensive overview of possible tree characteristics. Along trade-offs between two objectives, TreePOD provides efficient selection guidance by focusing on Pareto-optimal tree candidates. TreePOD also conveys the sensitivities of tree characteristics on variations of selected parameters by extending the tree generation process with a full-factorial sampling. We demonstrate how TreePOD supports a variety of tasks involved in decision tree selection and describe its integration in a holistic workflow for building and selecting decision trees. For evaluation, we illustrate a case study for predicting critical power grid states, and we report qualitative feedback from domain experts in the energy sector. This feedback suggests that TreePOD enables users with and without statistical background a confident and efficient identification of suitable decision trees.","AuthorNames-Deduped":"Thomas Mühlbacher;Lorenz Linhardt;Torsten Möller;Harald Piringer","AuthorNames":"Thomas Mühlbacher;Lorenz Linhardt;Torsten Möller;Harald Piringer","AuthorAffiliation":"VRVis Research Center;ETH Zurich;University of Vienna;VRVis Research Center","InternalReferences":"10.1109/VAST.2011.6102457;10.1109/TVCG.2010.190;10.1109/TVCG.2008.145;10.1109/TVCG.2014.2346578;10.1109/TVCG.2016.2598589;10.1109/TVCG.2009.110;10.1109/TVCG.2014.2346321;10.1109/TVCG.2010.130;10.1109/TVCG.2011.248;10.1109/VAST.2011.6102453","AuthorKeywords":"Model selection,classification trees,visual parameter search,sensitivity analysis,Pareto optimality","AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":8,"XploreCitationCount - 2020-01":3,"PubsCited":51,"Award":null,"image":"24tvcg01-muhlbacher-2745158-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"Visualizing Confidence in Cluster-Based Ensemble Weather Forecast Analyses","DOI":"10.1109/TVCG.2017.2745178","Link":"http://dx.doi.org/10.1109/TVCG.2017.2745178","FirstPage":"109","LastPage":"119","PaperType":"J","Abstract":"In meteorology, cluster analysis is frequently used to determine representative trends in ensemble weather predictions in a selected spatio-temporal region, e.g., to reduce a set of ensemble members to simplify and improve their analysis. Identified clusters (i.e., groups of similar members), however, can be very sensitive to small changes of the selected region, so that clustering results can be misleading and bias subsequent analyses. In this article, we - a team of visualization scientists and meteorologists-deliver visual analytics solutions to analyze the sensitivity of clustering results with respect to changes of a selected region. We propose an interactive visual interface that enables simultaneous visualization of a) the variation in composition of identified clusters (i.e., their robustness), b) the variability in cluster membership for individual ensemble members, and c) the uncertainty in the spatial locations of identified trends. We demonstrate that our solution shows meteorologists how representative a clustering result is, and with respect to which changes in the selected region it becomes unstable. Furthermore, our solution helps to identify those ensemble members which stably belong to a given cluster and can thus be considered similar. In a real-world application case we show how our approach is used to analyze the clustering behavior of different regions in a forecast of “Tropical Cyclone Karl”, guiding the user towards the cluster robustness information required for subsequent ensemble analysis.","AuthorNames-Deduped":"Alexander Kumpf;Bianca Tost;Marlene Baumgart;Michael Riemer;Rüdiger Westermann;Marc Rautenhaus","AuthorNames":"Alexander Kumpf;Bianca Tost;Marlene Baumgart;Michael Riemer;Rüdiger Westermann;Marc Rautenhaus","AuthorAffiliation":"Computer Graphics & Visualization Group, Technische Universität München, Garching, Germany;Computer Graphics & Visualization Group, Technische Universität München, Garching, Germany;Institute of Atmospheric Physics, Johannes Gutenberg Universität Mainz, Mainz, Germany;Institute of Atmospheric Physics, Johannes Gutenberg Universität Mainz, Mainz, Germany;Computer Graphics & Visualization Group, Technische Universität München, Garching, Germany;Computer Graphics & Visualization Group, Technische Universität München, Garching, Germany","InternalReferences":"10.1109/TVCG.2014.2346626;10.1109/TVCG.2010.190;10.1109/TVCG.2006.168;10.1109/TVCG.2015.2467204;10.1109/TVCG.2016.2598868;10.1109/VISUAL.1999.809866;10.1109/INFVIS.2002.1173157;10.1109/TVCG.2013.141;10.1109/TVCG.2010.138;10.1109/TVCG.2006.170;10.1109/TVCG.2012.207;10.1109/TVCG.2013.177;10.1109/TVCG.2014.2346332;10.1109/TVCG.2013.143","AuthorKeywords":"Uncertainty visualization,ensemble visualization,clustering,meteorology","AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":17,"XploreCitationCount - 2020-01":8,"PubsCited":72,"Award":null,"image":"24tvcg01-kumpf-2745178-fig-1-source-large.gif"},{"Conference":"VAST","Year":2017,"Title":"Beyond Tasks: An Activity Typology for Visual Analytics","DOI":"10.1109/TVCG.2017.2745180","Link":"http://dx.doi.org/10.1109/TVCG.2017.2745180","FirstPage":"267","LastPage":"277","PaperType":"J","Abstract":"As Visual Analytics (VA) research grows and diversifies to encompass new systems, techniques, and use contexts, gaining a holistic view of analytic practices is becoming ever more challenging. However, such a view is essential for researchers and practitioners seeking to develop systems for broad audiences that span multiple domains. In this paper, we interpret VA research through the lens of Activity Theory (AT) - a framework for modelling human activities that has been influential in the field of Human-Computer Interaction. We first provide an overview of Activity Theory, showing its potential for thinking beyond tasks, representations, and interactions to the broader systems of activity in which interactive tools are embedded and used. Next, we describe how Activity Theory can be used as an organizing framework in the construction of activity typologies, building and expanding upon the tradition of abstract task taxonomies in the field of Information Visualization. We then apply the resulting process to create an activity typology for Visual Analytics, synthesizing a wide range of systems and activity concepts from the literature. Finally, we use this typology as the foundation of an activity-centered design process, highlighting both tensions and opportunities in the design space of VA systems.","AuthorNames-Deduped":"Darren Edge;Nathalie Henry Riche;Jonathan Larson;Christopher White","AuthorNames":"Darren Edge;Nathalie Henry Riche;Jonathan Larson;Christopher White","AuthorAffiliation":"Microsoft Research;Microsoft Research;Microsoft Research;Microsoft Research","InternalReferences":"10.1109/INFVIS.2005.1532136;10.1109/VAST.2008.4677362;10.1109/TVCG.2013.124;10.1109/VAST.2006.261439;10.1109/TVCG.2016.2598468;10.1109/INFVIS.2000.885092;10.1109/VAST.2006.261430;10.1109/VAST.2008.4677365;10.1109/VAST.2007.4389011;10.1109/VAST.2011.6102438;10.1109/VAST.2010.5653598;10.1109/TVCG.2014.2346573;10.1109/VAST.2008.4677366;10.1109/TVCG.2015.2467551;10.1109/TVCG.2012.213;10.1109/VAST.2007.4389006;10.1109/TVCG.2009.162;10.1109/TVCG.2007.70577;10.1109/TVCG.2016.2598543","AuthorKeywords":"Activity theory,visual analytics,activity-centered design,literature review,human-computer interaction","AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":1,"PubsCited":77,"Award":null,"image":"24tvcg01-edge-2745180-fig-1-source-large.gif"},{"Conference":"VAST","Year":2017,"Title":"Bring It to the Pitch: Combining Video and Movement Data to Enhance Team Sport Analysis","DOI":"10.1109/TVCG.2017.2745181","Link":"http://dx.doi.org/10.1109/TVCG.2017.2745181","FirstPage":"13","LastPage":"22","PaperType":"J","Abstract":"Analysts in professional team sport regularly perform analysis to gain strategic and tactical insights into player and team behavior. Goals of team sport analysis regularly include identification of weaknesses of opposing teams, or assessing performance and improvement potential of a coached team. Current analysis workflows are typically based on the analysis of team videos. Also, analysts can rely on techniques from Information Visualization, to depict e.g., player or ball trajectories. However, video analysis is typically a time-consuming process, where the analyst needs to memorize and annotate scenes. In contrast, visualization typically relies on an abstract data model, often using abstract visual mappings, and is not directly linked to the observed movement context anymore. We propose a visual analytics system that tightly integrates team sport video recordings with abstract visualization of underlying trajectory data. We apply appropriate computer vision techniques to extract trajectory data from video input. Furthermore, we apply advanced trajectory and movement analysis techniques to derive relevant team sport analytic measures for region, event and player analysis in the case of soccer analysis. Our system seamlessly integrates video and visualization modalities, enabling analysts to draw on the advantages of both analysis forms. Several expert studies conducted with team sport analysts indicate the effectiveness of our integrated approach.","AuthorNames-Deduped":"Manuel Stein;Halldór Janetzko;Andreas Lamprecht;Thorsten Breitkreutz;Philipp Zimmermann;Bastian Goldlücke;Tobias Schreck;Gennady L. Andrienko;Michael Grossniklaus;Daniel A. Keim","AuthorNames":"Manuel Stein;Halldor Janetzko;Andreas Lamprecht;Thorsten Breitkreutz;Philipp Zimmermann;Bastian Goldlücke;Tobias Schreck;Gennady Andrienko;Michael Grossniklaus;Daniel A. Keim","AuthorAffiliation":"University of Konstanz;University of Zürich;University of Konstanz;University of Konstanz;University of Konstanz;University of Konstanz;Graz University of Technology;Fraunhofer IAIS, Germany;University of Konstanz;University of Konstanz","InternalReferences":"10.1109/TVCG.2007.70521;10.1109/VISUAL.2003.1250401;10.1109/TVCG.2013.207;10.1109/TVCG.2012.263;10.1109/TVCG.2014.2346445;10.1109/VAST.2014.7042477","AuthorKeywords":"visual analytics,sport analytics,immersive analytics","AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":20,"XploreCitationCount - 2020-01":9,"PubsCited":49,"Award":null,"image":"24tvcg01-stein-2745181-fig-1-source-large.gif"},{"Conference":"VAST","Year":2017,"Title":"Towards a Systematic Combination of Dimension Reduction and Clustering in Visual Analytics","DOI":"10.1109/TVCG.2017.2745258","Link":"http://dx.doi.org/10.1109/TVCG.2017.2745258","FirstPage":"131","LastPage":"141","PaperType":"J","Abstract":"Dimension reduction algorithms and clustering algorithms are both frequently used techniques in visual analytics. Both families of algorithms assist analysts in performing related tasks regarding the similarity of observations and finding groups in datasets. Though initially used independently, recent works have incorporated algorithms from each family into the same visualization systems. However, these algorithmic combinations are often ad hoc or disconnected, working independently and in parallel rather than integrating some degree of interdependence. A number of design decisions must be addressed when employing dimension reduction and clustering algorithms concurrently in a visualization system, including the selection of each algorithm, the order in which they are processed, and how to present and interact with the resulting projection. This paper contributes an overview of combining dimension reduction and clustering into a visualization system, discussing the challenges inherent in developing a visualization system that makes use of both families of algorithms.","AuthorNames-Deduped":"John E. Wenskovitch;Ian Crandell;Naren Ramakrishnan;Leanna House;Scotland Leman;Chris North","AuthorNames":"John Wenskovitch;Ian Crandell;Naren Ramakrishnan;Leanna House;Scotland Leman;Chris North","AuthorAffiliation":"Virginia Tech Department of Computer Science;Virginia Tech Department of Statistics;Virginia Tech Department of Computer Science;Virginia Tech Department of Statistics;Virginia Tech Department of Statistics;Virginia Tech Department of Computer Science","InternalReferences":"10.1109/TVCG.2006.120;10.1109/TVCG.2011.186;10.1109/INFVIS.2005.1532136;10.1109/VAST.2014.7042492;10.1109/TVCG.2013.124;10.1109/VAST.2012.6400486;10.1109/TVCG.2014.2346594;10.1109/VAST.2009.5332629;10.1109/TVCG.2013.212;10.1109/TVCG.2009.122;10.1109/TVCG.2006.156;10.1109/VAST.2011.6102449;10.1109/INFVIS.2005.1532126;10.1109/TVCG.2013.188;10.1109/VAST.2012.6400487;10.1109/VAST.2007.4388999;10.1109/TVCG.2014.2346422;10.1109/TVCG.2011.178;10.1109/TVCG.2007.70515","AuthorKeywords":"Dimension reduction,clustering,algorithms,visual analytics","AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":15,"XploreCitationCount - 2020-01":4,"PubsCited":94,"Award":null,"image":"24tvcg01-wenskovitch-2745258-fig-1-source-large.gif"},{"Conference":"VAST","Year":2017,"Title":"Supporting Handoff in Asynchronous Collaborative Sensemaking Using Knowledge-Transfer Graphs","DOI":"10.1109/TVCG.2017.2745279","Link":"http://dx.doi.org/10.1109/TVCG.2017.2745279","FirstPage":"340","LastPage":"350","PaperType":"J","Abstract":"During asynchronous collaborative analysis, handoff of partial findings is challenging because externalizations produced by analysts may not adequately communicate their investigative process. To address this challenge, we developed techniques to automatically capture and help encode tacit aspects of the investigative process based on an analyst's interactions, and streamline explicit authoring of handoff annotations. We designed our techniques to mediate awareness of analysis coverage, support explicit communication of progress and uncertainty with annotation, and implicit communication through playback of investigation histories. To evaluate our techniques, we developed an interactive visual analysis system, KTGraph, that supports an asynchronous investigative document analysis task. We conducted a two-phase user study to characterize a set of handoff strategies and to compare investigative performance with and without our techniques. The results suggest that our techniques promote the use of more effective handoff strategies, help increase an awareness of prior investigative process and insights, as well as improve final investigative outcomes.","AuthorNames-Deduped":"Jian Zhao 0010;Michael Glueck;Petra Isenberg;Fanny Chevalier;Azam Khan","AuthorNames":"Jian Zhao;Michael Glueck;Petra Isenberg;Fanny Chevalier;Azam Khan","AuthorAffiliation":"FX Palo Alto Laboratory;Autodesk Research;Inria;Inria;Autodesk Research","InternalReferences":"10.1109/VAST.2007.4389009;10.1109/VAST.2011.6102447;10.1109/VAST.2010.5652932;10.1109/VAST.2006.261420;10.1109/VAST.2007.4389011;10.1109/TVCG.2008.137;10.1109/TVCG.2007.70568;10.1109/VAST.2009.5333020;10.1109/VAST.2009.5333878;10.1109/VAST.2011.6102438;10.1109/VAST.2006.261415;10.1109/TVCG.2014.2346573;10.1109/TVCG.2015.2467551;10.1109/VAST.2008.4677358;10.1109/TVCG.2016.2598466;10.1109/VAST.2007.4389006;10.1109/TVCG.2007.70577;10.1109/TVCG.2007.70589;10.1109/TVCG.2016.2598543","AuthorKeywords":"Collaboration,sensemaking,handoff,handover,structured externalizations,interactive visual analysis","AminerCitationCount_02-2020":4,"AminerCitationCount_06-2020":11,"XploreCitationCount - 2020-01":4,"PubsCited":55,"Award":"HM","image":"24tvcg01-zhao-2745279-fig-1-source-large.gif"},{"Conference":"VAST","Year":2017,"Title":"Dynamic Influence Networks for Rule-Based Models","DOI":"10.1109/TVCG.2017.2745280","Link":"http://dx.doi.org/10.1109/TVCG.2017.2745280","FirstPage":"184","LastPage":"194","PaperType":"J","Abstract":"We introduce the Dynamic Influence Network (DIN), a novel visual analytics technique for representing and analyzing rule-based models of protein-protein interaction networks. Rule-based modeling has proved instrumental in developing biological models that are concise, comprehensible, easily extensible, and that mitigate the combinatorial complexity of multi-state and multi-component biological molecules. Our technique visualizes the dynamics of these rules as they evolve over time. Using the data produced by KaSim, an open source stochastic simulator of rule-based models written in the Kappa language, DINs provide a node-link diagram that represents the influence that each rule has on the other rules. That is, rather than representing individual biological components or types, we instead represent the rules about them (as nodes) and the current influence of these rules (as links). Using our interactive DIN-Viz software tool, researchers are able to query this dynamic network to find meaningful patterns about biological processes, and to identify salient aspects of complex rule-based models. To evaluate the effectiveness of our approach, we investigate a simulation of a circadian clock model that illustrates the oscillatory behavior of the KaiC protein phosphorylation cycle.","AuthorNames-Deduped":"Angus G. Forbes;Andrew Thomas Burks;Kristine Lee;Xing Li;Pierre Boutillier;Jean Krivine;Walter Fontana","AuthorNames":"Angus G. Forbes;Andrew Burks;Kristine Lee;Xing Li;Pierre Boutillier;Jean Krivine;Walter Fontana","AuthorAffiliation":"University of California, Santa Cruz;University of Illinois, Chicago;University of Illinois, Chicago;University of Illinois, Chicago;Harvard Medical School;Universit&#x00E9; Paris Diderot;Harvard Medical School","InternalReferences":"10.1109/TVCG.2011.185;10.1109/TVCG.2010.126;10.1109/TVCG.2013.198;10.1109/TVCG.2007.70528;10.1109/TVCG.2013.154;10.1109/TVCG.2012.189","AuthorKeywords":"Dynamic networks,biological data visualization,rule-based modeling,protein-protein interaction networks","AminerCitationCount_02-2020":2,"AminerCitationCount_06-2020":9,"XploreCitationCount - 2020-01":2,"PubsCited":67,"Award":null,"image":"24tvcg01-forbes-2745280-fig-1-source-large.gif"},{"Conference":"VAST","Year":2017,"Title":"EventThread: Visual Summarization and Stage Analysis of Event Sequence Data","DOI":"10.1109/TVCG.2017.2745320","Link":"http://dx.doi.org/10.1109/TVCG.2017.2745320","FirstPage":"56","LastPage":"65","PaperType":"J","Abstract":"Event sequence data such as electronic health records, a person's academic records, or car service records, are ordered series of events which have occurred over a period of time. Analyzing collections of event sequences can reveal common or semantically important sequential patterns. For example, event sequence analysis might reveal frequently used care plans for treating a disease, typical publishing patterns of professors, and the patterns of service that result in a well-maintained car. It is challenging, however, to visually explore large numbers of event sequences, or sequences with large numbers of event types. Existing methods focus on extracting explicitly matching patterns of events using statistical analysis to create stages of event progression over time. However, these methods fail to capture latent clusters of similar but not identical evolutions of event sequences. In this paper, we introduce a novel visualization system named EventThread which clusters event sequences into threads based on tensor analysis and visualizes the latent stage categories and evolution patterns by interactively grouping the threads by similarity into time-specific clusters. We demonstrate the effectiveness of EventThread through usage scenarios in three different application domains and via interviews with an expert user.","AuthorNames-Deduped":"Shunan Guo;Ke Xu;Rongwen Zhao;David Gotz;Hongyuan Zha;Nan Cao","AuthorNames":"Shunan Guo;Ke Xu;Rongwen Zhao;David Gotz;Hongyuan Zha;Nan Cao","AuthorAffiliation":"East China Normal University;Hong Kong University of Science and Technology;iDV<sup>x</sup> LabTongji University;University of North Carolina, Chapel Hill;East China Normal University;iDV<sup>x</sup> LabTongji University","InternalReferences":"10.1109/TVCG.2011.188;10.1109/TVCG.2014.2346682;10.1109/INFVIS.2003.1249017;10.1109/TVCG.2011.179;10.1109/TVCG.2013.200","AuthorKeywords":"Visual Knowledge Representation,Visual Knowledge Discovery,Data Clustering,Time Series Data,Illustrative Visualization","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":14,"XploreCitationCount - 2020-01":9,"PubsCited":39,"Award":null,"image":"24tvcg01-guo-2745320-fig-1-source-large.gif"},{"Conference":"VAST","Year":2017,"Title":"CRICTO: Supporting Sensemaking through Crowdsourced Information Schematization","DOI":"10.1109/VAST.2017.8585484","Link":"http://dx.doi.org/10.1109/VAST.2017.8585484","FirstPage":139,"LastPage":150,"PaperType":"C","Abstract":"We present CRICTO, a new crowdsourcing visual analytics environment for making sense of and analyzing text data, whereby multiple crowdworkers are able to parallelize the simple information schematization tasks of relating and connecting entities across documents. The diverse links from these schematization tasks are then automatically combined and the system visualizes them based on the semantic types of the linkages. CRICTO also includes several tools that allow analysts to interactively explore and refine crowdworkers' results to better support their own sensemaking processes. We evaluated CRICTO's techniques and analysis workflow with deployments of CRICTO using Amazon Mechanical Turk and a user study that assess the effect of crowdsourced schematization in sensemaking tasks. The results of our evaluation show that CRICTO's crowdsourcing approaches and workflow help analysts explore diverse aspects of datasets, and uncover more accurate hidden stories embedded in the text datasets.","AuthorNames-Deduped":"Haeyong Chung;Sai Prashanth Dasari;Santhosh Nandhakumar;Christopher Andrews","AuthorNames":"Haeyong Chung;Sai Prashanth Dasari;Santhosh Nandhakumar;Christopher Andrews","AuthorAffiliation":"University of Alabama in Huntsville;University of Alabama in Huntsville;University of Alabama in Huntsville;Middlebury College","InternalReferences":"10.1109/VAST.2007.4389006;10.1109/TVCG.2013.164;10.1109/TVCG.2007.70577;10.1109/VAST.2009.5333878;10.1109/VAST.2008.4677362;10.1109/TVCG.2014.2346573;10.1109/VAST.2006.261439;10.1109/VAST.2010.5652932;10.1109/VAST.2007.4389011","AuthorKeywords":"Visual text analytics,sensemaking,crowdsourcing","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":1,"PubsCited":51,"Award":null,"image":"Chung1-139_150_chung-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"The y of it Matters, Even for Storyline Visualization","DOI":"10.1109/VAST.2017.8585487","Link":"http://dx.doi.org/10.1109/VAST.2017.8585487","FirstPage":81,"LastPage":91,"PaperType":"C","Abstract":"Storylines are adept at communicating complex change by encoding time on the x-axis and using the proximity of lines in the y direction to represent interaction between entities. The original definition of a storyline visualization requires data defined in terms of explicit interaction groups. Relaxing this definition allows storyline visualization to be applied more generally, but this creates questions about how the y-coordinate should encode interactions when this is tied to a particular place or state. To answer this question, we conducted a design study where we considered two layout algorithm design alternatives within a geo-temporal analysis tool written to solve part of the VAST Challenge 2014. We measured the performance of users at overview and detail oriented tasks between two storyline layout algorithms. To the best of our knowledge, this paper is the first work to question the design principles for storyline visualization, and what we found surprised us. For overview tasks with the alternative layout, which has a consistent encoding for the y-coordinate, users performed moderately better (p &lt;; .05) than the storyline layout based on existing design constraints and aesthetic criteria. Our empirical findings were also supported by first-hand accounts taken from interviews with multiple expert analysts, who suggested that the inconsistent meaning of the y-axis was misleading. These findings led us to design a new storyline layout algorithm that is a “best of both” where the y-axis has a consistent meaning but aesthetic criteria (e.g., line crossings) are considered.","AuthorNames-Deduped":"Dustin Arendt;Meg Pirrung","AuthorNames":"Dustin Arendt;Meg Pirrung","AuthorAffiliation":"Pacific Northwest National Laboratory;Pacific Northwest National Laboratory","InternalReferences":"10.1109/VAST.2009.5332593;10.1109/TVCG.2014.2346433;10.1109/TVCG.2013.196;10.1109/TVCG.2013.221;10.1109/TVCG.2012.212;10.1109/TVCG.2015.2468151","AuthorKeywords":"Storyline visualization,layout algorithms,interaction context,geospatial analysis,VAST Challenge","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":0,"PubsCited":38,"Award":null,"image":"arend1-081_091_arendt-large.gif"},{"Conference":"VAST","Year":2017,"Title":"The Role of Explicit Knowledge: A Conceptual Model of Knowledge-Assisted Visual Analytics","DOI":"10.1109/VAST.2017.8585498","Link":"http://dx.doi.org/10.1109/VAST.2017.8585498","FirstPage":92,"LastPage":103,"PaperType":"C","Abstract":"Visual Analytics (VA) aims to combine the strengths of humans and computers for effective data analysis. In this endeavor, humans' tacit knowledge from prior experience is an important asset that can be leveraged by both human and computer to improve the analytic process. While VA environments are starting to include features to formalize, store, and utilize such knowledge, the mechanisms and degree in which these environments integrate explicit knowledge varies widely. Additionally, this important class of VA environments has never been elaborated on by existing work on VA theory. This paper proposes a conceptual model of Knowledge-assisted VA conceptually grounded on the visualization model by van Wijk. We apply the model to describe various examples of knowledge-assisted VA from the literature and elaborate on three of them in finer detail. Moreover, we illustrate the utilization of the model to compare different design alternatives and to evaluate existing approaches with respect to their use of knowledge. Finally, the model can inspire designers to generate novel VA environments using explicit knowledge effectively.","AuthorNames-Deduped":"Paolo Federico 0001;Markus Wagner 0008;Alexander Rind;Albert Amor-Amoros;Silvia Miksch;Wolfgang Aigner","AuthorNames":"Paolo Federico;Markus Wagner;Alexander Rind;Albert Amor-Amorós;Silvia Miksch;Wolfgang Aigner","AuthorAffiliation":"TU Wien, Austria;St. Poelten University of Applied Sciences, Austria and TU Wien, Austria;St. Poelten University of Applied Sciences, Austria and TU Wien, Austria;TU Wien, Austria;TU Wien, Austria;TU Wien, Austria","InternalReferences":"10.1109/TVCG.2013.146;10.1109/TVCG.2014.2346575;10.1109/INFVIS.1997.636792;10.1109/TVCG.2016.2598468;10.1109/INFVIS.2000.885092;10.1109/INFVIS.1998.729560;10.1109/TVCG.2016.2598460;10.1109/TVCG.2016.2598471;10.1109/VAST.2008.4677352;10.1109/TVCG.2008.109;10.1109/VAST.2012.6400555;10.1109/VAST.2010.5654451;10.1109/TVCG.2014.2346481;10.1109/TVCG.2016.2598839;10.1109/VAST.2007.4389021;10.1109/TVCG.2014.2346574;10.1109/TVCG.2016.2598829;10.1109/VISUAL.2005.1532781","AuthorKeywords":"Automated analysis,tacit knowledge,explicit knowledge,visual analytics,information visualization,theory and model","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":7,"XploreCitationCount - 2020-01":3,"PubsCited":81,"Award":null,"image":"feder1-092_103_federico-large.gif"},{"Conference":"VAST","Year":2017,"Title":"Interactive Visual Alignment of Medieval Text Versions","DOI":"10.1109/VAST.2017.8585505","Link":"http://dx.doi.org/10.1109/VAST.2017.8585505","FirstPage":127,"LastPage":138,"PaperType":"C","Abstract":"Textual criticism consists of the identification and analysis of variant readings among different versions of a text. Being a relatively simple task for modern languages, the collation of medieval text traditions ranges from the complex to the virtually impossible depending on the degree of instability of textual transmission. We present a visual analytics environment that supports computationally aligning such complex textual differences typical of orally inflected medieval poetry. For the purpose of analyzing alignment, we provide interactive visualizations for different text hierarchy levels, specifically, a meso reading view to support investigating repetition and variance at the line level across text segments. In addition to outlining important aspects of our interdisciplinary collaboration, we emphasize the utility of the proposed system by various usage scenarios in medieval French literature.","AuthorNames-Deduped":"Stefan Jänicke;David Joseph Wrisley","AuthorNames":"Stefan Jänicke;David Joseph Wrisley","AuthorAffiliation":"Image and Signal Processing Group, Institute for Computer Science, Leipzig University, Germany;Digital Humanities, New York University Abu Dhabi, United Arab Emirates","InternalReferences":"10.1109/VAST.2014.7042493;10.1109/TVCG.2014.2346431;10.1109/VAST.2009.5333443;10.1109/VAST.2007.4389004;10.1109/VAST.2009.5333564;10.1109/VAST.2009.5333248","AuthorKeywords":null,"AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":4,"PubsCited":63,"Award":null,"image":"janic1-127_138_jaenicke-large.gif"},{"Conference":"VAST","Year":2017,"Title":"Visualizing Real-Time Strategy Games: The Example of StarCraft II","DOI":"10.1109/VAST.2017.8585594","Link":"http://dx.doi.org/10.1109/VAST.2017.8585594","FirstPage":71,"LastPage":80,"PaperType":"C","Abstract":"We present a visualization system for users to examine real-time strategy games, which have become very popular globally in recent years. Unlike previous systems that focus on showing statistics and build order, our system can depict the most important part - battles in the games. Specifically, we visualize detailed movements of armies belonging to respective nations on a map and enable users to examine battles from a global view to a local view. In the global view, battles are depicted by curved arrows revealing how the armies enter and escape from the battlefield. By observing the arrows and the height map, users can make sense of offensive and defensive strategies easily. In the local view, units of each type are rendered on the map, and their movements are represented by animation. We also render an attack line between a pair of units if one of them can attack the other to help users analyze the advantages and disadvantages of a particular formation. Accordingly, users can utilize our system to discover statistics, build order, and battles, and learn strategies from games played by professionals.","AuthorNames-Deduped":"Yen-Ting Kuan;Yu-Shuen Wang;Jung-Hong Chuang","AuthorNames":"Yen-Ting Kuan;Yu-Shuen Wang;Jung-Hong Chuang","AuthorAffiliation":"National Chiao Tung University;National Chiao Tung University;National Chiao Tung University","InternalReferences":"10.1109/INFVIS.2000.885098","AuthorKeywords":"real-time strategy games,StarCraft,game visualization,trajectories","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":1,"PubsCited":23,"Award":null,"image":"kuan1-071_080_kuan-large.gif"},{"Conference":"VAST","Year":2017,"Title":"Pattern Trails: Visual Analysis of Pattern Transitions in Subspaces","DOI":"10.1109/VAST.2017.8585613","Link":"http://dx.doi.org/10.1109/VAST.2017.8585613","FirstPage":1,"LastPage":12,"PaperType":"C","Abstract":"Subspace analysis methods have gained interest for identifying patterns in subspaces of high-dimensional data. Existing techniques allow to visualize and compare patterns in subspaces. However, many subspace analysis methods produce an abundant amount of patterns, which often remain redundant and are difficult to relate. Creating effective layouts for comparison of subspace patterns remains challenging. We introduce Pattern Trails, a novel approach for visually ordering and comparing subspace patterns. Central to our approach is the notion of pattern transitions as an interpretable structure imposed to order and compare patterns between subspaces. The basic idea is to visualize projections of subspaces side-by-side, and indicate changes between adjacent patterns in the subspaces by a linked representation, hence introducing pattern transitions. Our contributions comprise a systematization for how pairs of subspace patterns can be compared, and how changes can be interpreted in terms of pattern transitions. We also contribute a technique for visual subspace analysis based on a data-driven similarity measure between subspace representations. This measure is useful to order the patterns, and interactively group subspaces to reduce redundancy. We demonstrate the usefulness of our approach by application to several use cases, indicating that data can be meaningfully ordered and interpreted in terms of pattern transitions.","AuthorNames-Deduped":"Dominik Jäckle;Michael Blumenschein;Michael Behrisch 0001;Daniel A. Keim;Tobias Schreck","AuthorNames":"Dominik Jäckle;Michael Hund;Michael Behrisch;Daniel A. Keim;Tobias Schreck","AuthorAffiliation":"University of Konstanz;University of Konstanz;University of Konstanz;University of Konstanz;TU Graz","InternalReferences":"10.1109/VAST.2012.6400490;10.1109/TVCG.2011.188;10.1109/TVCG.2015.2467552;10.1109/TVCG.2010.184;10.1109/INFVIS.2005.1532141;10.1109/TVCG.2013.173;10.1109/VAST.2010.5652392;10.1109/VISUAL.1990.146402;10.1109/VISUAL.1995.485140;10.1109/TVCG.2014.2346482;10.1109/TVCG.2015.2467132;10.1109/TVCG.2014.2346481;10.1109/TVCG.2016.2598495;10.1109/TVCG.2013.153;10.1109/TVCG.2015.2467717;10.1109/VAST.2012.6400488;10.1109/TVCG.2011.178;10.1109/VISUAL.1995.485139;10.1109/TVCG.2009.179;10.1109/TVCG.2013.150","AuthorKeywords":null,"AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":0,"PubsCited":58,"Award":null,"image":"jackl1ab-001_012_jaeckle-large.gif"},{"Conference":"VAST","Year":2017,"Title":"E-Map: A Visual Analytics Approach for Exploring Significant Event Evolutions in Social Media","DOI":"10.1109/VAST.2017.8585638","Link":"http://dx.doi.org/10.1109/VAST.2017.8585638","FirstPage":36,"LastPage":47,"PaperType":"C","Abstract":"Significant events are often discussed and spread through social media, involving many people. Reposting activities and opinions expressed in social media offer good opportunities to understand the evolution of events. However, the dynamics of reposting activities and the diversity of user comments pose challenges to understand event-related social media data. We propose E-Map, a visual analytics approach that uses map-like visualization tools to help multi-faceted analysis of social media data on a significant event and in-depth understanding of the development of the event. E-Map transforms extracted keywords, messages, and reposting behaviors into map features such as cities, towns, and rivers to build a structured and semantic space for users to explore. It also visualizes complex posting and reposting behaviors as simple trajectories and connections that can be easily followed. By supporting multi-level spatial temporal exploration, E-Map helps to reveal the patterns of event development and key players in an event, disclosing the ways they shape and affect the development of the event. Two cases analysing real-world events confirm the capacities of E-Map in facilitating the analysis of event evolution with social media data.","AuthorNames-Deduped":"Siming Chen;Shuai Chen 0001;Lijing Lin;Xiaoru Yuan;Jie Liang 0004;Xiaolong Zhang 0001","AuthorNames":"Siming Chen;Shuai Chen;Lijing Lin;Xiaoru Yuan;Jie Liang;Xiaolong Zhang","AuthorAffiliation":"Key Laboratory of Machine Perception (Ministry of Education) and School of EECS, Peking University, China;Key Laboratory of Machine Perception (Ministry of Education) and School of EECS, Peking University, China;Key Laboratory of Machine Perception (Ministry of Education) and School of EECS, Peking University, China;Key Laboratory of Machine Perception (Ministry of Education) and School of EECS, Peking University, China;Faculty of Engineer and Information Technology, The University of Technology, Sydney, Australia;College of Information Sciences and Technology, Pennsylvania State University, USA","InternalReferences":"10.1109/VAST.2008.4677356;10.1109/TVCG.2013.186;10.1109/TVCG.2011.185;10.1109/TVCG.2012.291;10.1109/VAST.2012.6400557;10.1109/VAST.2016.7883510;10.1109/TVCG.2015.2467619;10.1109/TVCG.2014.2346433;10.1109/TVCG.2010.129;10.1109/VAST.2012.6400485;10.1109/TVCG.2013.162;10.1109/INFVIS.2005.1532126;10.1109/TVCG.2007.70582;10.1109/TVCG.2016.2598590;10.1109/TVCG.2015.2467554;10.1109/VAST.2015.7347632;10.1109/TVCG.2013.196;10.1109/VAST.2011.6102456;10.1109/TVCG.2016.2598919;10.1109/TVCG.2009.171;10.1109/VAST.2016.7883511;10.1109/TVCG.2015.2467691;10.1109/TVCG.2014.2346920;10.1109/TVCG.2013.221;10.1109/TVCG.2014.2346922;10.1109/VAST.2014.7042496","AuthorKeywords":"Social Media,Event Analysis,Map-like Visual Metaphor,Spatial Temporal Visual Analytics","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":6,"PubsCited":63,"Award":null,"image":"chen1abc-036_047_chensiming-large.gif"},{"Conference":"VAST","Year":2017,"Title":"A Visual Analytics System for Optimizing Communications in Massively Parallel Applications","DOI":"10.1109/VAST.2017.8585646","Link":"http://dx.doi.org/10.1109/VAST.2017.8585646","FirstPage":59,"LastPage":70,"PaperType":"C","Abstract":"Current and future supercomputers have tens of thousands of compute nodes interconnected with high-dimensional networks and complex network topologies for improved performance. Application developers are required to write scalable parallel programs in order to achieve high throughput on these machines. Application performance is largely determined by efficient inter-process communication. A common way to analyze and optimize performance is through profiling parallel codes to identify communication bottlenecks. However, understanding gigabytes of profiled at a is not a trivial task. In this paper, we present a visual analytics system for identifying the scalability bottlenecks and improving the communication efficiency of massively parallel applications. Visualization methods used in this system are designed to comprehend large-scale and varied communication patterns on thousands of nodes in complex networks such as the 5D torus and the dragonfly. We also present efficient rerouting and remapping algorithms that can be coupled with our interactive visual analytics design for performance optimization. We demonstrate the utility of our system with several case studies using three benchmark applications on two leading supercomputers. The mapping suggestion from our system led to 38% improvement in hop-bytes for Mini AMR application on 4,096 MPI processes.","AuthorNames-Deduped":"Takanori Fujiwara;Preeti Malakar;Khairi Reda;Venkatram Vishwanath;Michael E. Papka;Kwan-Liu Ma","AuthorNames":"Takanori Fujiwara;Preeti Malakar;Khairi Reda;Venkatram Vishwanath;Michael E. Papka;Kwan-Liu Ma","AuthorAffiliation":"University of California, Davis;Argonne National Laboratory;Indiana University-Purdue University Indianapolis;Argonne National Laboratory;Argonne National Laboratory, Northern Illinois University;University of California, Davis","InternalReferences":"10.1109/INFVIS.2004.1;10.1109/TVCG.2012.286;10.1109/TVCG.2014.2346441","AuthorKeywords":"Supercomputing,parallel communications,performance analysis,visual analytics,communication visualization","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":0,"PubsCited":83,"Award":null,"image":"fujiw1-059_070_fujiwara-large.gif"},{"Conference":"VAST","Year":2017,"Title":"Visual Causality Analysis Made Practical","DOI":"10.1109/VAST.2017.8585647","Link":"http://dx.doi.org/10.1109/VAST.2017.8585647","FirstPage":151,"LastPage":161,"PaperType":"C","Abstract":"Deriving the exact casual model that governs the relations between variables in a multidimensional dataset is difficult in practice. It is because causal inference algorithms by themselves typically cannot encode an adequate amount of domain knowledge to break all ties. Visual analytic approaches are considered a feasible alternative to fully automated methods. However, their application in real-world scenarios can be tedious. This paper focuses on these practical aspects of visual causality analysis. The most imperative of these aspects is posed by Simpson' Paradox. It implies the existence of multiple causal models differing in both structure and parameter depending on how the data is subdivided. We propose a comprehensive interface that engages human experts in identifying these subdivisions and allowing them to establish the corresponding causal models via a rich set of interactive facilities. Other features of our interface include: (1) a new causal network visualization that emphasizes the flow of causal dependencies, (2) a model scoring mechanism with visual hints for interactive model refinement, and (3) flexible approaches for handling heterogeneous data. Various real-world data examples are given.","AuthorNames-Deduped":"Jun Wang;Klaus Mueller","AuthorNames":"Jun Wang;Klaus Mueller","AuthorAffiliation":"Visual Analytics and Imaging Lab, Stony Brook University;Visual Analytics and Imaging Lab, Stony Brook University","InternalReferences":"10.1109/TVCG.2015.2467931;10.1109/TVCG.2012.225;10.1109/TVCG.2013.200;10.1109/TVCG.2016.2598797;10.1109/TVCG.2016.2598919;10.1109/TVCG.2016.2598543;10.1109/TVCG.2015.2467691","AuthorKeywords":"Visual knowledge discovery,Causality,Hypothesis testing,Visual evidence,High-dimensional data","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":49,"Award":null,"image":"wang1abcdef-151_161_wang-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"CrystalBall: A Visual Analytic System for Future Event Discovery and Analysis from Social Media Data","DOI":"10.1109/VAST.2017.8585658","Link":"http://dx.doi.org/10.1109/VAST.2017.8585658","FirstPage":25,"LastPage":35,"PaperType":"C","Abstract":"Social media data bear valuable insights regarding events that occur around the world. Events are inherently temporal and spatial. Existing visual text analysis systems have focused on detecting and analyzing past and ongoing events. Few have leveraged social media information to look for events that may occur in the future. In this paper, we present an interactive visual analytic system, CrystalBall, that automatically identifies and ranks future events from Twitter streams. CrystalBall integrates new methods to discover events with interactive visualizations that permit sensemaking of the identified future events. Our computational methods integrate seven different measures to identify and characterize future events, leveraging information regarding time, location, social networks, and the informativeness of the messages. A visual interface is tightly coupled with the computational methods to present a concise summary of the possible future events. A novel connection graph and glyphs are designed to visualize the characteristics of the future events. To demonstrate the efficacy of CrystalBall in identifying future events and supporting interactive analysis, we present multiple case studies and validation studies on analyzing events derived from Twitter data.","AuthorNames-Deduped":"Isaac Cho;Ryan Wesslen;Svitlana Volkova;William Ribarsky;Wenwen Dou","AuthorNames":"Isaac Cho;Ryan Wesslen;Svitlana Volkova;William Ribarsky;Wenwen Dou","AuthorAffiliation":"University of North Carolina at Charlotte;University of North Carolina at Charlotte;Pacific Northwest National Laboratory;University of North Carolina at Charlotte;University of North Carolina at Charlotte","InternalReferences":"10.1109/VAST.2012.6400485","AuthorKeywords":"Social media analysis,Event detection and analysis,visual analytics","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":6,"XploreCitationCount - 2020-01":1,"PubsCited":35,"Award":null,"image":"cho1abcde-025_035_cho-crystal-large.gif","hasViewed":true},{"Conference":"VAST","Year":2017,"Title":"The Anchoring Effect in Decision-Making with Visual Analytics","DOI":"10.1109/VAST.2017.8585665","Link":"http://dx.doi.org/10.1109/VAST.2017.8585665","FirstPage":116,"LastPage":126,"PaperType":"C","Abstract":"Anchoring effect is the tendency to focus too heavily on one piece of information when making decisions. In this paper, we present a novel, systematic study and resulting analyses that investigate the effects of anchoring effect on human decision-making using visual analytic systems. Visual analytics interfaces typically contain multiple views that present various aspects of information such as spatial, temporal, and categorical. These views are designed to present complex, heterogeneous data in accessible forms that aid decision-making. However, human decision-making is often hindered by the use of heuristics, or cognitive biases, such as anchoring effect. Anchoring effect can be triggered by the order in which information is presented or the magnitude of information presented. Through carefully designed laboratory experiments, we present evidence of anchoring effect in analysis with visual analytics interfaces when users are primed by representation of different pieces of information. We also describe detailed analyses of users' interaction logs which reveal the impact of anchoring bias on the visual representation preferred and paths of analysis. We discuss implications for future research to possibly detect and alleviate anchoring bias.","AuthorNames-Deduped":"Isaac Cho;Ryan Wesslen;Alireza Karduni;Sashank Santhanam;Samira Shaikh;Wenwen Dou","AuthorNames":"Isaac Cho;Ryan Wesslen;Alireza Karduni;Sashank Santhanam;Samira Shaikh;Wenwen Dou","AuthorAffiliation":"University of North Carolina at Charlotte;University of North Carolina at Charlotte;University of North Carolina at Charlotte;University of North Carolina at Charlotte;University of North Carolina at Charlotte;University of North Carolina at Charlotte","InternalReferences":"10.1109/TVCG.2015.2467971;10.1109/VAST.2017.8585658;10.1109/TVCG.2016.2598594;10.1109/VAST.2011.6102491;10.1109/TVCG.2009.152;10.1109/TVCG.2015.2467591","AuthorKeywords":"Visual Analytics,Anchoring Effect,Sense Making,Cognitive Bias,Interaction Log Analysis","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":10,"XploreCitationCount - 2020-01":4,"PubsCited":46,"Award":null,"image":"cho1-116_126_cho-anchoring-large.gif","hasViewed":false},{"Conference":"VAST","Year":2017,"Title":"Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics","DOI":"10.1109/VAST.2017.8585669","Link":"http://dx.doi.org/10.1109/VAST.2017.8585669","FirstPage":104,"LastPage":115,"PaperType":"C","Abstract":"Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models; however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.","AuthorNames-Deduped":"Emily Wall;Leslie M. Blaha;Lyndsey Franklin;Alex Endert","AuthorNames":"Emily Wall;Leslie M. Blaha;Lyndsey Franklin;Alex Endert","AuthorAffiliation":"Georgia Tech;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Georgia Tech","InternalReferences":"10.1109/VAST.2012.6400486;10.1109/TVCG.2014.2346575;10.1109/VAST.2015.7347625;10.1109/TVCG.2016.2598594;10.1109/VAST.2011.6102449;10.1109/TVCG.2016.2599058;10.1109/VAST.2008.4677365;10.1109/VAST.2008.4677361;10.1109/VISUAL.2000.885678;10.1109/TVCG.2015.2467615;10.1109/TVCG.2016.2598446;10.1109/TVCG.2012.273;10.1109/TVCG.2015.2467551;10.1109/TVCG.2015.2467591;10.1109/TVCG.2014.2346481;10.1109/TVCG.2016.2598466;10.1109/TVCG.2017.2745078;10.1109/TVCG.2007.70589;10.1109/TVCG.2007.70515","AuthorKeywords":"cognitive bias,visual analytics,human-in-the-loop,mixed initiative,user interaction,H.5.0 [Information Systems]: Human-Computer Interaction-General","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":23,"XploreCitationCount - 2020-01":8,"PubsCited":80,"Award":null,"image":"wall1-104_115_wall-large.gif"},{"Conference":"VAST","Year":2017,"Title":"A Workflow for Visual Diagnostics of Binary Classifiers using Instance-Level Explanations","DOI":"10.1109/VAST.2017.8585720","Link":"http://dx.doi.org/10.1109/VAST.2017.8585720","FirstPage":162,"LastPage":172,"PaperType":"C","Abstract":"Human-in-the-loop data analysis applications necessitate greater transparency in machine learning models for experts to understand and trust their decisions. To this end, we propose a visual analytics workflow to help data scientists and domain experts explore, diagnose, and understand the decisions made by a binary classifier. The approach leverages “instance-level explanations”, measures of local feature relevance that explain single instances, and uses them to build a set of visual representations that guide the users in their investigation. The workflow is based on three main visual representations and steps: one based on aggregate statistics to see how data distributes across correct / incorrect decisions; one based on explanations to understand which features are used to make these decisions; and one based on raw data, to derive insights on potential root causes for the observed patterns. The workflow is derived from a long-term collaboration with a group of machine learning and healthcare professionals who used our method to make sense of machine learning models they developed. The case study from this collaboration demonstrates that the proposed workflow helps experts derive useful knowledge about the model and the phenomena it describes, thus experts can generate useful hypotheses on how a model can be improved.","AuthorNames-Deduped":"Josua Krause;Aritra Dasgupta;Jordan Swartz;Yindalon Aphinyanagphongs;Enrico Bertini","AuthorNames":"Josua Krause;Aritra Dasgupta;Jordan Swartz;Yindalon Aphinyanaphongs;Enrico Bertini","AuthorAffiliation":"NYU Tandon School of Engineering;Pacific Northwest National Laboratory;NYU School of Medicine;NYU School of Medicine;NYU Tandon School of Engineering","InternalReferences":"10.1109/TVCG.2014.2346660;10.1109/TVCG.2016.2598544;10.1109/TVCG.2014.2346482;10.1109/TVCG.2016.2598828;10.1109/TVCG.2016.2598829","AuthorKeywords":"Machine Learning,Interpretation,Visual Analytics","AminerCitationCount_02-2020":3,"AminerCitationCount_06-2020":14,"XploreCitationCount - 2020-01":4,"PubsCited":33,"Award":null,"image":"kraus1-162_172_krause-large.gif"},{"Conference":"VAST","Year":2017,"Title":"Understanding Hidden Memories of Recurrent Neural Networks","DOI":"10.1109/VAST.2017.8585721","Link":"http://dx.doi.org/10.1109/VAST.2017.8585721","FirstPage":13,"LastPage":24,"PaperType":"C","Abstract":"Recurrent neural networks (RNNs) have been successfully applied to various natural language processing (NLP) tasks and achieved better results than conventional methods. However, the lack of understanding of the mechanisms behind their effectiveness limits further improvements on their architectures. In this paper, we present a visual analytics method for understanding and comparing RNN models for NLP tasks. We propose a technique to explain the function of individual hidden state units based on their expected response to input texts. We then co-cluster hidden state units and words based on the expected response and visualize co-clustering results as memory chips and word clouds to provide more structured knowledge on RNNs' hidden states. We also propose a glyph-based sequence visualization based on aggregate information to analyze the behavior of an RNN's hidden state at the sentence-level. The usability and effectiveness of our method are demonstrated through case studies and reviews from domain experts.","AuthorNames-Deduped":"Yao Ming;Shaozu Cao;Ruixiang Zhang;Zhen Li;Yuanzhe Chen;Yangqiu Song;Huamin Qu","AuthorNames":"Yao Ming;Shaozu Cao;Ruixiang Zhang;Zhen Li;Yuanzhe Chen;Yangqiu Song;Huamin Qu","AuthorAffiliation":"Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology","InternalReferences":"10.1109/VAST.2015.7347637;10.1109/VAST.2010.5652443;10.1109/TVCG.2012.252;10.1109/TVCG.2016.2598831;10.1109/TVCG.2011.212;10.1109/TVCG.2016.2598838;10.1109/TVCG.2016.2598838;10.1109/TVCG.2016.2598828;10.1109/TVCG.2016.2598465;10.1109/TVCG.2014.2346665","AuthorKeywords":"recurrent neural networks,visual analytics,understanding neural model,co-clustering","AminerCitationCount_02-2020":9,"AminerCitationCount_06-2020":20,"XploreCitationCount - 2020-01":17,"PubsCited":52,"Award":null,"image":"ming1-013_024_ming-large.gif"},{"Conference":"VAST","Year":2017,"Title":"QSAnglyzer: Visual Analytics for Prismatic Analysis of Question Answering System Evaluations","DOI":"10.1109/VAST.2017.8585733","Link":"http://dx.doi.org/10.1109/VAST.2017.8585733","FirstPage":48,"LastPage":58,"PaperType":"C","Abstract":"Developing sophisticated artificial intelligence (AI) systems requires AI researchers to experiment with different designs and analyze results from evaluations (we refer this task as evaluation analysis). In this paper, we tackle the challenges of evaluation analysis in the domain of question-answering (QA) systems. Through in-depth studies with QA researchers, we identify tasks and goals of evaluation analysis and derive a set of design rationales, based on which we propose a novel approach termed prismatic analysis. Prismatic analysis examines data through multiple ways of categorization (referred as angles). Categories in each angle are measured by aggregate metrics to enable diverse comparison scenarios. To facilitate prismatic analysis of QA evaluations, we design and implement the Question Space Anglyzer (QSAnglyzer), a visual analytics (VA) tool. In QSAnglyzer, the high-dimensional space formed by questions is divided into categories based on several angles (e.g., topic and question type). Each category is aggregated by accuracy, the number of questions, and accuracy variance across evaluations. QSAnglyzer visualizes these angles so that QA researchers can examine and compare evaluations from various aspects both individually and collectively. Furthermore, QA researchers filter questions based on any angle by clicking to construct complex queries. We validate QSAnglyzer through controlled experiments and by expert reviews. The results indicate that when using QSAnglyzer, users perform analysis tasks faster (p &lt;; 0.01) and more accurately (p &lt;; 0.05), and are quick to gain new insight. We discuss how prismatic analysis and QSAnglyzer scaffold evaluation analysis, and provide directions for future research.","AuthorNames-Deduped":"Nan-Chen Chen;Been Kim","AuthorNames":"Nan-Chen Chen;Been Kim","AuthorAffiliation":"University of Washington;Allen Institute for Artificial Intelligence","InternalReferences":"10.1109/VAST.2015.7347637;10.1109/INFVIS.2000.885091;10.1109/VAST.2012.6400488","AuthorKeywords":"visual analytics,visualization,interactive visualization,question answering,multi-experiment analysis,visual comparison,visual exploration,prismatic analysis,H.5.2 [Information Interfaces and Presentation]: User Interfaces—","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":0,"PubsCited":34,"Award":null,"image":"chen1-048_058_chen-large.gif"},{"Conference":"VAST","Year":2018,"Title":"iForest: Interpreting Random Forests via Visual Analytics","DOI":"10.1109/TVCG.2018.2864475","Link":"http://dx.doi.org/10.1109/TVCG.2018.2864475","FirstPage":407,"LastPage":416,"PaperType":"J","Abstract":"As an ensemble model that consists of many independent decision trees, random forests generate predictions by feeding the input to internal trees and summarizing their outputs. The ensemble nature of the model helps random forests outperform any individual decision tree. However, it also leads to a poor model interpretability, which significantly hinders the model from being used in fields that require transparent and explainable predictions, such as medical diagnosis and financial fraud detection. The interpretation challenges stem from the variety and complexity of the contained decision trees. Each decision tree has its unique structure and properties, such as the features used in the tree and the feature threshold in each tree node. Thus, a data input may lead to a variety of decision paths. To understand how a final prediction is achieved, it is desired to understand and compare all decision paths in the context of all tree structures, which is a huge challenge for any users. In this paper, we propose a visual analytic system aiming at interpreting random forest models and predictions. In addition to providing users with all the tree information, we summarize the decision paths in random forests, which eventually reflects the working mechanism of the model and reduces users' mental burden of interpretation. To demonstrate the effectiveness of our system, two usage scenarios and a qualitative user study are conducted.","AuthorNames-Deduped":"Xun Zhao;Yanhong Wu;Dik Lun Lee;Weiwei Cui","AuthorNames":"Xun Zhao;Yanhong Wu;Dik Lun Lee;Weiwei Cui","AuthorAffiliation":"Hong Kong University of Science and Technology;Visa Research;Hong Kong University of Science and Technology;Microsoft Research Asia","InternalReferences":"10.1109/TVCG.2017.2744378;10.1109/TVCG.2017.2745158;10.1109/VAST.2011.6102453","AuthorKeywords":"Interpretable Machine Learning,Random Forests,Random Forest Visualization,Visual Analytics","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":5,"XploreCitationCount - 2020-01":3,"PubsCited":60,"Award":null,"image":"25tvcg01-zhao-2864475-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2018,"Title":"Clustrophile 2: Guided Visual Clustering Analysis","DOI":"10.1109/TVCG.2018.2864477","Link":"http://dx.doi.org/10.1109/TVCG.2018.2864477","FirstPage":267,"LastPage":276,"PaperType":"J","Abstract":"Data clustering is a common unsupervised learning method frequently used in exploratory data analysis. However, identifying relevant structures in unlabeled, high-dimensional data is nontrivial, requiring iterative experimentation with clustering parameters as well as data features and instances. The number of possible clusterings for a typical dataset is vast, and navigating in this vast space is also challenging. The absence of ground-truth labels makes it impossible to define an optimal solution, thus requiring user judgment to establish what can be considered a satisfiable clustering result. Data scientists need adequate interactive tools to effectively explore and navigate the large clustering space so as to improve the effectiveness of exploratory clustering analysis. We introduce Clustrophile 2, a new interactive tool for guided clustering analysis. Clustrophile 2 guides users in clustering-based exploratory analysis, adapts user feedback to improve user guidance, facilitates the interpretation of clusters, and helps quickly reason about differences between clusterings. To this end, Clustrophile 2 contributes a novel feature, the Clustering Tour, to help users choose clustering parameters and assess the quality of different clustering results in relation to current analysis goals and user expectations. We evaluate Clustrophile 2 through a user study with 12 data scientists, who used our tool to explore and interpret sub-cohorts in a dataset of Parkinson's disease patients. Results suggest that Clustrophile 2 improves the speed and effectiveness of exploratory clustering analysis for both experts and non-experts.","AuthorNames-Deduped":"Marco Cavallo;Çagatay Demiralp","AuthorNames":"Marco Cavallo;Çağatay Demiralp","AuthorAffiliation":"IBM Research;MIT CSAIL & Fitnescity Labs","InternalReferences":"10.1109/TVCG.2011.188;10.1109/TVCG.2013.119;10.1109/TVCG.2012.219;10.1109/TVCG.2017.2745085;10.1109/TVCG.2010.138;10.1109/VAST.2007.4388999;10.1109/TVCG.2012.207;10.1109/TVCG.2017.2744805;10.1109/VAST.2008.4677350;10.1109/INFVIS.2004.3;10.1109/TVCG.2015.2467191","AuthorKeywords":"Clustering tour,Guided data analysis,Unsupervised learning,Exploratory data analysis,Interactive clustering analysis,Interpretability,Explainability,Visual data exploration recommendation,Dimensionality reduction,What-if analysis,Clustrophile","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":11,"XploreCitationCount - 2020-01":3,"PubsCited":46,"Award":null,"image":"25tvcg01-cavallo-2864477-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2018,"Title":"Manifold: A Model-Agnostic Framework for Interpretation and Diagnosis of Machine Learning Models","DOI":"10.1109/TVCG.2018.2864499","Link":"http://dx.doi.org/10.1109/TVCG.2018.2864499","FirstPage":364,"LastPage":373,"PaperType":"J","Abstract":"Interpretation and diagnosis of machine learning models have gained renewed interest in recent years with breakthroughs in new approaches. We present Manifold, a framework that utilizes visual analysis techniques to support interpretation, debugging, and comparison of machine learning models in a more transparent and interactive manner. Conventional techniques usually focus on visualizing the internal logic of a specific model type (i.e., deep neural networks), lacking the ability to extend to a more complex scenario where different model types are integrated. To this end, Manifold is designed as a generic framework that does not rely on or access the internal logic of the model and solely observes the input (i.e., instances or features) and the output (i.e., the predicted result and probability distribution). We describe the workflow of Manifold as an iterative process consisting of three major phases that are commonly involved in the model development and diagnosis process: inspection (hypothesis), explanation (reasoning), and refinement (verification). The visual components supporting these tasks include a scatterplot-based visual summary that overviews the models' outcome and a customizable tabular view that reveals feature discrimination. We demonstrate current applications of the framework on the classification and regression tasks and discuss other potential machine learning use scenarios where Manifold can be applied.","AuthorNames-Deduped":"Jiawei Zhang;Yang Wang;Piero Molino;Lezhi Li;David S. Ebert","AuthorNames":"Jiawei Zhang;Yang Wang;Piero Molino;Lezhi Li;David S. Ebert","AuthorAffiliation":"Purdue University;Uber Technologies, Inc;Uber AI Labs;Uber Technologies, Inc;Purdue University","InternalReferences":"10.1109/TVCG.2014.2346660;10.1109/VAST.2015.7347637;10.1109/TVCG.2014.2346594;10.1109/TVCG.2013.212;10.1109/TVCG.2017.2744718;10.1109/TVCG.2017.2744938;10.1109/TVCG.2017.2744378;10.1109/TVCG.2013.125;10.1109/TVCG.2014.2346578;10.1109/TVCG.2009.111;10.1109/TVCG.2016.2598838;10.1109/TVCG.2016.2598828;10.1109/INFVIS.2000.885086;10.1109/TVCG.2017.2744158;10.1109/TVCG.2016.2598829;10.1109/TVCG.2017.2744878","AuthorKeywords":"Interactive machine learning,performance analysis,model comparison,model debugging","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":14,"XploreCitationCount - 2020-01":9,"PubsCited":43,"Award":null,"image":"25tvcg01-zhang-2864499-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation","DOI":"10.1109/TVCG.2018.2864500","Link":"http://dx.doi.org/10.1109/TVCG.2018.2864500","FirstPage":310,"LastPage":320,"PaperType":"J","Abstract":"Recent success in deep learning has generated immense interest among practitioners and students, inspiring many to learn about this new technology. While visual and interactive approaches have been successfully developed to help people more easily learn deep learning, most existing tools focus on simpler models. In this work, we present GAN Lab, the first interactive visualization tool designed for non-experts to learn and experiment with Generative Adversarial Networks (GANs), a popular class of complex deep learning models. With GAN Lab, users can interactively train generative models and visualize the dynamic training process's intermediate results. GAN Lab tightly integrates an model overview graph that summarizes GAN's structure, and a layered distributions view that helps users interpret the interplay between submodels. GAN Lab introduces new interactive experimentation features for learning complex deep learning models, such as step-by-step training at multiple levels of abstraction for understanding intricate training dynamics. Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web browsers, without the need for installation or specialized hardware, overcoming a major practical challenge in deploying interactive tools for deep learning.","AuthorNames-Deduped":"Minsuk Kahng;Nikhil Thorat;Duen Horng Chau;Fernanda B. Viégas;Martin Wattenberg","AuthorNames":"Minsuk Kahng;Nikhil Thorat;Duen Horng (Polo) Chau;Fernanda B. Viégas;Martin Wattenberg","AuthorAffiliation":"Georgia Institute of Technology;Google Brain;Georgia Institute of Technology;Google Brain;Google Brain","InternalReferences":"10.1109/TVCG.2008.119;10.1109/TVCG.2017.2744683;10.1109/TVCG.2017.2744718;10.1109/TVCG.2017.2744938;10.1109/TVCG.2016.2598831;10.1109/TVCG.2010.177;10.1109/VAST.2017.8585721;10.1109/TVCG.2017.2744358;10.1109/TVCG.2017.2744158;10.1109/TVCG.2017.2744878","AuthorKeywords":"Deep learning,information visualization,visual analytics,generative adversarial networks,machine learning,interactive experimentation,explorable explanations","AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":16,"XploreCitationCount - 2020-01":6,"PubsCited":44,"Award":null,"image":"25tvcg01-kahng-2864500-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"Visual Abstraction of Large Scale Geospatial Origin-Destination Movement Data","DOI":"10.1109/TVCG.2018.2864503","Link":"http://dx.doi.org/10.1109/TVCG.2018.2864503","FirstPage":43,"LastPage":53,"PaperType":"J","Abstract":"A variety of human movement datasets are represented in an Origin-Destination(OD) form, such as taxi trips, mobile phone locations, etc. As a commonly-used method to visualize OD data, flow map always fails to discover patterns of human mobility, due to massive intersections and occlusions of lines on a 2D geographical map. A large number of techniques have been proposed to reduce visual clutter of flow maps, such as filtering, clustering and edge bundling, but the correlations of OD flows are often neglected, which makes the simplified OD flow map present little semantic information. In this paper, a characterization of OD flows is established based on an analogy between OD flows and natural language processing (NPL) terms. Then, an iterative multi-objective sampling scheme is designed to select OD flows in a vectorized representation space. To enhance the readability of sampled OD flows, a set of meaningful visual encodings are designed to present the interactions of OD flows. We design and implement a visual exploration system that supports visual inspection and quantitative evaluation from a variety of perspectives. Case studies based on real-world datasets and interviews with domain experts have demonstrated the effectiveness of our system in reducing the visual clutter and enhancing correlations of OD flows.","AuthorNames-Deduped":"Zhiguang Zhou;Linhao Meng;Cheng Tang;Ying Zhao 0001;Zhiyong Guo;Miaoxin Hu;Wei Chen 0001","AuthorNames":"Zhiguang Zhou;Linhao Meng;Cheng Tang;Ying Zhao;Zhiyong Guo;Miaoxin Hu;Wei Chen","AuthorAffiliation":"School of InformationZhejiang University of Finance and Economics;State Key Lab of CAD & CGZhejiang University;Information SchoolZhejiang Sci-tech University;Central South University;School of InformationZhejiang University of Finance and Economics;School of InformationZhejiang University of Finance and Economics;State Key Lab of CAD & CGZhejiang University","InternalReferences":"10.1109/TVCG.2017.2744322;10.1109/TVCG.2016.2598667;10.1109/TVCG.2011.202;10.1109/TVCG.2014.2346594;10.1109/TVCG.2008.135;10.1109/TVCG.2011.233;10.1109/TVCG.2013.226;10.1109/TVCG.2009.143;10.1109/TVCG.2014.2346271;10.1109/TVCG.2016.2598432;10.1109/TVCG.2013.196;10.1109/INFVIS.2005.1532150;10.1109/TVCG.2015.2467691;10.1109/TVCG.2014.2346746;10.1109/TVCG.2016.2598885","AuthorKeywords":"Visual abstraction,human mobility,origin-destination,flow map,representation learning","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":12,"XploreCitationCount - 2020-01":12,"PubsCited":51,"Award":null,"image":"25tvcg01-chen-2864503-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2018,"Title":"DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks","DOI":"10.1109/TVCG.2018.2864504","Link":"http://dx.doi.org/10.1109/TVCG.2018.2864504","FirstPage":288,"LastPage":298,"PaperType":"J","Abstract":"Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand, diagnose, and potentially improve DQN models.","AuthorNames-Deduped":"Junpeng Wang;Liang Gou;Han-Wei Shen;Hao Yang","AuthorNames":"Junpeng Wang;Liang Gou;Han-Wei Shen;Hao Yang","AuthorAffiliation":"The Ohio State University;Visa Research;The Ohio State University;Visa Research","InternalReferences":"10.1109/TVCG.2017.2744683;10.1109/TVCG.2014.2346682;10.1109/TVCG.2017.2745320;10.1109/TVCG.2017.2744718;10.1109/TVCG.2011.179;10.1109/TVCG.2017.2744938;10.1109/TVCG.2016.2598831;10.1109/VAST.2017.8585721;10.1109/TVCG.2013.200;10.1109/TVCG.2017.2744358;10.1109/TVCG.2017.2744158","AuthorKeywords":"Deep Q-Network (DQN),reinforcement learning,model interpretation,visual analytics","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":11,"XploreCitationCount - 2020-01":8,"PubsCited":55,"Award":"HM","image":"25tvcg01-wang-2864504-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2018,"Title":"Duet: Helping Data Analysis Novices Conduct Pairwise Comparisons by Minimal Specification","DOI":"10.1109/TVCG.2018.2864526","Link":"http://dx.doi.org/10.1109/TVCG.2018.2864526","FirstPage":427,"LastPage":437,"PaperType":"J","Abstract":"Data analysis novices often encounter barriers in executing low-level operations for pairwise comparisons. They may also run into barriers in interpreting the artifacts (e.g., visualizations) created as a result of the operations. We developed Duet, a visual analysis system designed to help data analysis novices conduct pairwise comparisons by addressing execution and interpretation barriers. To reduce the barriers in executing low-level operations during pairwise comparison, Duet employs minimal specification: when one object group (i.e. a group of records in a data table) is specified, Duet recommends object groups that are similar to or different from the specified one; when two object groups are specified, Duet recommends similar and different attributes between them. To lower the barriers in interpreting its recommendations, Duet explains the recommended groups and attributes using both visualizations and textual descriptions. We conducted a qualitative evaluation with eight participants to understand the effectiveness of Duet. The results suggest that minimal specification is easy to use and Duet's explanations are helpful for interpreting the recommendations despite some usability issues.","AuthorNames-Deduped":"Po-Ming Law;Rahul C. Basole;Yanhong Wu","AuthorNames":"Po-Ming Law;Rahul C. Basole;Yanhong Wu","AuthorAffiliation":"Georgia Institute of Technology;Georgia Institute of Technology;Visa Research","InternalReferences":"10.1109/TVCG.2011.188;10.1109/TVCG.2016.2598468;10.1109/VAST.2011.6102435;10.1109/TVCG.2017.2744199;10.1109/TVCG.2010.164;10.1109/TVCG.2017.2744684;10.1109/TVCG.2008.109;10.1109/TVCG.2015.2467195;10.1109/TVCG.2017.2745219;10.1109/TVCG.2015.2467191","AuthorKeywords":"Pairwise comparison,novices,data analysis,automatic insight generation","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":2,"PubsCited":51,"Award":null,"image":"25tvcg01-law-2864526-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2018,"Title":"Visual Analytics for Topic Model Optimization based on User-Steerable Speculative Execution","DOI":"10.1109/TVCG.2018.2864769","Link":"http://dx.doi.org/10.1109/TVCG.2018.2864769","FirstPage":374,"LastPage":384,"PaperType":"J","Abstract":"To effectively assess the potential consequences of human interventions in model-driven analytics systems, we establish the concept of speculative execution as a visual analytics paradigm for creating user-steerable preview mechanisms. This paper presents an explainable, mixed-initiative topic modeling framework that integrates speculative execution into the algorithmic decision-making process. Our approach visualizes the model-space of our novel incremental hierarchical topic modeling algorithm, unveiling its inner-workings. We support the active incorporation of the user's domain knowledge in every step through explicit model manipulation interactions. In addition, users can initialize the model with expected topic seeds, the backbone priors. For a more targeted optimization, the modeling process automatically triggers a speculative execution of various optimization strategies, and requests feedback whenever the measured model quality deteriorates. Users compare the proposed optimizations to the current model state and preview their effect on the next model iterations, before applying one of them. This supervised human-in-the-Ioop process targets maximum improvement for minimum feedback and has proven to be effective in three independent studies that confirm topic model quality improvements.","AuthorNames-Deduped":"Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel A. Keim;Christopher Collins 0001","AuthorNames":"Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel Keim;Christopher Collins","AuthorAffiliation":"University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Ontario Institute of Technology, Canada","InternalReferences":"10.1109/VAST.2014.7042493;10.1109/VAST.2011.6102461;10.1109/TVCG.2013.162;10.1109/TVCG.2017.2745080;10.1109/TVCG.2017.2744199;10.1109/TVCG.2017.2743959;10.1109/TVCG.2013.231;10.1109/TVCG.2013.212;10.1109/TVCG.2016.2598445;10.1109/TVCG.2014.2346578;10.1109/TVCG.2013.232","AuthorKeywords":"User-Steerable Topic Modeling,Speculative Execution,Mixed-Initiative Visual Analytics,Explainable Machine Learning","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":8,"XploreCitationCount - 2020-01":4,"PubsCited":69,"Award":null,"image":"25tvcg01-elassady-2864769-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"Analysis of Flight Variability: a Systematic Approach","DOI":"10.1109/TVCG.2018.2864811","Link":"http://dx.doi.org/10.1109/TVCG.2018.2864811","FirstPage":54,"LastPage":64,"PaperType":"J","Abstract":"In movement data analysis, there exists a problem of comparing multiple trajectories of moving objects to common or distinct reference trajectories. We introduce a general conceptual framework for comparative analysis of trajectories and an analytical procedure, which consists of (1) finding corresponding points in pairs of trajectories, (2) computation of pairwise difference measures, and (3) interactive visual analysis of the distributions of the differences with respect to space, time, set of moving objects, trajectory structures, and spatio-temporal context. We propose a combination of visualisation, interaction, and data transformation techniques supporting the analysis and demonstrate the use of our approach for solving a challenging problem from the aviation domain.","AuthorNames-Deduped":"Natalia V. Andrienko;Gennady L. Andrienko;Jose Manuel Cordero Garcia;David Scarlatti","AuthorNames":"Natalia Andrienko;Gennady Andrienko;Jose Manuel Cordero Garcia;David Scarlatti","AuthorAffiliation":"Fraunhofer IAISCity, University of London;Fraunhofer IAISCity, University of London;CRIDA (Reference Center for Research, Development and Innovation in ATM);Boeing Research & Development Europe","InternalReferences":"10.1109/VAST.2008.4677356;10.1109/VAST.2010.5653580;10.1109/TVCG.2017.2744322;10.1109/TVCG.2013.193;10.1109/TVCG.2015.2467851;10.1109/TVCG.2011.233;10.1109/TVCG.2012.265;10.1109/TVCG.2015.2468078","AuthorKeywords":"Visual analytics,movement data,flight trajectories","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":5,"XploreCitationCount - 2020-01":2,"PubsCited":55,"Award":null,"image":"25tvcg01-andrienko-2864811-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"RuleMatrix: Visualizing and Understanding Classifiers with Rules","DOI":"10.1109/TVCG.2018.2864812","Link":"http://dx.doi.org/10.1109/TVCG.2018.2864812","FirstPage":342,"LastPage":352,"PaperType":"J","Abstract":"With the growing adoption of machine learning techniques, there is a surge of research interest towards making machine learning systems more transparent and interpretable. Various visualizations have been developed to help model developers understand, diagnose, and refine machine learning models. However, a large number of potential but neglected users are the domain experts with little knowledge of machine learning but are expected to work with machine learning systems. In this paper, we present an interactive visualization technique to help users with little expertise in machine learning to understand, explore and validate predictive models. By viewing the model as a black box, we extract a standardized rule-based knowledge representation from its input-output behavior. Then, we design RuleMatrix, a matrix-based visualization of rules to help users navigate and verify the rules and the black-box model. We evaluate the effectiveness of RuleMatrix via two use cases and a usability study.","AuthorNames-Deduped":"Yao Ming;Huamin Qu;Enrico Bertini","AuthorNames":"Yao Ming;Huamin Qu;Enrico Bertini","AuthorAffiliation":"University of Science and Technology;University of Science and Technology;New York University","InternalReferences":"10.1109/TVCG.2017.2744683;10.1109/TVCG.2017.2744718;10.1109/VAST.2017.8585720;10.1109/TVCG.2016.2598831;10.1109/VAST.2017.8585721;10.1109/TVCG.2017.2744358;10.1109/TVCG.2016.2598838;10.1109/TVCG.2016.2598828;10.1109/TVCG.2017.2744158;10.1109/VISUAL.2005.1532820;10.1109/VAST.2011.6102453;10.1109/TVCG.2017.2744878","AuthorKeywords":"explainable machine learning,rule visualization,visual analytics","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":7,"PubsCited":48,"Award":null,"image":"25tvcg01-ming-2864812-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2018,"Title":"BitExTract: Interactive Visualization for Extracting Bitcoin Exchange Intelligence","DOI":"10.1109/TVCG.2018.2864814","Link":"http://dx.doi.org/10.1109/TVCG.2018.2864814","FirstPage":162,"LastPage":171,"PaperType":"J","Abstract":"The emerging prosperity of cryptocurrencies, such as Bitcoin, has come into the spotlight during the past few years. Cryptocurrency exchanges, which act as the gateway to this world, now play a dominant role in the circulation of Bitcoin. Thus, delving into the analysis of the transaction patterns of exchanges can shed light on the evolution and trends in the Bitcoin market, and participants can gain hints for identifying credible exchanges as well. Not only Bitcoin practitioners but also researchers in the financial domains are interested in the business intelligence behind the curtain. However, the task of multiple exchanges exploration and comparisons has been limited owing to the lack of efficient tools. Previous methods of visualizing Bitcoin data have mainly concentrated on tracking suspicious transaction logs, but it is cumbersome to analyze exchanges and their relationships with existing tools and methods. In this paper, we present BitExTract, an interactive visual analytics system, which, to the best of our knowledge, is the first attempt to explore the evolutionary transaction patterns of Bitcoin exchanges from two perspectives, namely, exchange versus exchange and exchange versus client. In particular, BitExTract summarizes the evolution of the Bitcoin market by observing the transactions between exchanges over time via a massive sequence view. A node-link diagram with ego-centered views depicts the trading network of exchanges and their temporal transaction distribution. Moreover, BitExTract embeds multiple parallel bars on a timeline to examine and compare the evolution patterns of transactions between different exchanges. Three case studies with novel insights demonstrate the effectiveness and usability of our system.","AuthorNames-Deduped":"Xuanwu Yue;Xinhuan Shu;Xinyu Zhu;Xinnan Du;Zheqing Yu;Dimitrios Papadopoulos;Siyuan Liu","AuthorNames":"Xuanwu Yue;Xinhuan Shu;Xinyu Zhu;Xinnan Du;Zheqing Yu;Dimitrios Papadopoulos;Siyuan Liu","AuthorAffiliation":"Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Penn State University","InternalReferences":"10.1109/TVCG.2008.117;10.1109/TVCG.2011.169;10.1109/VAST.2007.4389009;10.1109/TVCG.2009.180;10.1109/TVCG.2014.2346913","AuthorKeywords":"Bitcoin exchange,transaction data,comparative analysis,visual analytics,FinTech","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":2,"PubsCited":50,"Award":null,"image":"25tvcg01-yue-2864814-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2018,"Title":"EnsembleLens: Ensemble-based Visual Exploration of Anomaly Detection Algorithms with Multidimensional Data","DOI":"10.1109/TVCG.2018.2864825","Link":"http://dx.doi.org/10.1109/TVCG.2018.2864825","FirstPage":109,"LastPage":119,"PaperType":"J","Abstract":"The results of anomaly detection are sensitive to the choice of detection algorithms as they are specialized for different properties of data, especially for multidimensional data. Thus, it is vital to select the algorithm appropriately. To systematically select the algorithms, ensemble analysis techniques have been developed to support the assembly and comparison of heterogeneous algorithms. However, challenges remain due to the absence of the ground truth, interpretation, or evaluation of these anomaly detectors. In this paper, we present a visual analytics system named EnsembleLens that evaluates anomaly detection algorithms based on the ensemble analysis process. The system visualizes the ensemble processes and results by a set of novel visual designs and multiple coordinated contextual views to meet the requirements of correlation analysis, assessment and reasoning of anomaly detection algorithms. We also introduce an interactive analysis workflow that dynamically produces contextualized and interpretable data summaries that allow further refinements of exploration results based on user feedback. We demonstrate the effectiveness of EnsembleLens through a quantitative evaluation, three case studies with real-world data and interviews with two domain experts.","AuthorNames-Deduped":"Ke Xu;Meng Xia;Xing Mu;Yun Wang 0012;Nan Cao","AuthorNames":"Ke Xu;Meng Xia;Xing Mu;Yun Wang;Nan Cao","AuthorAffiliation":"Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;iDVxLabTongji University","InternalReferences":"10.1109/SciVis.2015.7429487;10.1109/TVCG.2017.2744419;10.1109/TVCG.2014.2346448;10.1109/TVCG.2015.2468093;10.1109/VISUAL.1990.146402;10.1109/TVCG.2017.2745178;10.1109/TVCG.2010.181;10.1109/TVCG.2016.2598830;10.1109/TVCG.2014.2346922","AuthorKeywords":"Algorithm Evaluation,Ensemble Analysis,Anomaly Detection,Visual Analysis,Multidimensional Data","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":0,"PubsCited":80,"Award":null,"image":"25tvcg01-xu-2864825-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"VIBR: Visualizing Bipartite Relations at Scale with the Minimum Description Length Principle","DOI":"10.1109/TVCG.2018.2864826","Link":"http://dx.doi.org/10.1109/TVCG.2018.2864826","FirstPage":321,"LastPage":330,"PaperType":"J","Abstract":"Bipartite graphs model the key relations in many large scale real-world data: customers purchasing items, legislators voting for bills, people's affiliation with different social groups, faults occurring in vehicles, etc. However, it is challenging to visualize large scale bipartite graphs with tens of thousands or even more nodes or edges. In this paper, we propose a novel visual summarization technique for bipartite graphs based on the minimum description length (MDL) principle. The method simultaneously groups the two different set of nodes and constructs aggregated bipartite relations with balanced granularity and precision. It addresses the key trade-off that often occurs for visualizing large scale and noisy data: acquiring a clear and uncluttered overview while maximizing the information content in it. We formulate the visual summarization task as a co-clustering problem and propose an efficient algorithm based on locality sensitive hashing (LSH) that can easily scale to large graphs under reasonable interactive time constraints that previous related methods cannot satisfy. The method leads to the opportunity of introducing a visual analytics framework with multiple levels-of-detail to facilitate interactive data exploration. In the framework, we also introduce a compact visual design inspired by adjacency list representation of graphs as the building block for a small multiples display to compare the bipartite relations for different subsets of data. We showcase the applicability and effectiveness of our approach by applying it on synthetic data with ground truth and performing case studies on real-world datasets from two application domains including roll-call vote record analysis and vehicle fault pattern analysis. Interviews with experts in the political science community and the automotive industry further highlight the benefits of our approach.","AuthorNames-Deduped":"Gromit Yeuk-Yin Chan;Panpan Xu;Zeng Dai;Liu Ren","AuthorNames":"Gromit Yeuk-Yin Chan;Panpan Xu;Zeng Dai;Liu Ren","AuthorAffiliation":"New York University;Bosch Research North America, Sunnyvale;Bosch Research North America, Sunnyvale;Bosch Research North America, Sunnyvale","InternalReferences":"10.1109/TVCG.2010.154;10.1109/TVCG.2012.252;10.1109/TVCG.2013.223;10.1109/INFVIS.2004.1;10.1109/TVCG.2016.2598831;10.1109/TVCG.2009.111;10.1109/TVCG.2014.2346279;10.1109/TVCG.2006.166;10.1109/VAST.2007.4389006;10.1109/TVCG.2015.2467813;10.1109/TVCG.2014.2346665;10.1109/TVCG.2016.2598591","AuthorKeywords":"Bipartite Graph,Visual Summarization,Minimum Description Length,Information Theory","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":4,"PubsCited":47,"Award":null,"image":"25tvcg01-chan-2864826-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2018,"Title":"VIS4ML: An Ontology for Visual Analytics Assisted Machine Learning","DOI":"10.1109/TVCG.2018.2864838","Link":"http://dx.doi.org/10.1109/TVCG.2018.2864838","FirstPage":385,"LastPage":395,"PaperType":"J","Abstract":"While many VA workflows make use of machine-learned models to support analytical tasks, VA workflows have become increasingly important in understanding and improving Machine Learning (ML) processes. In this paper, we propose an ontology (VIS4ML) for a subarea of VA, namely “VA-assisted ML”. The purpose of VIS4ML is to describe and understand existing VA workflows used in ML as well as to detect gaps in ML processes and the potential of introducing advanced VA techniques to such processes. Ontologies have been widely used to map out the scope of a topic in biology, medicine, and many other disciplines. We adopt the scholarly methodologies for constructing VIS4ML, including the specification, conceptualization, formalization, implementation, and validation of ontologies. In particular, we reinterpret the traditional VA pipeline to encompass model-development workflows. We introduce necessary definitions, rules, syntaxes, and visual notations for formulating VIS4ML and make use of semantic web technologies for implementing it in the Web Ontology Language (OWL). VIS4ML captures the high-level knowledge about previous workflows where VA is used to assist in ML. It is consistent with the established VA concepts and will continue to evolve along with the future developments in VA and ML. While this ontology is an effort for building the theoretical foundation of VA, it can be used by practitioners in real-world applications to optimize model-development workflows by systematically examining the potential benefits that can be brought about by either machine or human capabilities. Meanwhile, VIS4ML is intended to be extensible and will continue to be updated to reflect future advancements in using VA for building high-quality data-analytical models or for building such models rapidly.","AuthorNames-Deduped":"Dominik Sacha;Matthias Kraus;Daniel A. Keim;Min Chen 0001","AuthorNames":"Dominik Sacha;Matthias Kraus;Daniel A. Keim;Min Chen","AuthorAffiliation":"University of Konstanz;University of Konstanz;University of Konstanz;University of Oxford","InternalReferences":"10.1109/TVCG.2017.2744683;10.1109/VISUAL.2004.10;10.1109/VAST.2008.4677361;10.1109/TVCG.2017.2744718;10.1109/TVCG.2017.2745178;10.1109/TVCG.2017.2745085;10.1109/TVCG.2017.2744938;10.1109/TVCG.2016.2598831;10.1109/TVCG.2017.2744378;10.1109/TVCG.2017.2745158;10.1109/TVCG.2017.2744358;10.1109/TVCG.2016.2598838;10.1109/TVCG.2016.2598828;10.1109/TVCG.2017.2744805;10.1109/TVCG.2014.2346481;10.1109/TVCG.2016.2598495;10.1109/TVCG.2017.2744158;10.1109/TVCG.2016.2598829;10.1109/VISUAL.2005.1532781;10.1109/TVCG.2016.2598830;10.1109/TVCG.2017.2744878","AuthorKeywords":"Visual Analytics,Visualization,Machine Learning,Human-Computer Interaction,Ontology,VIS4ML","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":12,"XploreCitationCount - 2020-01":5,"PubsCited":71,"Award":null,"image":"25tvcg01-sacha-2864838-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"An Interactive Method to Improve Crowdsourced Annotations","DOI":"10.1109/TVCG.2018.2864843","Link":"http://dx.doi.org/10.1109/TVCG.2018.2864843","FirstPage":235,"LastPage":245,"PaperType":"J","Abstract":"In order to effectively infer correct labels from noisy crowdsourced annotations, learning-from-crowds models have introduced expert validation. However, little research has been done on facilitating the validation procedure. In this paper, we propose an interactive method to assist experts in verifying uncertain instance labels and unreliable workers. Given the instance labels and worker reliability inferred from a learning-from-crowds model, candidate instances and workers are selected for expert validation. The influence of verified results is propagated to relevant instances and workers through the learning-from-crowds model. To facilitate the validation of annotations, we have developed a confusion visualization to indicate the confusing classes for further exploration, a constrained projection method to show the uncertain labels in context, and a scatter-plot-based visualization to illustrate worker reliability. The three visualizations are tightly integrated with the learning-from-crowds model to provide an iterative and progressive environment for data validation. Two case studies were conducted that demonstrate our approach offers an efficient method for validating and improving crowdsourced annotations.","AuthorNames-Deduped":"Shixia Liu;Changjian Chen;Yafeng Lu;Fang-Xin Ou-Yang;Bin Wang","AuthorNames":"Shixia Liu;Changjian Chen;Yafeng Lu;Fangxin Ouyang;Bin Wang","AuthorAffiliation":"School of SoftwareTsinghua University;School of SoftwareTsinghua University;Arizona State University;School of SoftwareTsinghua University;School of SoftwareTsinghua University","InternalReferences":"10.1109/TVCG.2016.2598592;10.1109/VAST.2014.7042480;10.1109/TVCG.2017.2744818;10.1109/VAST.2016.7883520;10.1109/TVCG.2011.202;10.1109/TVCG.2014.2346594;10.1109/TVCG.2013.212;10.1109/TVCG.2011.239;10.1109/TVCG.2012.277;10.1109/VAST.2012.6400492;10.1109/TVCG.2016.2598445;10.1109/TVCG.2015.2467622;10.1109/TVCG.2015.2467554;10.1109/TVCG.2017.2744938;10.1109/TVCG.2016.2598831;10.1109/TVCG.2017.2744378;10.1109/VAST.2016.7883508;10.1109/TVCG.2009.139;10.1109/TVCG.2016.2598829;10.1109/TVCG.2017.2745078;10.1109/VAST.2014.7042494;10.1109/TVCG.2017.2744685;10.1109/TVCG.2013.164;10.1109/VAST.2016.7883514","AuthorKeywords":"Crowdsourcing,learning-from-crowds,interactive visualization,focus + context","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":2,"PubsCited":65,"Award":null,"image":"25tvcg01-liu-2864843-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"A Visual Analytics Framework for Spatiotemporal Trade Network Analysis","DOI":"10.1109/TVCG.2018.2864844","Link":"http://dx.doi.org/10.1109/TVCG.2018.2864844","FirstPage":331,"LastPage":341,"PaperType":"J","Abstract":"Economic globalization is increasing connectedness among regions of the world, creating complex interdependencies within various supply chains. Recent studies have indicated that changes and disruptions within such networks can serve as indicators for increased risks of violence and armed conflicts. This is especially true of countries that may not be able to compete for scarce commodities during supply shocks. Thus, network-induced vulnerability to supply disruption is typically exported from wealthier populations to disadvantaged populations. As such, researchers and stakeholders concerned with supply chains, political science, environmental studies, etc. need tools to explore the complex dynamics within global trade networks and how the structure of these networks relates to regional instability. However, the multivariate, spatiotemporal nature of the network structure creates a bottleneck in the extraction and analysis of correlations and anomalies for exploratory data analysis and hypothesis generation. Working closely with experts in political science and sustainability, we have developed a highly coordinated, multi-view framework that utilizes anomaly detection, network analytics, and spatiotemporal visualization methods for exploring the relationship between global trade networks and regional instability. Requirements for analysis and initial research questions to be investigated are elicited from domain experts, and a variety of visual encoding techniques for rapid assessment of analysis and correlations between trade goods, network patterns, and time series signatures are explored. We demonstrate the application of our framework through case studies focusing on armed conflicts in Africa, regional instability measures, and their relationship to international global trade.","AuthorNames-Deduped":"Hong Wang;Yafeng Lu;Shade T. Shutters;Michael Steptoe;Feng Wang 0012;Steven Landis;Ross Maciejewski","AuthorNames":"Hong Wang;Yafeng Lu;Shade T. Shutters;Michael Steptoe;Feng Wang;Steven Landis;Ross Maciejewski","AuthorAffiliation":"Arizona State University;Arizona State University;Arizona State University;Arizona State University;GE Global Research;University of Nevada;Arizona State University","InternalReferences":"10.1109/VAST.2008.4677356;10.1109/TVCG.2011.202;10.1109/VAST.2012.6400557;10.1109/TVCG.2008.135;10.1109/VAST.2012.6400485;10.1109/TVCG.2014.2346682;10.1109/TVCG.2009.143;10.1109/TVCG.2014.2346271;10.1109/TVCG.2015.2467991;10.1109/VAST.2012.6400491;10.1109/INFVIS.1996.559226;10.1109/INFVIS.2005.1532150;10.1109/TVCG.2016.2598885","AuthorKeywords":"Global trade network,anomaly detection,visual analytics","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":1,"PubsCited":82,"Award":null,"image":"25tvcg01-wang-2864844-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2018,"Title":"Visual Progression Analysis of Event Sequence Data","DOI":"10.1109/TVCG.2018.2864885","Link":"http://dx.doi.org/10.1109/TVCG.2018.2864885","FirstPage":417,"LastPage":426,"PaperType":"J","Abstract":"Event sequence data is common to a broad range of application domains, from security to health care to scholarly communication. This form of data captures information about the progression of events for an individual entity (e.g., a computer network device; a patient; an author) in the form of a series of time-stamped observations. Moreover, each event is associated with an event type (e.g., a computer login attempt, or a hospital discharge). Analyses of event sequence data have been shown to help reveal important temporal patterns, such as clinical paths resulting in improved outcomes, or an understanding of common career trajectories for scholars. Moreover, recent research has demonstrated a variety of techniques designed to overcome methodological challenges such as large volumes of data and high dimensionality. However, the effective identification and analysis of latent stages of progression, which can allow for variation within different but similarly evolving event sequences, remain a significant challenge with important real-world motivations. In this paper, we propose an unsupervised stage analysis algorithm to identify semantically meaningful progression stages as well as the critical events which help define those stages. The algorithm follows three key steps: (1) event representation estimation, (2) event sequence warping and alignment, and (3) sequence segmentation. We also present a novel visualization system, ET<sup>2</sup>, which interactively illustrates the results of the stage analysis algorithm to help reveal evolution patterns across stages. Finally, we report three forms of evaluation for ET<sup>2</sup>: (1) case studies with two real-world datasets, (2) interviews with domain expert users, and (3) a performance evaluation on the progression analysis algorithm and the visualization design.","AuthorNames-Deduped":"Shunan Guo;Zhuochen Jin;David Gotz;Fan Du;Hongyuan Zha;Nan Cao","AuthorNames":"Shunan Guo;Zhuochen Jin;David Gotz;Fan Du;Hongyuan Zha;Nan Cao","AuthorAffiliation":"East China Normal University;iDVX labTongji University;University of North Carolina, Chapel Hill;University of Maryland;East China Normal University;iDVX labTongji University","InternalReferences":"10.1109/TVCG.2011.188;10.1109/TVCG.2017.2745278;10.1109/TVCG.2017.2745083;10.1109/VAST.2016.7883512;10.1109/TVCG.2014.2346682;10.1109/TVCG.2017.2745320;10.1109/TVCG.2014.2346574;10.1109/TVCG.2009.187;10.1109/TVCG.2014.2346913","AuthorKeywords":"Progression Analysis,Visual Analysis,Event Sequence Data","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":3,"PubsCited":49,"Award":null,"image":"25tvcg01-guo-2864885-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"MAQUI: Interweaving Queries and Pattern Mining for Recursive Event Sequence Exploration","DOI":"10.1109/TVCG.2018.2864886","Link":"http://dx.doi.org/10.1109/TVCG.2018.2864886","FirstPage":396,"LastPage":406,"PaperType":"J","Abstract":"Exploring event sequences by defining queries alone or by using mining algorithms alone is often not sufficient to support analysis. Analysts often interweave querying and mining in a recursive manner during event sequence analysis: sequences extracted as query results are used for mining patterns, patterns generated are incorporated into a new query for segmenting the sequences, and the resulting segments are mined or queried again. To support flexible analysis, we propose a framework that describes the process of interwoven querying and mining. Based on this framework, we developed MAQUI, a Mining And Querying User Interface that enables recursive event sequence exploration. To understand the efficacy of MAQUI, we conducted two case studies with domain experts. The findings suggest that the capability of interweaving querying and mining helps the participants articulate their questions and gain novel insights from their data.","AuthorNames-Deduped":"Po-Ming Law;Zhicheng Liu;Sana Malik;Rahul C. Basole","AuthorNames":"Po-Ming Law;Zhicheng Liu;Sana Malik;Rahul C. Basole","AuthorAffiliation":"Georgia Institute of Technology;Adobe Research;Adobe Research;Georgia Institute of Technology","InternalReferences":"10.1109/TVCG.2017.2745278;10.1109/TVCG.2017.2745083;10.1109/VAST.2016.7883512;10.1109/VAST.2006.261421;10.1109/TVCG.2017.2744199;10.1109/TVCG.2014.2346682;10.1109/TVCG.2015.2467622;10.1109/TVCG.2014.2346452;10.1109/TVCG.2016.2598797;10.1109/TVCG.2013.200;10.1109/VAST.2015.7347682;10.1109/TVCG.2014.2346574;10.1109/TVCG.2009.117;10.1109/VAST.2009.5332595","AuthorKeywords":"Sequential pattern mining,temporal query,event sequence exploration","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":5,"XploreCitationCount - 2020-01":3,"PubsCited":46,"Award":null,"image":"25tvcg01-law-2864886-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"InkPlanner: Supporting Prewriting via Intelligent Visual Diagramming","DOI":"10.1109/TVCG.2018.2864887","Link":"http://dx.doi.org/10.1109/TVCG.2018.2864887","FirstPage":277,"LastPage":287,"PaperType":"J","Abstract":"Prewriting is the process of generating and organizing ideas before drafting a document. Although often overlooked by novice writers and writing tool developers, prewriting is a critical process that improves the quality of a final document. To better understand current prewriting practices, we first conducted interviews with writing learners and experts. Based on the learners' needs and experts' recommendations, we then designed and developed InkPlanner, a novel pen and touch visualization tool that allows writers to utilize visual diagramming for ideation during prewriting. InkPlanner further allows writers to sort their ideas into a logical and sequential narrative by using a novel widget- NarrativeLine. Using a NarrativeLine, InkPlanner can automatically generate a document outline to guide later drafting exercises. Inkplanner is powered by machine-generated semantic and structural suggestions that are curated from various texts. To qualitatively review the tool and understand how writers use InkPlanner for prewriting, two writing experts were interviewed and a user study was conducted with university students. The results demonstrated that InkPlanner encouraged writers to generate more diverse ideas and also enabled them to think more strategically about how to organize their ideas for later drafting.","AuthorNames-Deduped":"Zhicong Lu;Mingming Fan 0001;Yun Wang;Jian Zhao 0010;Michelle Annett;Daniel J. Wigdor","AuthorNames":"Zhicong Lu;Mingming Fan;Yun Wang;Jian Zhao;Michelle Annett;Daniel Wigdor","AuthorAffiliation":"University of Toronto;University of Toronto;Hong Kong University of Science and Technology;FX Palo Alto Laboratory;MishMashMakers;University of Toronto","InternalReferences":"10.1109/TVCG.2013.191","AuthorKeywords":"Writing,prewriting,diagraming,content and structure recommendation,pen and touch interfaces","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":2,"PubsCited":60,"Award":null,"image":"25tvcg01-lu-2864887-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"Visual Analysis of the Temporal Evolution of Ensemble Forecast Sensitivities","DOI":"10.1109/TVCG.2018.2864901","Link":"http://dx.doi.org/10.1109/TVCG.2018.2864901","FirstPage":98,"LastPage":108,"PaperType":"J","Abstract":"Ensemble sensitivity analysis (ESA) has been established in the atmospheric sciences as a correlation-based approach to determine the sensitivity of a scalar forecast quantity computed by a numerical weather prediction model to changes in another model variable at a different model state. Its applications include determining the origin of forecast errors and placing targeted observations to improve future forecasts. We - a team of visualization scientists and meteorologists - present a visual analysis framework to improve upon current practice of ESA. We support the user in selecting regions to compute a meaningful target forecast quantity by embedding correlation-based grid-point clustering to obtain statistically coherent regions. The evolution of sensitivity features computed via ESA are then traced through time, by integrating a quantitative measure of feature matching into optical-flow-based feature assignment, and displayed by means of a swipe-path showing the geo-spatial evolution of the sensitivities. Visualization of the internal correlation structure of computed features guides the user towards those features robustly predicting a certain weather event. We demonstrate the use of our method by application to real-world 2D and 3D cases that occurred during the 2016 NAWDEX field campaign, showing the interactive generation of hypothesis chains to explore how atmospheric processes sensitive to each other are interrelated.","AuthorNames-Deduped":"Alexander Kumpf;Marc Rautenhaus;Michael Riemer;Rüdiger Westermann","AuthorNames":"Alexander Kumpf;Marc Rautenhaus;Michael Riemer;Rüdiger Westermann","AuthorAffiliation":"Computer Graphics & Visualization Group, Technische Universitiit München, Garching, Germany;Computer Graphics & Visualization Group, Technische Universitiit München, Garching, Germany;Universität Hamburg, Regional Computing Center, Hamburg, Germany;Computer Graphics & Visualization Group, Technische Universitiit München, Garching, Germany","InternalReferences":"10.1109/TVCG.2013.131;10.1109/VISUAL.2004.46;10.1109/TVCG.2017.2743989;10.1109/TVCG.2017.2745178;10.1109/TVCG.2006.165","AuthorKeywords":"Correlation,clustering,tracking,ensemble visualization","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":0,"PubsCited":47,"Award":null,"image":"25tvcg01-kumpf-2864901-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"Doccurate: A Curation-Based Approach for Clinical Text Visualization","DOI":"10.1109/TVCG.2018.2864905","Link":"http://dx.doi.org/10.1109/TVCG.2018.2864905","FirstPage":142,"LastPage":151,"PaperType":"J","Abstract":"Before seeing a patient, physicians seek to obtain an overview of the patient's medical history. Text plays a major role in this activity since it represents the bulk of the clinical documentation, but reviewing it quickly becomes onerous when patient charts grow too large. Text visualization methods have been widely explored to manage this large scale through visual summaries that rely on information retrieval algorithms to structure text and make it amenable to visualization. However, the integration with such automated approaches comes with a number of limitations, including significant error rates and the need for healthcare providers to fine-tune algorithms without expert knowledge of their inner mechanics. In addition, several of these approaches obscure or substitute the original clinical text and therefore fail to leverage qualitative and rhetorical flavours of the clinical notes. These drawbacks have limited the adoption of text visualization and other summarization technologies in clinical practice. In this work we present Doccurate, a novel system embodying a curation-based approach for the visualization of large clinical text datasets. Our approach offers automation auditing and customizability to physicians while also preserving and extensively linking to the original text. We discuss findings of a formal qualitative evaluation conducted with 6 domain experts, shedding light onto physicians' information needs, perceived strengths and limitations of automated tools, and the importance of customization while balancing efficiency. We also present use case scenarios to showcase Doccurate's envisioned usage in practice.","AuthorNames-Deduped":"Nicole Sultanum;Devin Singh;Michael Brudno;Fanny Chevalier","AuthorNames":"Nicole Sultanum;Devin Singh;Michael Brudno;Fanny Chevalier","AuthorAffiliation":"Hospital for Sick Children;Hospital for Sick Children;University of Toronto;University of Toronto","InternalReferences":"10.1109/VAST.2012.6400485;10.1109/TVCG.2015.2467531;10.1109/TVCG.2017.2745118;10.1109/INFVIS.2000.885098;10.1109/TVCG.2017.2744478;10.1109/TVCG.2015.2467591","AuthorKeywords":"Visual Curation,Clinical Text,Text Visualization,Medical Narrative","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":2,"PubsCited":40,"Award":null,"image":"25tvcg01-sultanum-2864905-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"TPFlow: Progressive Partition and Multidimensional Pattern Extraction for Large-Scale Spatio-Temporal Data Analysis","DOI":"10.1109/TVCG.2018.2865018","Link":"http://dx.doi.org/10.1109/TVCG.2018.2865018","FirstPage":1,"LastPage":11,"PaperType":"J","Abstract":"Consider a multi-dimensional spatio-temporal (ST) dataset where each entry is a numerical measure defined by the corresponding temporal, spatial and other domain-specific dimensions. A typical approach to explore such data utilizes interactive visualizations with multiple coordinated views. Each view displays the aggregated measures along one or two dimensions. By brushing on the views, analysts can obtain detailed information. However, this approach often cannot provide sufficient guidance for analysts to identify patterns hidden within subsets of data. Without a priori hypotheses, analysts need to manually select and iterate through different slices to search for patterns, which can be a tedious and lengthy process. In this work, we model multidimensional ST data as tensors and propose a novel piecewise rank-one tensor decomposition algorithm which supports automatically slicing the data into homogeneous partitions and extracting the latent patterns in each partition for comparison and visual summarization. The algorithm optimizes a quantitative measure about how faithfully the extracted patterns visually represent the original data. Based on the algorithm we further propose a visual analytics framework that supports a top-down, progressive partitioning workflow for level-of-detail multidimensional ST data exploration. We demonstrate the general applicability and effectiveness of our technique on three datasets from different application domains: regional sales trend analysis, customer traffic analysis in department stores, and taxi trip analysis with origin-destination (OD) data. We further interview domain experts to verify the usability of the prototype.","AuthorNames-Deduped":"Dongyu Liu;Panpan Xu;Liu Ren","AuthorNames":"Dongyu Liu;Panpan Xu;Liu Ren","AuthorAffiliation":"Hong Kong University of Science and Technology;Bosch Research North America, Sunnyvale, CA;Bosch Research North America, Sunnyvale, CA","InternalReferences":"10.1109/TVCG.2016.2598862;10.1109/TVCG.2017.2744419;10.1109/TVCG.2006.161;10.1109/TVCG.2014.2346449;10.1109/TVCG.2013.226;10.1109/TVCG.2013.179;10.1109/TVCG.2016.2598432;10.1109/TVCG.2016.2598831;10.1109/TVCG.2016.2598624;10.1109/TVCG.2017.2744805;10.1109/TVCG.2015.2467112;10.1109/TVCG.2014.2346574;10.1109/INFVIS.2003.1249018;10.1109/INFVIS.1999.801851;10.1109/TVCG.2015.2468111;10.1109/TVCG.2018.2865126;10.1109/TVCG.2007.70515","AuthorKeywords":"Spatio-temporal data,tensor decomposition,interactive exploration,automatic pattern discoveries","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":13,"XploreCitationCount - 2020-01":2,"PubsCited":67,"Award":"BP","image":"25tvcg01-liu-2865018-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2018,"Title":"Evaluating Multi-Dimensional Visualizations for Understanding Fuzzy Clusters","DOI":"10.1109/TVCG.2018.2865020","Link":"http://dx.doi.org/10.1109/TVCG.2018.2865020","FirstPage":12,"LastPage":21,"PaperType":"J","Abstract":"Fuzzy clustering assigns a probability of membership for a datum to a cluster, which veritably reflects real-world clustering scenarios but significantly increases the complexity of understanding fuzzy clusters. Many studies have demonstrated that visualization techniques for multi-dimensional data are beneficial to understand fuzzy clusters. However, no empirical evidence exists on the effectiveness and efficiency of these visualization techniques in solving analytical tasks featured by fuzzy clusters. In this paper, we conduct a controlled experiment to evaluate the ability of fuzzy clusters analysis to use four multi-dimensional visualization techniques, namely, parallel coordinate plot, scatterplot matrix, principal component analysis, and Radviz. First, we define the analytical tasks and their representative questions specific to fuzzy clusters analysis. Then, we design objective questionnaires to compare the accuracy, time, and satisfaction in using the four techniques to solve the questions. We also design subjective questionnaires to collect the experience of the volunteers with the four techniques in terms of ease of use, informativeness, and helpfulness. With a complete experiment process and a detailed result analysis, we test against four hypotheses that are formulated on the basis of our experience, and provide instructive guidance for analysts in selecting appropriate and efficient visualization techniques to analyze fuzzy clusters.","AuthorNames-Deduped":"Ying Zhao 0001;Feng Luo;Minghui Chen;Yingchao Wang;Jiazhi Xia;Fangfang Zhou;Yunhai Wang;Yi Chen 0007;Wei Chen 0001","AuthorNames":"Ying Zhao;Feng Luo;Minghui Chen;Yingchao Wang;Jiazhi Xia;Fangfang Zhou;Yunhai Wang;Yi Chen;Wei Chen","AuthorAffiliation":"Central South University;Central South University;Central South University;Central South University;Central South University;Central South University;Shandong University;Beijing TechnologyBusiness University;State Key Lab of CAD & CGZhejiang University","InternalReferences":"10.1109/INFVIS.2005.1532136;10.1109/INFVIS.1998.729559;10.1109/TVCG.2017.2745138;10.1109/VAST.2010.5652450;10.1109/VISUAL.1997.663916;10.1109/TVCG.2009.153;10.1109/TVCG.2016.2598831;10.1109/INFVIS.2004.15;10.1109/TVCG.2017.2744198;10.1109/TVCG.2015.2467324;10.1109/TVCG.2013.153;10.1109/TVCG.2008.173;10.1109/VISUAL.1990.146375;10.1109/TVCG.2017.2744098;10.1109/TVCG.2016.2598479;10.1109/INFVIS.2003.1249015","AuthorKeywords":"Evaluation,multi-dimensional visualization,fuzzy clustering,parallel coordinate plot,scatterplot matrix,principal component analysis,radviz","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":16,"XploreCitationCount - 2020-01":6,"PubsCited":63,"Award":null,"image":"25tvcg01-xia-2865020-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"GraphProtector: A Visual Interface for Employing and Assessing Multiple Privacy Preserving Graph Algorithms","DOI":"10.1109/TVCG.2018.2865021","Link":"http://dx.doi.org/10.1109/TVCG.2018.2865021","FirstPage":193,"LastPage":203,"PaperType":"J","Abstract":"Analyzing social networks reveals the relationships between individuals and groups in the data. However, such analysis can also lead to privacy exposure (whether intentionally or inadvertently): leaking the real-world identity of ostensibly anonymous individuals. Most sanitization strategies modify the graph's structure based on hypothesized tactics that an adversary would employ. While combining multiple anonymization schemes provides a more comprehensive privacy protection, deciding the appropriate set of techniques-along with evaluating how applying the strategies will affect the utility of the anonymized results-remains a significant challenge. To address this problem, we introduce GraphProtector, a visual interface that guides a user through a privacy preservation pipeline. GraphProtector enables multiple privacy protection schemes which can be simultaneously combined together as a hybrid approach. To demonstrate the effectiveness of GraphPro tector, we report several case studies and feedback collected from interviews with expert users in various scenarios.","AuthorNames-Deduped":"Xu-Meng Wang;Wei Chen 0001;Jia-Kai Chou;Chris Bryan;Huihua Guan;Wenlong Chen;Rusheng Pan;Kwan-Liu Ma","AuthorNames":"Xumeng Wang;Wei Chen;Jia-Kai Chou;Chris Bryan;Huihua Guan;Wenlong Chen;Rusheng Pan;Kwan-Liu Ma","AuthorAffiliation":"Zhejiang University;Zhejiang University;University of California, Davis;University of California, Davis;Zhejiang UniversityAlibaba Group;Zhejiang University;Zhejiang University;University of California, Davis","InternalReferences":"10.1109/TVCG.2011.163;10.1109/TVCG.2017.2745139;10.1109/TVCG.2014.2346920","AuthorKeywords":"Graph privacy,k-anonymity,structural features,privacy preservation","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":1,"PubsCited":57,"Award":null,"image":"25tvcg01-chen-2865021-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2018,"Title":"VIS Author Profiles: Interactive Descriptions of Publication Records Combining Text and Visualization","DOI":"10.1109/TVCG.2018.2865022","Link":"http://dx.doi.org/10.1109/TVCG.2018.2865022","FirstPage":152,"LastPage":161,"PaperType":"J","Abstract":"Publication records and collaboration networks are important for assessing the expertise and experience of researchers. Existing digital libraries show the raw publication lists in author profiles, whereas visualization techniques focus on specific subproblems. Instead, we look at publication records from various perspectives mixing low-level publication data with high-level abstractions and background information. This work presents VIS Author Profiles, a novel approach to generate integrated textual and visual descriptions to highlight patterns in publication records. We leverage template-based natural language generation to summarize notable publication statistics, evolution of research topics, and collaboration relationships. Seamlessly integrated visualizations augment the textual description and are interactively connected with each other and the text. The underlying publication data and detailed explanations of the analysis are available on demand. We compare our approach to existing systems by taking into account information needs of users and demonstrate its usefulness in two realistic application examples.","AuthorNames-Deduped":"Shahid Latif;Fabian Beck 0001","AuthorNames":"Shahid Latif;Fabian Beck","AuthorAffiliation":"Paluno, University of Duisburg, Essen, Germany;Paluno, University of Duisburg, Essen, Germany","InternalReferences":"10.1109/TVCG.2015.2467757;10.1109/TVCG.2012.252;10.1109/TVCG.2014.2346435;10.1109/TVCG.2007.70582;10.1109/VAST.2015.7347632;10.1109/TVCG.2015.2468151;10.1109/TVCG.2013.167","AuthorKeywords":"Natural language generation,document visualization,interactive documents,sparklines,digital libraries","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":2,"PubsCited":36,"Award":null,"image":"25tvcg01-lagif-2865022-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"Lessons Learned Developing a Visual Analytics Solution for Investigative Analysis of Scamming Activities","DOI":"10.1109/TVCG.2018.2865023","Link":"http://dx.doi.org/10.1109/TVCG.2018.2865023","FirstPage":225,"LastPage":234,"PaperType":"J","Abstract":"The forensic investigation of communication datasets which contain unstructured text, social network information, and metadata is a complex task that is becoming more important due to the immense amount of data being collected. Currently there are limited approaches that allow an investigator to explore the network, text and metadata in a unified manner. We developed Beagle as a forensic tool for email datasets that allows investigators to flexibly form complex queries in order to discover important information in email data. Beagle was successfully deployed at a security firm which had a large email dataset that was difficult to properly investigate. We discuss our experience developing Beagle as well as the lessons we learned applying visual analytic techniques to a difficult real-world problem.","AuthorNames-Deduped":"Jay Koven;Cristian Felix;Hossein Siadati;Markus Jakobsson;Enrico Bertini","AuthorNames":"Jay Koven;Cristian Felix;Hossein Siadati;Markus Jakobsson;Enrico Bertini","AuthorAffiliation":"NYU Tandon School of Engineering;NYU Tandon School of Engineering;NYU Tandon School of Engineering;Amber Solutions Inc.;NYU Tandon School of Engineering","InternalReferences":"10.1109/VAST.2007.4389011;10.1109/VAST.2010.5652968;10.1109/INFVIS.2003.1249028;10.1109/TVCG.2009.111;10.1109/TVCG.2014.2346481;10.1109/TVCG.2012.213","AuthorKeywords":"Visual Analytics,Email Investigation,Email Forensics","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":0,"PubsCited":26,"Award":null,"image":"25tvcg01-koven-2865023-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"KnowledgePearls: Provenance-Based Visualization Retrieval","DOI":"10.1109/TVCG.2018.2865024","Link":"http://dx.doi.org/10.1109/TVCG.2018.2865024","FirstPage":120,"LastPage":130,"PaperType":"J","Abstract":"Storing analytical provenance generates a knowledge base with a large potential for recalling previous results and guiding users in future analyses. However, without extensive manual creation of meta information and annotations by the users, search and retrieval of analysis states can become tedious. We present KnowledgePearls, a solution for efficient retrieval of analysis states that are structured as provenance graphs containing automatically recorded user interactions and visualizations. As a core component, we describe a visual interface for querying and exploring analysis states based on their similarity to a partial definition of a requested analysis state. Depending on the use case, this definition may be provided explicitly by the user by formulating a search query or inferred from given reference states. We explain our approach using the example of efficient retrieval of demographic analyses by Hans Rosling and discuss our implementation for a fast look-up of previous states. Our approach is independent of the underlying visualization framework. We discuss the applicability for visualizations which are based on the declarative grammar Vega and we use a Vega-based implementation of Gapminder as guiding example. We additionally present a biomedical case study to illustrate how KnowledgePearls facilitates the exploration process by recalling states from earlier analyses.","AuthorNames-Deduped":"Holger Stitz;Samuel Gratzl;Harald Piringer;Thomas Zichner;Marc Streit","AuthorNames":"Holger Stitz;Samuel Gratzl;Harald Piringer;Thomas Zichner;Marc Streit","AuthorAffiliation":"Johannes Kepler University, Linz, Austria;Johannes Kepler University, Linz, Austria;VRVis Research Center, Austria;Boehringer Ingelheim RCV GmbH & Co KG, Austria;Johannes Kepler University, Linz, Austria","InternalReferences":"10.1109/VISUAL.2005.1532788;10.1109/TVCG.2011.229;10.1109/TVCG.2013.155;10.1109/TVCG.2009.176;10.1109/VAST.2008.4677365;10.1109/INFVIS.2004.2;10.1109/TVCG.2012.271;10.1109/TVCG.2016.2598589;10.1109/TVCG.2017.2744320;10.1109/TVCG.2015.2467551;10.1109/TVCG.2016.2599030;10.1109/TVCG.2015.2467091;10.1109/TVCG.2006.142;10.1109/TVCG.2017.2745219;10.1109/INFVIS.2000.885086;10.1109/INFVIS.2005.1532142","AuthorKeywords":"Visualization provenance,interaction provenance,retrieval","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":2,"PubsCited":47,"Award":null,"image":"25tvcg01-stitz-2865024-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"An Information-Theoretic Approach to the Cost-benefit Analysis of Visualization in Virtual Environments","DOI":"10.1109/TVCG.2018.2865025","Link":"http://dx.doi.org/10.1109/TVCG.2018.2865025","FirstPage":32,"LastPage":42,"PaperType":"J","Abstract":"Visualization and virtual environments (VEs) have been two interconnected parallel strands in visual computing for decades. Some VEs have been purposely developed for visualization applications, while many visualization applications are exemplary showcases in general-purpose VEs. Because of the development and operation costs of VEs, the majority of visualization applications in practice have yet to benefit from the capacity of VEs. In this paper, we examine this status quo from an information-theoretic perspective. Our objectives are to conduct cost-benefit analysis on typical VE systems (including augmented and mixed reality, theater-based systems, and large powerwalls), to explain why some visualization applications benefit more from VEs than others, and to sketch out pathways for the future development of visualization applications in VEs. We support our theoretical propositions and analysis using theories and discoveries in the literature of cognitive sciences and the practical evidence reported in the literatures of visualization and VEs.","AuthorNames-Deduped":"Min Chen 0001;Kelly P. Gaither;Nigel W. John;Brian McCann","AuthorNames":"Min Chen;Kelly Gaither;Nigel W. John;Brian Mccann","AuthorAffiliation":"University of Oxford, UK;University of Texas, Austin, USA;University of Chester, UK;University of Texas, Austin, USA","InternalReferences":"10.1109/TVCG.2010.131;10.1109/TVCG.2008.142;10.1109/TVCG.2011.231;10.1109/TVCG.2014.2346325;10.1109/TVCG.2013.127;10.1109/TVCG.2016.2598829;10.1109/INFVIS.2004.59;10.1109/VISUAL.2005.1532781;10.1109/TVCG.2010.131;10.1109/TVCG.2006.184","AuthorKeywords":"Theory of visualization,virtual environments,four levels of visualization,virtual reality,augmented reality,mixed reality,cost-benefit analysis,information theory,cognitive sciences,visualization applications,immersive analytics","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":1,"PubsCited":119,"Award":null,"image":"25tvcg01-chen-2865025-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"A Visual Analytics Framework for the Detection of Anomalous Call Stack Trees in High Performance Computing Applications","DOI":"10.1109/TVCG.2018.2865026","Link":"http://dx.doi.org/10.1109/TVCG.2018.2865026","FirstPage":215,"LastPage":224,"PaperType":"J","Abstract":"Anomalous runtime behavior detection is one of the most important tasks for performance diagnosis in High Performance Computing (HPC). Most of the existing methods find anomalous executions based on the properties of individual functions, such as execution time. However, it is insufficient to identify abnormal behavior without taking into account the context of the executions, such as the invocations of children functions and the communications with other HPC nodes. We improve upon the existing anomaly detection approaches by utilizing the call stack structures of the executions, which record rich temporal and contextual information. With our call stack tree (CSTree) representation of the executions, we formulate the anomaly detection problem as finding anomalous tree structures in a call stack forest. The CSTrees are converted to vector representations using our proposed stack2vec embedding. Structural and temporal visualizations of CSTrees are provided to support users in the identification and verification of the anomalies during an active anomaly detection process. Three case studies of real-world HPC applications demonstrate the capabilities of our approach.","AuthorNames-Deduped":"Cong Xie;Wei Xu;Klaus Mueller","AuthorNames":"Cong Xie;Wei Xu;Klaus Mueller","AuthorAffiliation":"Department of Computer ScienceStony Brook University;Computational Science InitiativeBrookhaven National Laboratory;Department of Computer ScienceStony Brook University","InternalReferences":"10.1109/TVCG.2015.2467552;10.1109/TVCG.2012.277;10.1109/VAST.2016.7883514;10.1109/TVCG.2016.2598664","AuthorKeywords":"Call Stack,Performance Visualization,Representation Learning,Active Learning,Anomaly Detection","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":6,"XploreCitationCount - 2020-01":5,"PubsCited":55,"Award":"HM","image":"25tvcg01-xie-2865026-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2018,"Title":"RetainVis: Visual Analytics with Interpretable and Interactive Recurrent Neural Networks on Electronic Medical Records","DOI":"10.1109/TVCG.2018.2865027","Link":"http://dx.doi.org/10.1109/TVCG.2018.2865027","FirstPage":299,"LastPage":309,"PaperType":"J","Abstract":"We have recently seen many successful applications of recurrent neural networks (RNNs) on electronic medical records (EMRs), which contain histories of patients' diagnoses, medications, and other various events, in order to predict the current and future states of patients. Despite the strong performance of RNNs, it is often challenging for users to understand why the model makes a particular prediction. Such black-box nature of RNNs can impede its wide adoption in clinical practice. Furthermore, we have no established methods to interactively leverage users' domain expertise and prior knowledge as inputs for steering the model. Therefore, our design study aims to provide a visual analytics solution to increase interpretability and interactivity of RNNs via a joint effort of medical experts, artificial intelligence scientists, and visual analytics researchers. Following the iterative design process between the experts, we design, implement, and evaluate a visual analytics tool called RetainVis, which couples a newly improved, interpretable, and interactive RNN-based model called RetainEX and visualizations for users' exploration of EMR data in the context of prediction tasks. Our study shows the effective use of RetainVis for gaining insights into how individual medical codes contribute to making risk predictions, using EMRs of patients with heart failure and cataract symptoms. Our study also demonstrates how we made substantial changes to the state-of-the-art RNN model called RETAIN in order to make use of temporal information and increase interactivity. This study will provide a useful guideline for researchers that aim to design an interpretable and interactive visual analytics tool for RNNs.","AuthorNames-Deduped":"Bum Chul Kwon;Min-Je Choi;Joanne Taery Kim;Edward Choi;Young Bin Kim;Soonwook Kwon;Jimeng Sun;Jaegul Choo","AuthorNames":"Bum Chul Kwon;Min-Je Choi;Joanne Taery Kim;Edward Choi;Young Bin Kim;Soonwook Kwon;Jimeng Sun;Jaegul Choo","AuthorAffiliation":"IBM T.J. Watson Research CenterKorea University;Georgia Institute of Technology;Georgia Institute of Technology;Chung-Ang University;IBM T.J. Watson Research CenterKorea University;Catholic University, Daegu;IBM T.J. Watson Research CenterKorea University;Georgia Institute of Technology","InternalReferences":"10.1109/TVCG.2013.212;10.1109/TVCG.2017.2745080;10.1109/TVCG.2012.277;10.1109/TVCG.2017.2744718;10.1109/TVCG.2017.2745085;10.1109/TVCG.2016.2598446;10.1109/TVCG.2015.2467555;10.1109/TVCG.2016.2598831;10.1109/VAST.2017.8585721;10.1109/TVCG.2017.2744358;10.1109/TVCG.2016.2598838;10.1109/TVCG.2012.213;10.1109/TVCG.2017.2744158;10.1109/TVCG.2017.2744878","AuthorKeywords":"Interactive Artificial Intelligence,XAI (Explainable Artificial Intelligence),Interpretable Deep Learning,Healthcare","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":18,"XploreCitationCount - 2020-01":6,"PubsCited":85,"Award":null,"image":"25tvcg01-kwon-2865027-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2018,"Title":"Vulnus: Visual Vulnerability Analysis for Network Security","DOI":"10.1109/TVCG.2018.2865028","Link":"http://dx.doi.org/10.1109/TVCG.2018.2865028","FirstPage":183,"LastPage":192,"PaperType":"J","Abstract":"Vulnerabilities represent one of the main weaknesses of IT systems and the availability of consolidated official data, like CVE (Common Vulnerabilities and Exposures), allows for using them to compute the paths an attacker is likely to follow. However, even if patches are available, business constraints or lack of resources create obstacles to their straightforward application. As a consequence, the security manager of a network needs to deal with a large number of vulnerabilities, making decisions on how to cope with them. This paper presents VULNUS (VULNerabilities visUal aSsessment), a visual analytics solution for dynamically inspecting the vulnerabilities spread on networks, allowing for a quick understanding of the network status and visually classifying nodes according to their vulnerabilities. Moreover, VULNUS computes the approximated optimal sequence of patches able to eliminate all the attack paths and allows for exploring sub-optimal patching strategies, simulating the effect of removing one or more vulnerabilities. VULNUS has been evaluated by domain experts using a lab-test experiment, investigating the effectiveness and efficiency of the proposed solution.","AuthorNames-Deduped":"Marco Angelini;Graziano Blasilli;Tiziana Catarci;Simone Lenti;Giuseppe Santucci","AuthorNames":"Marco Angelini;Graziano Blasilli;Tiziana Catarci;Simone Lenti;Giuseppe Santucci","AuthorAffiliation":"University of Rome “La Sapienza”;University of Rome “La Sapienza”;University of Rome “La Sapienza”;University of Rome “La Sapienza”;University of Rome “La Sapienza”","InternalReferences":"10.1109/TVCG.2007.70540;10.1109/TVCG.2007.70522;10.1109/INFVIS.2001.963283","AuthorKeywords":"Visual Analytics,Network security,Vulnerability analysis,CVE,CVSS,Attack Graph,Vulnerability triage and management","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":3,"PubsCited":46,"Award":null,"image":"25tvcg01-angelini-2865028-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2018,"Title":"Situ: Identifying and Explaining Suspicious Behavior in Networks","DOI":"10.1109/TVCG.2018.2865029","Link":"http://dx.doi.org/10.1109/TVCG.2018.2865029","FirstPage":204,"LastPage":214,"PaperType":"J","Abstract":"Despite the best efforts of cyber security analysts, networked computing assets are routinely compromised, resulting in the loss of intellectual property, the disclosure of state secrets, and major financial damages. Anomaly detection methods are beneficial for detecting new types of attacks and abnormal network activity, but such algorithms can be difficult to understand and trust. Network operators and cyber analysts need fast and scalable tools to help identify suspicious behavior that bypasses automated security systems, but operators do not want another automated tool with algorithms they do not trust. Experts need tools to augment their own domain expertise and to provide a contextual understanding of suspicious behavior to help them make decisions. In this paper we present Situ, a visual analytics system for discovering suspicious behavior in streaming network data. Situ provides a scalable solution that combines anomaly detection with information visualization. The system's visualizations enable operators to identify and investigate the most anomalous events and IP addresses, and the tool provides context to help operators understand why they are anomalous. Finally, operators need tools that can be integrated into their workflow and with their existing tools. This paper describes the Situ platform and its deployment in an operational network setting. We discuss how operators are currently using the tool in a large organization's security operations center and present the results of expert reviews with professionals.","AuthorNames-Deduped":"John R. Goodall;Eric D. Ragan;Chad A. Steed;Joel W. Reed;G. David Richardson;Kelly M. T. Huffer;Robert A. Bridges;Jason A. Laska","AuthorNames":"John R. Goodall;Eric D. Ragan;Chad A. Steed;Joel W. Reed;G. David Richardson;Kelly M.T. Huffer;Robert A. Bridges;Jason A. Laska","AuthorAffiliation":"Oak Ridge National Laboratory;University of Florida;Oak Ridge National Laboratory;Oak Ridge National Laboratory;Oak Ridge National Laboratory;Oak Ridge National Laboratory;Oak Ridge National Laboratory;Oak Ridge National Laboratory","InternalReferences":"10.1109/TVCG.2007.70589","AuthorKeywords":"Network security,situational awareness,privacy and security,streaming data,machine learning,visualization","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":3,"PubsCited":55,"Award":null,"image":"25tvcg01-goodall-2865029-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2018,"Title":"Enhancing Web-based Analytics Applications through Provenance","DOI":"10.1109/TVCG.2018.2865039","Link":"http://dx.doi.org/10.1109/TVCG.2018.2865039","FirstPage":131,"LastPage":141,"PaperType":"J","Abstract":"Visual analytics systems continue to integrate new technologies and leverage modern environments for exploration and collaboration, making tools and techniques available to a wide audience through web browsers. Many of these systems have been developed with rich interactions, offering users the opportunity to examine details and explore hypotheses that have not been directly encoded by a designer. Understanding is enhanced when users can replay and revisit the steps in the sensemaking process, and in collaborative settings, it is especially important to be able to review not only the current state but also what decisions were made along the way. Unfortunately, many web-based systems lack the ability to capture such reasoning, and the path to a result is transient, forgotten when a user moves to a new view. This paper explores the requirements to augment existing client-side web applications with support for capturing, reviewing, sharing, and reusing steps in the reasoning process. Furthermore, it considers situations where decisions are made with streaming data, and the insights gained from revisiting those choices when more data is available. It presents a proof of concept, the Shareable Interactive Manipulation Provenance framework (SIMProv.js), that addresses these requirements in a modern, client-side JavaScript library, and describes how it can be integrated with existing frameworks.","AuthorNames-Deduped":"Akhilesh Camisetty;Chaitanya Chandurkar;Maoyuan Sun;David Koop","AuthorNames":"Akhilesh Camisetty;Chaitanya Chandurkar;Maoyuan Sun;David Koop","AuthorAffiliation":"UMass Dartmouth;UMass Dartmouth;UMass Dartmouth;UMass Dartmouth","InternalReferences":"10.1109/VISUAL.1993.398857;10.1109/VAST.2011.6102447;10.1109/VAST.2010.5652932;10.1109/TVCG.2016.2598471;10.1109/TVCG.2016.2599058;10.1109/VAST.2008.4677365;10.1109/TVCG.2013.197;10.1109/VAST.2007.4389011;10.1109/VISUAL.1999.809871;10.1109/TVCG.2014.2346573;10.1109/TVCG.2015.2467551;10.1109/TVCG.2015.2467191;10.1109/TVCG.2017.2745279","AuthorKeywords":"Collaboration,provenance,streaming data,history,web","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":0,"PubsCited":60,"Award":null,"image":"25tvcg01-camisetty-2865039-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"Futzing and Moseying: Interviews with Professional Data Analysts on Exploration Practices","DOI":"10.1109/TVCG.2018.2865040","Link":"http://dx.doi.org/10.1109/TVCG.2018.2865040","FirstPage":22,"LastPage":31,"PaperType":"J","Abstract":"We report the results of interviewing thirty professional data analysts working in a range of industrial, academic, and regulatory environments. This study focuses on participants' descriptions of exploratory activities and tool usage in these activities. Highlights of the findings include: distinctions between exploration as a precursor to more directed analysis versus truly open-ended exploration; confirmation that some analysts see “finding something interesting” as a valid goal of data exploration while others explicitly disavow this goal; conflicting views about the role of intelligent tools in data exploration; and pervasive use of visualization for exploration, but with only a subset using direct manipulation interfaces. These findings provide guidelines for future tool development, as well as a better understanding of the meaning of the term “data exploration” based on the words of practitioners “in the wild”.","AuthorNames-Deduped":"Sara Alspaugh;Nava Zokaei;Andrea Liu;Cindy Jin;Marti A. Hearst","AuthorNames":"Sara Alspaugh;Nava Zokaei;Andrea Liu;Cindy Jin;Marti A. Hearst","AuthorAffiliation":"UC, Berkeley;UC, Berkeley;UC, Berkeley;UC, Berkeley;UC, Berkeley","InternalReferences":"10.1109/VAST.2008.4677365;10.1109/INFVIS.1997.636793;10.1109/TVCG.2012.219;10.1109/VAST.2011.6102438;10.1109/TVCG.2006.122;10.1109/TVCG.2015.2467191","AuthorKeywords":"EDA,exploratory data analysis,interview study,visual analytics tools","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":7,"XploreCitationCount - 2020-01":3,"PubsCited":26,"Award":null,"image":"25tvcg01-alspaugh-2865040-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"ForVizor: Visualizing Spatio-Temporal Team Formations in Soccer","DOI":"10.1109/TVCG.2018.2865041","Link":"http://dx.doi.org/10.1109/TVCG.2018.2865041","FirstPage":65,"LastPage":75,"PaperType":"J","Abstract":"Regarded as a high-level tactic in soccer, a team formation assigns players different tasks and indicates their active regions on the pitch, thereby influencing the team performance significantly. Analysis of formations in soccer has become particularly indispensable for soccer analysts. However, formations of a team are intrinsically time-varying and contain inherent spatial information. The spatio-temporal nature of formations and other characteristics of soccer data, such as multivariate features, make analysis of formations in soccer a challenging problem. In this study, we closely worked with domain experts to characterize domain problems of formation analysis in soccer and formulated several design goals. We design a novel spatio-temporal visual representation of changes in team formation, allowing analysts to visually analyze the evolution of formations and track the spatial flow of players within formations over time. Based on the new design, we further design and develop ForVizor, a visual analytics system, which empowers users to track the spatio-temporal changes in formation and understand how and why such changes occur. With ForVizor, domain experts conduct formation analysis of two games. Analysis results with insights and useful feedback are summarized in two case studies.","AuthorNames-Deduped":"Yingcai Wu;Xiao Xie;Jiachen Wang;Dazhen Deng;Hongye Liang;Hui Zhang;Shoubin Cheng;Wei Chen 0001","AuthorNames":"Yingcai Wu;Xiao Xie;Jiachen Wang;Dazhen Deng;Hongye Liang;Hui Zhang;Shoubin Cheng;Wei Chen","AuthorAffiliation":"State Key Lab of CAD&CGZhejiang University;State Key Lab of CAD&CGZhejiang University;State Key Lab of CAD&CGZhejiang University;State Key Lab of CAD&CGZhejiang University;State Key Lab of CAD&CGZhejiang University;Department of Sport ScienceZhejiang University;Department of Sport ScienceZhejiang University;State Key Lab of CAD&CGZhejiang University","InternalReferences":"10.1109/VAST.2008.4677356;10.1109/TVCG.2011.239;10.1109/TVCG.2014.2346433;10.1109/VAST.2014.7042477;10.1109/TVCG.2018.2865018;10.1109/TVCG.2016.2598831;10.1109/TVCG.2013.192;10.1109/TVCG.2014.2346445;10.1109/TVCG.2017.2745181;10.1109/TVCG.2017.2744218","AuthorKeywords":"Soccer data,formation analysis,spatio-temporal visualization","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":11,"XploreCitationCount - 2020-01":5,"PubsCited":48,"Award":null,"image":"25tvcg01-chen-2865041-fig-1-source-large.gif","hasViewed":true},{"Conference":"VAST","Year":2018,"Title":"Identification of Temporally Varying Areas of Interest in Long-Duration Eye-Tracking Data Sets","DOI":"10.1109/TVCG.2018.2865042","Link":"http://dx.doi.org/10.1109/TVCG.2018.2865042","FirstPage":87,"LastPage":97,"PaperType":"J","Abstract":"Eye-tracking has become an invaluable tool for the analysis of working practices in many technological fields of activity. Typically studies focus on short tasks and use static expected areas of interest (AoI) in the display to explore subjects' behaviour, making the analyst's task quite straightforward. In long-duration studies, where the observations may last several hours over a complete work session, the AoIs may change over time in response to altering workload, emergencies or other variables making the analysis more difficult. This work puts forward a novel method to automatically identify spatial AoIs changing over time through a combination of clustering and cluster merging in the temporal domain. A visual analysis system based on the proposed methods is also presented. Finally, we illustrate our approach within the domain of air traffic control, a complex task sensitive to prevailing conditions over long durations, though it is applicable to other domains such as monitoring of complex systems.","AuthorNames-Deduped":"Prithiviraj K. Muthumanickam;Katerina Vrotsou;Aida Nordman;Jimmy Johansson;Matthew D. Cooper","AuthorNames":"Prithiviraj K. Muthumanickam;Katerina Vrotsou;Aida Nordman;Jimmy Johansson;Matthew Cooper","AuthorAffiliation":"Linköping University;Linköping University;Linköping University;Linköping University;Linköping University","InternalReferences":"10.1109/TVCG.2012.276;10.1109/TVCG.2015.2468091;10.1109/TVCG.2016.2598695;10.1109/TVCG.2013.194;10.1109/TVCG.2017.2743939;10.1109/TVCG.2009.117","AuthorKeywords":"Eye-tracking data,areas of interest,clustering,minimum spanning tree,temporal data,spatio-temporal data","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":4,"XploreCitationCount - 2020-01":2,"PubsCited":49,"Award":null,"image":"25tvcg01-muthumanickam-2865042-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"RegressionExplorer: Interactive Exploration of Logistic Regression Models with Subgroup Analysis","DOI":"10.1109/TVCG.2018.2865043","Link":"http://dx.doi.org/10.1109/TVCG.2018.2865043","FirstPage":246,"LastPage":255,"PaperType":"J","Abstract":"We present RegressionExplorer, a Visual Analytics tool for the interactive exploration of logistic regression models. Our application domain is Clinical Biostatistics, where models are derived from patient data with the aim to obtain clinically meaningful insights and consequences. Development and interpretation of a proper model requires domain expertise and insight into model characteristics. Because of time constraints, often a limited number of candidate models is evaluated. RegressionExplorer enables experts to quickly generate, evaluate, and compare many different models, taking the workflow for model development as starting point. Global patterns in parameter values of candidate models can be explored effectively. In addition, experts are enabled to compare candidate models across multiple subpopulations. The insights obtained can be used to formulate new hypotheses or to steer model development. The effectiveness of the tool is demonstrated for two uses cases: prediction of a cardiac conduction disorder in patients after receiving a heart valve implant and prediction of hypernatremia in critically ill patients.","AuthorNames-Deduped":"Dennis Dingen;Marcel van 't Veer;Patrick Houthuizen;Eveline H. J. Mestrom;Hendrikus H. M. Korsten;Arthur R. A. Bouwman;Jarke J. van Wijk","AuthorNames":"Dennis Dingen;Marcel van't Veer;Patrick Houthuizen;Eveline H. J. Mestrom;Erik H.H.M. Korsten;Arthur R.A. Bouwman;Jarke van Wijk","AuthorAffiliation":"Eindhoven University of Technology;Catharina Hospital Eindhoven;Catharina Hospital Eindhoven;Catharina Hospital Eindhoven;Catharina Hospital Eindhoven;Catharina Hospital Eindhoven;University of Technology","InternalReferences":"10.1109/VAST.2017.8585720;10.1109/TVCG.2015.2467325;10.1109/TVCG.2013.125;10.1109/VAST.2011.6102453;10.1109/TVCG.2015.2467931","AuthorKeywords":"Visual analytics,Predictive visual analytics,Exploratory data analysis,Multivariate statistics,Regression analysis,Variable selection,Subgroup analysis","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":2,"XploreCitationCount - 2020-01":1,"PubsCited":32,"Award":null,"image":"25tvcg01-dingen-2865043-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"Seq2Seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models","DOI":"10.1109/TVCG.2018.2865044","Link":"http://dx.doi.org/10.1109/TVCG.2018.2865044","FirstPage":353,"LastPage":363,"PaperType":"J","Abstract":"Neural sequence-to-sequence models have proven to be accurate and robust for many sequence prediction tasks, and have become the standard approach for automatic translation of text. The models work with a five-stage blackbox pipeline that begins with encoding a source sequence to a vector space and then decoding out to a new target sequence. This process is now standard, but like many deep learning methods remains quite difficult to understand or debug. In this work, we present a visual analysis tool that allows interaction and “what if”-style exploration of trained sequence-to-sequence models through each stage of the translation process. The aim is to identify which patterns have been learned, to detect model errors, and to probe the model with counterfactual scenario. We demonstrate the utility of our tool through several real-world sequence-to-sequence use cases on large-scale models.","AuthorNames-Deduped":"Hendrik Strobelt;Sebastian Gehrmann;Michael Behrisch 0001;Adam Perer;Hanspeter Pfister;Alexander M. Rush","AuthorNames":"Hendrik Strobelt;Sebastian Gehrmann;Michael Behrisch;Adam Perer;Hanspeter Pfister;Alexander M. Rush","AuthorAffiliation":"IBM ReseatchMIT-IBM Watson AI Lab.;Harvard NLP group;Hatvatd Visual Computing group;IBM ReseatchMIT-IBM Watson AI Lab.;Hatvatd Visual Computing group;Harvard NLP group","InternalReferences":"10.1109/TVCG.2017.2744718;10.1109/TVCG.2017.2744478;10.1109/TVCG.2017.2744158","AuthorKeywords":"Explainable AI,Visual Debugging,Visual Analytics,Machine Learning,Deep Learning,NLP","AminerCitationCount_02-2020":1,"AminerCitationCount_06-2020":8,"XploreCitationCount - 2020-01":12,"PubsCited":55,"Award":"HM","image":"25tvcg01-strobelt-2865044-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"SIRIUS: Dual, Symmetric, Interactive Dimension Reductions","DOI":"10.1109/TVCG.2018.2865047","Link":"http://dx.doi.org/10.1109/TVCG.2018.2865047","FirstPage":172,"LastPage":182,"PaperType":"J","Abstract":"Much research has been done regarding how to visualize and interact with observations and attributes of high-dimensional data for exploratory data analysis. From the analyst's perceptual and cognitive perspective, current visualization approaches typically treat the observations of the high-dimensional dataset very differently from the attributes. Often, the attributes are treated as inputs (e.g., sliders), and observations as outputs (e.g., projection plots), thus emphasizing investigation of the observations. However, there are many cases in which analysts wish to investigate both the observations and the attributes of the dataset, suggesting a symmetry between how analysts think about attributes and observations. To address this, we define SIRIUS (Symmetric Interactive Representations In a Unified System), a symmetric, dual projection technique to support exploratory data analysis of high-dimensional data. We provide an example implementation of SIRIUS and demonstrate how this symmetry affords additional insights.","AuthorNames-Deduped":"Michelle Dowling;John E. Wenskovitch;J. T. Fry;Scotland Leman;Leanna House;Chris North","AuthorNames":"Michelle Dowling;John Wenskovitch;J.T. Fry;Scotland Leman;Leanna House;Chris North","AuthorAffiliation":"Virginia Tech Department of Computer Science;Virginia Tech Department of Computer Science;Virginia Tech Department of Statistics;Virginia Tech Department of Statistics;Virginia Tech Department of Statistics;Virginia Tech Department of Computer Science","InternalReferences":"10.1109/VAST.2012.6400493;10.1109/INFVIS.2005.1532136;10.1109/VAST.2014.7042492;10.1109/VAST.2012.6400486;10.1109/TVCG.2015.2467552;10.1109/VAST.2010.5652443;10.1109/TVCG.2012.260;10.1109/VAST.2011.6102449;10.1109/TVCG.2011.220;10.1109/TVCG.2016.2598445;10.1109/TVCG.2016.2598446;10.1109/INFVIS.2003.1249020;10.1109/TVCG.2008.173;10.1109/TVCG.2011.178;10.1109/TVCG.2012.256;10.1109/TVCG.2016.2598479;10.1109/TVCG.2013.150","AuthorKeywords":"Dimension reduction,semantic interaction,exploratory data analysis,observation projection,attribute projection","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":1,"PubsCited":59,"Award":null,"image":"25tvcg01-dowling-2865047-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"MotionRugs: Visualizing Collective Trends in Space and Time","DOI":"10.1109/TVCG.2018.2865049","Link":"http://dx.doi.org/10.1109/TVCG.2018.2865049","FirstPage":76,"LastPage":86,"PaperType":"J","Abstract":"Understanding the movement patterns of collectives, such as flocks of birds or fish swarms, is an interesting open research question. The collectives are driven by mutual objectives or react to individual direction changes and external influence factors and stimuli. The challenge in visualizing collective movement data is to show space and time of hundreds of movements at the same time to enable the detection of spatiotemporal patterns. In this paper, we propose MotionRugs, a novel space efficient technique for visualizing moving groups of entities. Building upon established space-partitioning strategies, our approach reduces the spatial dimensions in each time step to a one-dimensional ordered representation of the individual entities. By design, MotionRugs provides an overlap-free, compact overview of the development of group movements over time and thus, enables analysts to visually identify and explore group-specific temporal patterns. We demonstrate the usefulness of our approach in the field of fish swarm analysis and report on initial feedback of domain experts from the field of collective behavior.","AuthorNames-Deduped":"Juri Buchmüller;Dominik Jäckle;Eren Cakmak;Ulrik Brandes;Daniel A. Keim","AuthorNames":"Juri Buchmüller;Dominik Jäckle;Eren Cakmak;Ulrik Brandes;Daniel A. Keim","AuthorAffiliation":"University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;ETH Zurich, Switzerland;University of Konstanz, Germany","InternalReferences":"10.1109/TVCG.2013.193;10.1109/TVCG.2011.226;10.1109/VAST.2009.5332593;10.1109/TVCG.2009.145;10.1109/VAST.2014.7042477;10.1109/TVCG.2006.193;10.1109/TVCG.2013.192;10.1109/TVCG.2008.125;10.1109/TVCG.2012.265","AuthorKeywords":"Spatio-Temporal Visualization,Spatial Abstraction,Spatial Index Structures,Collective Movement","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":5,"XploreCitationCount - 2020-01":4,"PubsCited":54,"Award":null,"image":"25tvcg01-buchmuller-2865049-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"Drag and Track: A Direct Manipulation Interface for Contextualizing Data Instances within a Continuous Parameter Space","DOI":"10.1109/TVCG.2018.2865051","Link":"http://dx.doi.org/10.1109/TVCG.2018.2865051","FirstPage":256,"LastPage":266,"PaperType":"J","Abstract":"We present a direct manipulation technique that allows material scientists to interactively highlight relevant parameterized simulation instances located in dimensionally reduced spaces, enabling a user-defined understanding of a continuous parameter space. Our goals are two-fold: first, to build a user-directed intuition of dimensionally reduced data, and second, to provide a mechanism for creatively exploring parameter relationships in parameterized simulation sets, called ensembles. We start by visualizing ensemble data instances in dimensionally reduced scatter plots. To understand these abstract views, we employ user-defined virtual data instances that, through direct manipulation, search an ensemble for similar instances. Users can create multiple of these direct manipulation queries to visually annotate the spaces with sets of highlighted ensemble data instances. User-defined goals are therefore translated into custom illustrations that are projected onto the dimensionally reduced spaces. Combined forward and inverse searches of the parameter space follow naturally allowing for continuous parameter space prediction and visual query comparison in the context of an ensemble. The potential for this visualization technique is confirmed via expert user feedback for a shock physics application and synthetic model analysis.","AuthorNames-Deduped":"Daniel Orban;Daniel F. Keefe;Ayan Biswas;James P. Ahrens;David H. Rogers","AuthorNames":"Daniel Orban;Daniel F. Keefe;Ayan Biswas;James Ahrens;David Rogers","AuthorAffiliation":"University of Minnesota, USA;University of Minnesota, USA;Los Alamos National Labs;Los Alamos National Labs;Los Alamos National Labs","InternalReferences":"10.1109/TVCG.2013.133;10.1109/TVCG.2016.2598869;10.1109/VAST.2012.6400486;10.1109/TVCG.2010.190;10.1109/TVCG.2013.147;10.1109/VAST.2012.6400489;10.1109/TVCG.2015.2467436;10.1109/TVCG.2012.260;10.1109/VAST.2011.6102449;10.1109/TVCG.2015.2467204;10.1109/TVCG.2013.141;10.1109/TVCG.2017.2745178;10.1109/TVCG.2014.2346455;10.1109/TVCG.2016.2598589;10.1109/TVCG.2016.2598495;10.1109/TVCG.2016.2598839;10.1109/TVCG.2014.2346321;10.1109/TVCG.2011.248;10.1109/TVCG.2016.2598830","AuthorKeywords":"Visual Parameter Space Analysis,Ensemble Visualization,Semantic Interaction,Direct Manipulation,Shock Physics","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":5,"XploreCitationCount - 2020-01":3,"PubsCited":56,"Award":null,"image":"25tvcg01-orban-2865051-fig-1-source-large.gif"},{"Conference":"VAST","Year":2018,"Title":"VUSphere: Visual Analysis of Video Utilization in Online Distance Education","DOI":"10.1109/VAST.2018.8802383","Link":"http://dx.doi.org/10.1109/VAST.2018.8802383","FirstPage":25,"LastPage":35,"PaperType":"C","Abstract":"Online Distance Education (ODE) provides massive course videos of various specialties for students across the country to learn professional knowledge anytime and anywhere. Analyzing the utilization of these videos from user log data can help academics better understand the learning process of students, evaluate the quality of service provided by regional learning centers, and improve the quality of program curriculum in the future. However, due to the lack of comparable indicators, it is a great challenge to discover the utilization patterns of massive videos and analyze the learning process of large-scale student population from learning log data. In this paper, we introduce a visual analytics system, called VUSphere, to explore the video utilization from multiple perspectives with two proposed indicators. This system offers three coordinated views: a spherical layout overview to depict the overall utilization distribution of videos, courses, and students; a detailed statistics view with four panels to present video utilization statistics of each element from multiple perspectives; and a comparison view to examine the differences in individual elements. Based on the real dataset from our ODE school, several patterns related to video utilization and enrollment are found in the case study with our domain experts.","AuthorNames-Deduped":"Huan He;Qinghua Zheng;Bo Dong","AuthorNames":"Huan He;Oinghua Zheng;Bo Dong","AuthorAffiliation":"Key Laboratory of Intelligent Networks and Network Security, Ministry of Education, Xi’an Jiaotong University;Key Laboratory of Intelligent Networks and Network Security, Ministry of Education, Xi’an Jiaotong University;School of Continuing Education, Xi’an Jiaotong University","InternalReferences":"10.1109/TVCG.2016.2598444;10.1109/VAST.2016.7883517;10.1109/TVCG.2015.2467322","AuthorKeywords":"Video utilization pattern,online distance education,visual analytics","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":null,"PubsCited":0,"Award":null,"image":"he1abcdef-025_035_he-large.gif","hasViewed":true},{"Conference":"VAST","Year":2018,"Title":"Segue: Overviewing Evolution Patterns of Egocentric Networks by Interactive Construction of Spatial Layouts","DOI":"10.1109/VAST.2018.8802415","Link":"http://dx.doi.org/10.1109/VAST.2018.8802415","FirstPage":72,"LastPage":83,"PaperType":"C","Abstract":"Getting the overall picture of how a large number of ego-networks evolve is a common yet challenging task. Existing techniques often require analysts to inspect the evolution patterns of ego-networks one after another. In this study, we explore an approach that allows analysts to interactively create spatial layouts in which each dot is a dynamic ego-network. These spatial layouts provide overviews of the evolution patterns of ego-networks, thereby revealing different global patterns such as trends, clusters and outliers in evolution patterns. To let analysts interactively construct interpretable spatial layouts, we propose a data transformation pipeline, with which analysts can adjust the spatial layouts and convert dynamic ego-networks into event sequences to aid interpretations of the spatial positions. Based on this transformation pipeline, we develop Segue, a visual analysis system that supports thorough exploration of the evolution patterns of ego-networks. Through two usage scenarios, we demonstrate how analysts can gain insights into the overall evolution patterns of a large collection of ego-networks by interactively creating different spatial layouts.","AuthorNames-Deduped":"Po-Ming Law;Yanhong Wu;Rahul C. Basole","AuthorNames":"Po-Ming Law;Yanhong Wu;Rahul C. Basole","AuthorAffiliation":"Georgia Institute of Technology;Visa Research;Georgia Institute of Technology","InternalReferences":"10.1109/TVCG.2015.2467851;10.1109/VAST.2012.6400486;10.1109/TVCG.2011.226;10.1109/TVCG.2011.188;10.1109/VAST.2016.7883512;10.1109/VAST.2011.6102449;10.1109/TVCG.2013.198;10.1109/TVCG.2015.2467615;10.1109/TVCG.2016.2598446;10.1109/VAST.2015.7347632;10.1109/TVCG.2016.2598797;10.1109/TVCG.2013.200;10.1109/TVCG.2017.2744198;10.1109/TVCG.2015.2468078;10.1109/VAST.2009.5332595;10.1109/TVCG.2015.2468151","AuthorKeywords":"Human-centered computing,Visualization,Visualization techniques,Graph drawings","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":27,"PubsCited":0,"Award":null,"image":"law1-072_083_law-large.gif"},{"Conference":"VAST","Year":2018,"Title":"The Effect of Semantic Interaction on Foraging in Text Analysis","DOI":"10.1109/VAST.2018.8802424","Link":"http://dx.doi.org/10.1109/VAST.2018.8802424","FirstPage":13,"LastPage":24,"PaperType":"C","Abstract":"Completing text analysis tasks is a continuous sensemaking loop of foraging for information and incrementally synthesizing it into hypotheses. Past research has shown the advantages of using spatial workspaces as a means for synthesizing information through externalizing hypotheses and creating spatial schemas. However, spatializing the entirety of datasets becomes prohibitive as the number of documents available to the analysts grows, particularly when only a small subset are relevant to the task at hand. StarSPIRE is a visual analytics tool designed to explore collections of documents, leveraging users' semantic interactions to steer (1) a synthesis model that aids in document layout, and (2) a foraging model to automatically retrieve new relevant information. In contrast to traditional keyword search foraging (KSF), “semantic interaction foraging” (SIF) occurs as a result of the user's synthesis actions. To quantify the value of semantic interaction foraging, we use StarSPIRE to evaluate its utility for an intelligence analysis sensemaking task. Semantic interaction foraging accounted for 26% of useful documents found, and it also resulted in increased synthesis interactions and improved sensemaking task performance by users in comparison to only using keyword search.","AuthorNames-Deduped":"John E. Wenskovitch;Lauren Bradel;Michelle Dowling;Leanna House;Chris North","AuthorNames":"John Wenskovitch;Lauren Bradel;Michelle Dowling;Leanna House;Chris North","AuthorAffiliation":"Virginia Tech Computer Science;Department of Defense;Virginia Tech Computer Science;Virginia Tech Statistics;Virginia Tech Computer Science","InternalReferences":"10.1109/TVCG.2006.120;10.1109/VAST.2012.6400559;10.1109/TVCG.2013.205;10.1109/VAST.2008.4677362;10.1109/VAST.2014.7042492;10.1109/VAST.2012.6400486;10.1109/TVCG.2012.260;10.1109/VAST.2011.6102449;10.1109/INFVIS.2005.1532126;10.1109/INFVIS.1995.528686;10.1109/INFVIS.2004.37","AuthorKeywords":"Human-centered computing,Visualization,Empirical studies in visualization,Human-centered computing,Visualization,Visual analytics","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":1,"XploreCitationCount - 2020-01":29,"PubsCited":0,"Award":null,"image":"wensk1-013_024_wenskovitch-large.gif"},{"Conference":"VAST","Year":2018,"Title":"The Effect of Proximity in Social Data Charts on Perceived Unity","DOI":"10.1109/VAST.2018.8802449","Link":"http://dx.doi.org/10.1109/VAST.2018.8802449","FirstPage":1,"LastPage":12,"PaperType":"C","Abstract":"Social data charts - visual presentations of quantitative data about peer behaviors - may offer new means to motivate individuals to participate in group goals. However, to do so these charts need to create a semantic response of `unity' among the chart viewers in order to overcome the problems of social loafing where people act selfishly and undervalue the group's goal. In this paper, we focus on two properties of social data charts that may affect a viewer's perceptions of unity: (1) The skewness in the data structure - the statistical distribution of the social data, and (2) the proximity in the visual structure of the chart - the spatial organization of the data points. We performed a controlled perceptual experiment to examine the effect of proximity and skewness on four different semantic facets of perceived group unity: similarity, entitativity, rapport, and centrality. We exposed 179 participants on Amazon Mechanical Turk to different group charts using a 2 x 2 factorial design, varying both proximity and skewness. Our two-way ANCOVA analyses reveal three important findings: (1) Across all conditions, proximity has a strong positive effect on perceived group unity and conveys a social meaning of group entitativity, as well as, member similarity and rapport; (2) Skewness and proximity interact in a non-linear way, suggesting that skewness creates a negative force that modifies the semantic responses to high proximity; and (3) The perceptual responses to proximity have different semantic facets that are either stable or sensitive to skewness. These findings contribute to perceptual as well as social InfoVis literature.","AuthorNames-Deduped":"Marlen Promann;Sabine Brunswicker","AuthorNames":"Marlen Promann;Sabine Brunswicker","AuthorAffiliation":"Purdue University;Purdue University","InternalReferences":"10.1109/TVCG.2017.2746018;10.1109/TVCG.2008.171;10.1109/TVCG.2014.2346419;10.1109/TVCG.2007.70541;10.1109/TVCG.2013.234;10.1109/TVCG.2010.174;10.1109/TVCG.2012.221;10.1109/TVCG.2017.2745240","AuthorKeywords":"Social visualization,information visualization,Human-centered computing,Empirical studies in HCI,Displays and imagers,Social engineering (social sciences),Visualization theory, concepts and paradigms,Gestalt theory,controlled experiment,data density,data spread,perception,group identity,unity,priming","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":27,"PubsCited":0,"Award":null,"image":"proma1-001_012_promann-large.gif"},{"Conference":"VAST","Year":2018,"Title":"EmbeddingVis: A Visual Analytics Approach to Comparative Network Embedding Inspection","DOI":"10.1109/VAST.2018.8802454","Link":"http://dx.doi.org/10.1109/VAST.2018.8802454","FirstPage":48,"LastPage":59,"PaperType":"C","Abstract":"Constructing latent vector representation for nodes in a network through embedding models has shown its practicality in many graph analysis applications, such as node classification, clustering, and link prediction. However, despite the high efficiency and accuracy of learning an embedding model, people have little clue of what information about the original network is preserved in the embedding vectors. The abstractness of low-dimensional vector representation, stochastic nature of the construction process, and non-transparent hyper-parameters all obscure understanding of network embedding results. Visualization techniques have been introduced to facilitate embedding vector inspection, usually by projecting the embedding space to a two-dimensional display. Although the existing visualization methods allow simple examination of the structure of embedding space, they cannot support in-depth exploration of the embedding vectors. In this paper, we design an exploratory visual analytics system that supports the comparative visual interpretation of embedding vectors at the cluster, instance, and structural levels. To be more specific, it facilitates comparison of what and how node metrics are preserved across different embedding models and investigation of relationships between node metrics and selected embedding vectors. Several case studies confirm the efficacy of our system. Experts' feedback suggests that our approach indeed helps them better embrace the understanding of network embedding models.","AuthorNames-Deduped":"Quan Li;Kristanto Sean Njotoprawiro;Hammad Haleem;Qiaoan Chen;Chris Yi;Xiaojuan Ma","AuthorNames":"Quan Li;Kristanto Sean Njotoprawiro;Hammad Haleem;Qiaoan Chen;Chris Yi;Xiaojuan Ma","AuthorAffiliation":"Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong;Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong;Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong;WeChat, Tencent Technology (Shenzhen) Co., Ltd., Shenzhen, China;WeChat, Tencent Technology (Shenzhen) Co., Ltd., Shenzhen, China;Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong","InternalReferences":"10.1109/TVCG.2007.70521;10.1109/TVCG.2013.173;10.1109/VISUAL.1990.146402;10.1109/TVCG.2016.2598415;10.1109/TVCG.2017.2745141;10.1109/TVCG.2016.2598838;10.1109/TVCG.2015.2468151","AuthorKeywords":"Human-centered computing,Visualization,Visualization application domains,Visual analytics,Human-centered computing,Visualization,Visualization design and evaluation methods","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":3,"PubsCited":0,"Award":null,"image":"li1-048_059_li-large.gif","hasViewed":true},{"Conference":"VAST","Year":2018,"Title":"SMARTexplore: Simplifying High-Dimensional Data Analysis through a Table-Based Visual Analytics Approach","DOI":"10.1109/VAST.2018.8802486","Link":"http://dx.doi.org/10.1109/VAST.2018.8802486","FirstPage":36,"LastPage":47,"PaperType":"C","Abstract":"We present SMARTEXPLORE, a novel visual analytics technique that simplifies the identification and understanding of clusters, correlations, and complex patterns in high-dimensional data. The analysis is integrated into an interactive table-based visualization that maintains a consistent and familiar representation throughout the analysis. The visualization is tightly coupled with pattern matching, subspace analysis, reordering, and layout algorithms. To increase the analyst's trust in the revealed patterns, SMARTEXPLORE automatically selects and computes statistical measures based on dimension and data properties. While existing approaches to analyzing high-dimensional data (e.g., planar projections and Parallel coordinates) have proven effective, they typically have steep learning curves for non-visualization experts. Our evaluation, based on three expert case studies, confirms that non-visualization experts successfully reveal patterns in high-dimensional data when using SMARTEXPLORE.","AuthorNames-Deduped":"Michael Blumenschein;Michael Behrisch 0001;Stefanie Schmid;Simon Butscher;Deborah Wahl;Karoline Villinger;Britta Renner;Harald Reiterer;Daniel A. Keim","AuthorNames":"Michael Blumenschein;Michael Behrisch;Stefanie Schmid;Simon Butscher;Deborah R. Wahl;Karoline Villinger;Britta Renner;Harald Reiterer;Daniel A. Keim","AuthorAffiliation":"University of Konstanz, Germany;Harvard University, USA;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany;University of Konstanz, Germany","InternalReferences":"10.1109/INFVIS.2004.46;10.1109/VAST.2010.5652433;10.1109/INFVIS.1998.729559;10.1109/TVCG.2017.2743978;10.1109/TVCG.2011.188;10.1109/VAST.2009.5332611;10.1109/TVCG.2010.184;10.1109/TVCG.2014.2346260;10.1109/TVCG.2013.173;10.1109/TVCG.2015.2467553;10.1109/TVCG.2014.2346248;10.1109/TVCG.2010.138;10.1109/INFVIS.2004.15;10.1109/TVCG.2014.2346279;10.1109/INFVIS.2003.1249016;10.1109/VAST.2009.5332628;10.1109/TVCG.2015.2468078;10.1109/TVCG.2017.2745078;10.1109/TVCG.2017.2744098;10.1109/TVCG.2013.150","AuthorKeywords":"High-dimensional data,visual exploration,pattern-driven analysis,tabular visualization,subspace,aggregation","AminerCitationCount_02-2020":null,"AminerCitationCount_06-2020":0,"XploreCitationCount - 2020-01":7,"PubsCited":0,"Award":null,"image":"blume1-025_035_he-large.gif"},{"Conference":"VAST","Year":2018,"Title":"Analyzing the Noise Robustness of Deep Neural Networks","DOI":"10.1109/VAST.2018.8802509","Link":"http://dx.doi.org/10.1109/VAST.2018.8802509","FirstPage":60,"LastPage":71,"PaperType":"C","Abstract":"Deep neural networks (DNNs) are vulnerable to maliciously generated adversarial examples. These examples are intentionally designed by making imperceptible perturbations and often mislead a DNN into making an incorrect prediction. This phenomenon means that there is significant risk in applying DNNs to safety-critical applications, such as driverless cars. To address this issue, we present a visual analytics approach to explain the primary cause of the wrong predictions introduced by adversarial examples. The key is to analyze the datapaths of the adversarial examples and compare them with those of the normal examples. A datapath is a group of critical neurons and their connections. To this end, we formulate the datapath extraction as a subset selection problem and approximately solve it based on back-propagation. A multi-level visualization consisting of a segmented DAG (layer level), an Euler diagram (feature map level), and a heat map (neuron level), has been designed to help experts investigate datapaths from the high-level layers to the detailed neuron activations. Two case studies are conducted that demonstrate the promise of our approach in support of explaining the working mechanism of adversarial examples.","AuthorNames-Deduped":"Mengchen Liu;Shixia Liu;Hang Su;Kelei Cao;Jun Zhu 0001","AuthorNames":"Mengchen Liu;Shixia Liu;Hang Su;Kelei Cao;Jun Zhu","AuthorAffiliation":"School of Software, Tsinghua University;School of Software, Tsinghua University;Dept.of Comp.Sci.Tech., Tsinghua University;School of Software, Tsinghua University;Dept.of Comp.Sci.Tech., Tsinghua University","InternalReferences":"10.1109/TVCG.2015.2467618;10.1109/TVCG.2011.186;10.1109/TVCG.2016.2598496;10.1109/TVCG.2017.2744683;10.1109/TVCG.2014.2346431;10.1109/TVCG.2014.2346433;10.1109/TVCG.2017.2744199;10.1109/TVCG.2017.2744718;10.1109/TVCG.2017.2744938;10.1109/TVCG.2016.2598831;10.1109/TVCG.2013.196;10.1109/TVCG.2011.209;10.1109/TVCG.2017.2744358;10.1109/TVCG.2016.2598838;10.1109/TVCG.2010.210;10.1109/TVCG.2017.2744018;10.1109/TVCG.2011.183;10.1109/TVCG.2017.2744158;10.1109/VISUAL.2005.1532820;10.1109/VAST.2014.7042494;10.1109/TVCG.2017.2744878;10.1109/TVCG.2018.2865041","AuthorKeywords":"Deep neural networks,robustness,adversarial examples,back propagation,multi-level visualization.","AminerCitationCount_02-2020":0,"AminerCitationCount_06-2020":3,"XploreCitationCount - 2020-01":0,"PubsCited":0,"Award":null,"image":"liu1abc-060_071_liu-large.gif","hasViewed":true}]}